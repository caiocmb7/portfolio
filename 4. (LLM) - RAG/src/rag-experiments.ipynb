{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfcefde2-1635-4564-bbbf-162c39baf81f",
   "metadata": {},
   "source": [
    "## Installation & Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc60e266-89a0-401d-8fe9-2d7601580a12",
   "metadata": {},
   "source": [
    "You will need to install some others requirements like:\n",
    "\n",
    "    - Ollama\n",
    "    - Poppler\n",
    "    - Pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eb76140-0333-4fc6-a103-04d460a01d31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ctgan 0.7.3 requires packaging<22,>=20, but you have packaging 23.2 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ../requirements.txt --q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43fd5785-5aea-4bb2-8229-1cb55f52ec1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME          \tID          \tSIZE  \tMODIFIED     \n",
      "llama2:latest \t78e26419b446\t3.8 GB\t3 months ago\t\n",
      "mistral:latest\t61e88e884507\t4.1 GB\t3 months ago\t\n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e5ecc42-a743-4965-9293-7847d17b4feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b185358b-205b-4a53-8ba3-4c4cbf687b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "import sys\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_community.document_loaders import OnlinePDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import SingleStoreDB\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "load_dotenv(\"../config/.env\", override=True)\n",
    "model = \"mistral:latest\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb71f53d-b21e-4fb4-855f-a8024230e61c",
   "metadata": {},
   "source": [
    "## Getting PDF Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cc29dc2-8e7f-444b-8719-1b81de5ae927",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\caio_barros\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\caio_barros\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "unstructured_loader = UnstructuredPDFLoader(\"../data/generative-ai-fundamentals-v1.pdf\")\n",
    "\n",
    "unstructured_pdf_data = unstructured_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afd82534-593c-4504-bca4-64aeec4708e2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='© databricks Academy\\n\\nGenerative Al Fundamentals\\n\\nDatabricks Academy 2023\\n\\nAE ad PO TOL\\n\\nSe Everyone Asks\\n\\na Z Yj @ at 3a tei UD a i Mam is Generative 4 {+ How exactly | 4 om = Y ‘waa How can l use Prag 9 Alathreat or canluse : ~ y, | ‘ ; my data an Generative Al | = wo yt.» securely with |\\n\\ns \\\\ opportunity to gaina 7 “\\\\ Generative\\n\\nBy for my U), competitive Hh Al?\\n\\nbusiness? es advantage? :\\n\\ny) e —~ x\\n\\nsession goals\\n\\nUpon completion of this content, you should be able to:\\n\\nDescribe how generative artificial intelligence (Al) is being used to revolutionize practical Al applications\\n\\nDescribe how Generative Al models works and discuss their potential business uses cases\\n\\nDescribe how a data organization can find initial success with generative Al applications\\n\\nRecognize the potential legal and ethical considerations of utilizing generative Al for applications and within the workplace.\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\nAGENDA\\n\\nO01. Introducing Generative Al\\n\\nGenerative Al Basics\\n\\nLLMs and Generative Al\\n\\nO02. Finding Success with Generative Al LLM Applications\\n\\nGenerative Al with Databricks ML\\n\\nAl Adoption Preparation 03. Assessing Potential Risks and Challenges Legality\\n\\nEthical Considerations\\n\\nHuman-aAl Interaction\\n\\n© databricks\\n\\nAcademy\\n\\nIntroducing Generative AI:\\n\\nGenerative Al Basics\\n\\nDatabricks Academy 2023\\n\\nWhat is Generative Al?\\n\\nDeep Learning (DL)\\n\\nGenerative Al\\n\\nile ae\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nArtificial Intelligence:\\n\\nA multidisciplinary field of computer science that aims to create systems capable of emulating and surpassing human-level intelligence.\\n\\nMachine Learning:\\n\\nLearn from existing data and make predictions/prediction without being explicitly programmed.\\n\\nDeep Learning:\\n\\nUses “artificial neural networks” to learn from data.\\n\\ng\\n\\nWhat is Generative Al?\\n\\nDeep Learning (DL)\\n\\nGenerative Al\\n\\nal o\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nGenerative Artificial Intelligence:\\n\\nSub-field of Al that focuses on generating new content such as:\\n\\n¢ Images\\n\\n¢ Text\\n\\n¢ Audio/music « Video\\n\\n¢ Code\\n\\n¢ 3D objects\\n\\n¢ Synthetic data\\n\\nGenerative Models\\n\\nA branch of ML modeling which mathematically approximates the world\\n\\nData objects Deep Neural Network Tasks\\n\\n¢ Synthetic image ° Bird - generation\\n\\n[0.5, 14, -1, ..] ¢ Style transfer / edit\\n\\nTranslation Question Answering « Semantic search\\n\\n| “i\\n\\n[0.8, 1.4, -2.3, ....]\\n\\nSpeech-to-text Music transcription\\n\\nA\\n\\nq) —\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\n[1.8, 0.4, -1.5, ....]\\n\\nWhy Now?\\n\\nFactors making Generative Al possible now\\n\\nLarge Datasets\\n\\ne Availability of large and diverse datasets Al models learn patterns, correlations, and characteristics of large datasets Pre-trained state-of-the-art models\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nWhy Now?\\n\\nFactors making Generative Al possible now\\n\\nae\\n\\nLarge Datasets Computational Power e Availability of large and e Advancements in diverse datasets hardware; GPUs e Al models learn e Access to cloud patterns, correlations, computing and characteristics of e Open-source software,\\n\\nlarge datasets Hugging Face\\n\\ne Pre-trained state-of-the-art models\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nWhy Now?\\n\\nFactors making Generative Al possible now\\n\\nae\\n\\nLarge Datasets Computational Power\\n\\ne Advancements in hardware; GPUs\\n\\ne Access to cloud computing\\n\\ne Open-source software, Hugging Face\\n\\ne Availability of large and diverse datasets\\n\\ne Al models learn patterns, correlations, and characteristics of large datasets\\n\\ne Pre-trained state-of-the-art models\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\n&\\n\\nInnovative DL Models\\n\\ne Generative Adversarial Networks (GANs)\\n\\ne Transformers Architecture\\n\\ne Reinforcement learning from human feedback (RLHF)\\n\\nWhy should | care now?\\n\\nML/AI has been around for a while, why it matters now\\n\\nGenerative Al models’ accuracy and\\n\\neffectiveness have hit a tipping point Chegg shares drop more than\\n\\n40% after company says\\n\\n¢ Powerful enough to enable use cases not ChatGPT is killing its business\" feasible even a year ago ¢ Economical enough for use even by 50910212023\\n\\nnon-technical business users\\n\\nGenerative Al models and tooling are readily available\\n\\ne Many models are open source and customizable\\n\\ne Requires powerful GPUs, but are available in the cloud\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\nGenerative Al Use Cases\\n\\nIntelligent conversations, creative text creation, code generation\\n\\n¢ Content generation ¢ Question/answers\\n\\n¢ Virtual assistants ¢ Content personalization ¢ Language style transfer\\n\\n¢ Story telling, poetry, creative writing ¢ Translation\\n\\n¢ Code generation / auto-completion\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nHey! | am giving a presentation about Generative Al in Data +\\n\\nAl Summit. Write a short funny text to convince audiences\\n\\nthat Generative Al is the future. Max 100 words.\\n\\nStep into the future with Generative AI! It\\'s not just about flying cars and robot butlers. This mind-boggling technology can compose symphonies, craft witty jokes, and design cutting-edge fashion trends. It pushes the limits of human creativity, conjuring up entire worlds, unique characters, and mind-bending art. But it doesn\\'t stop there. Generative AI revolutionizes industries too, discovering new drugs and predicting market trends. So, get ready to be amazed. Embrace the future, where imagination knows no bounds, and Generative AI becomes our ultimate creative companion. The\\n\\nfuture is here, and it\\'s hilariously amazing!\\n\\ng\\n\\nGenerative Al Use Cases\\n\\nVisual/audio/3D generation: Unlocking creativity\\n\\nImage generation Video Synthesis a e Generate realistic/artistic Sy e Animation\\n\\nhigh-quality images e Scene generation e Virtual agent generation\\n\\n3D Generation Audio Generation\\n\\ne Object, character generation ZS) e Narration e Animations ae e Music composition\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nGenerative Al Use Cases\\n\\nsynthetic data generation\\n\\n¢ Synthetic dataset generation ° Increase size, diversity of dataset ° Privacy protection ° Simulate scenarios ° Fraud detection, network attack detection\\n\\n¢ Synthetic data for computer vision (e.g. autonomous Cars)\\n\\n° Object detection\\n\\n« Adversarial scenarios (weather, road condition)\\n\\n¢ Synthetic text for natural language processing\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nGenerative Al Use Cases\\n\\nGenerative design: Discover drugs, design unique systems\\n\\n¢ Drug discovery ¢ Product and material design\\n\\n¢ Chip design\\n\\n¢ Architectural design and urban planning\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\n© databricks Academy\\n\\nIntroducing Generative AI: Generative Al and LLMs\\n\\nDatabricks Academy 2023\\n\\n“Vicuna: an open-source chatbot impressing GPT-4 with 90%* ChatGPT quality”\\n\\n03/30/2023\\n\\n“GPT-4 beats 90% of lawyers trying to pass the bar”\\n\\nForbes 03/14/2023\\n\\n©2023 Databricks Inc. — All rights reserved | Confidential and proprietary\\n\\n“Smaller, more performant models such as LLaMA enable... further democratizing access in this important, fast-changing field...”\\n\\nOO Meta Al 02/24/2023\\n\\n“Falcon is now free of royalties for commercial and research use... Falcon 40B outperforms ... Meta’s LLaMA and Stability Al\\'s StableLM”\\n\\n05/31/2023\\n\\nLLMs are not hype—they change the Al game\\n\\nGenerative Al & LLMs are a once-in-a-generation shift in technology\\n\\nWhat is a LLM?\\n\\n: Large Language Model (LLM): Generative Al Model trained on massive datasets to achieve\\n\\nadvanced language processing capabilities\\n\\nBased on deep learning neural networks\\n\\nLarge Language Models (LLMs)\\n\\nFoundation Model:\\n\\nLarge ML model trained on vast amount of data & fine-tuned for more specific language understanding and generation tasks\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\nHow Do LLMs Work?\\n\\nA simplified version of LLM training process\\n\\n>. co Input > Tokenize Token Embeddings Shape Sea laae< Output Text\\n\\n(Encode text into numeric rep.) (Put words with similar meaning close in vector space)\\n\\nI\\n\\nI\\n\\nI\\n\\nDolly is a language model that can help Embedding Functions you generate text for a variety of tasks. (Pre-trained model) I 1\\n\\nI I v\\n\\nWikipedia\\n\\nDolly is a language model that can help you generate text for a variety of tasks.\\n\\n[0.2, 1.5, O06 .... 0.6]\\n\\nScientific Research Pre-Trained\\n\\nTransformer Model\\n\\nWhen done well, similar words will be [35, 5098, 318, 257, 3303, 2746, 326, 460, closer in these embedding/vector 1037, 345, 7716, 2420, 329, 257, 4996, 286, spaces. Example 2D representation; 8861, 13]\\n\\nPredicted next word\\n\\nCrawled data from the Internet\\n\\niN\\n\\nCustom Curated Billions of parameters Datasets ...\\n\\nTokens: 18, Characters: 81 (100 tokens ~= 75 words)\\n\\nEncoding Decoding\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\nAn Overview of Common LLMs\\n\\nOpen-source and Closed LLMs\\n\\nModel ormodel Model size License Created by Released family (# params) Falcon 7B-40B Apache 2.0 Technology 2023 A newer potentially state-of-the-art model Innovation Institute MPT 7B Apache 2.0 MosaicML 2023 Comes with various models for chat, writing etc. Dolly 12B MIT Databricks 2023 Instruction-tuned Pythia model Pythia 19M-12B Apache 2.0 EleutherAl 2023 Series of 8 models for comparisons across sizes GPT-3.5 175 B proprietary OpenAl 2022 ChatGPT model option; related models GPT-1/2/3/4 BLOOM 560 M - 176 B RAIL v1.0 BigScience 2022 46 languages FLAN-T5 80 M - 540 B Apache 2.0 Google 2021 methods to improve training for existing architectures BART 139 M - 406 M Apache 2.0 Meta 2019 derived from BERT, GPT, others BERT 109 M- 335M Apache 2.0 Google 2018 early breakthrough\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nFor up-to-date list of recommended LLMs: https://www.databricks.com\\n\\nPlease note: Databricks does not endorse any of these models - you should evaluate these if they meet your needs.\\n\\nLLMs Generate Outputs for NLP Tasks\\n\\nCommon LLM tasks\\n\\nweg\\n\\n</>\\n\\nContent Creation and Augmentation\\n\\nSummarization Question Answering Machine Translation\\n\\nClassification\\n\\nNamed Entity Recognition (NER)\\n\\nTone / Level of content\\n\\nCode generation\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nGenerating coherent and contextually relevant text. LLMs excel at tasks like text completion, creative writing, story generation, and dialogue generation.\\n\\nSummarizing long documents or articles into concise summaries. LLMs provide an efficient way to extract key information from large volumes of text.\\n\\nComprehend questions and provide relevant answers by extracting information from their pre-trained knowledge.\\n\\nAutomatically converting a text from one language to another. LLMs are also capable to explain language structure such as grammatical rules.\\n\\nCategorizing text into predefined classes or topics. LLMs are useful for tasks like topic classification, soam detection, or sentiment analysis.\\n\\nIdentifying and extracting named entities like names of persons, organizations, locations, dates, and more from text.\\n\\nAdjusting the text\\'s tone (professional, humorous, etc.) or complexity level (e.g., fourth-grade level).\\n\\nGenerating code in a specified programming language or converting code from one language to another.\\n\\ng\\n\\nLLMs Business Use Cases\\n\\nCustomer Engagement\\n\\n¢ Personalization and customer segmentation: provided data?\\n\\nWhat are the top 5 customer complaints based on the\\n\\nProvide personalized product/content recommendation based on customer nactoten complaints arenes follower behaviour and preferences 1. Shipping Delays - 25% of customers expressed\\n\\nfrustration with delayed deliveries. 2. Product Quality - 20% of customers reported\\n\\ne Feed back Analysis issues with the quality of the received\\n\\nproducts.\\n\\n. . 3. Customer Service Responsiveness - 18% of\\n\\ne Virtual assista nts customers felt that the response time from customer service was slow.\\n\\n4. Billing and Payment Errors - 15% of customers encountered errors and discrepancies in their bills and payments.\\n\\n5. Order Inaccuracies - 12% of customers received incorrect or incomplete orders.\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\nLLMs Business Use Cases\\n\\nContent Creation\\n\\nin . ole . . yoo ¢ Creative writing: Short stories, creative jaunches on” | . : icnatGPt oks 0” ae a, pEUTER? narratives, scripts etc. carter PS = ¢ Technical writing: Documentation, user . ope “4 manuals, simplifying content etc. a, alre , . a. “Online re) Iting p ¢ Translation and localization Cipes» © 200ks, Websip ° eo. ° ° > Iles ¢ Article writing for blogs/social media be Wass, Noten 2 NZ) will ChatGPT supplant ws writers, thinkers?” irvard Gazette\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\nLLMs Business Use Cases\\n\\nProcess automation and efficiency\\n\\nCheck customer order history,\\n\\nretrieve product details > generate personalized message.\\n\\n¢ Customer support augmentation =\\n\\nfreeform text\\n\\nand automated question answering lam angry! Your [Product Name] is a Customer Data\\n\\ncomplete disaster. It\\'s cheaply made, Order Data falling apart after just a few uses. It\\n\\n¢- Automated customer response eee eee\\n\\nresolution - either replace it with a\\n\\nO working product or refund my money ° Et Y 1a il immediately. This is unacceptable, and | won\\'t tolerate such poor quality. Fix this\\n\\nn n n now, or I\\'ll take my business elsewhere ° Soc | a | aa ed | a, p rod U ct revi ews and spread the word about your shoddy products.\\n\\n¢ Sentiment analysis, prioritization Expecting immeciate action\\n\\nSuggested Automated Message\\n\\nDear Jack Doe,\\n\\nWe understand your frustration with the TV you purchased. It seems that there are widespread issues with the manufacturing of these TVs (10% deficiency), affecting their performance. We apologize for any inconvenience caused and recommend submitting a refund request through\\n\\nthe following link: [Refund Request Form]. Rest assured, we are actively working with the manufacturer to address these problems. Thank you for your patience and understanding.\\n\\nSincerely,\\n\\nCustomer Support\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\nLLMs Business Use Cases\\n\\nCode generation and developer productivity\\n\\nsentiments.ts\\n\\n#!/usr/bin/env ts-node\\n\\n¢ Code completion, boilerplate code port { fetch } from \"fetch\"\\n\\ngeneration ee - Error detection and debugging iphone - Convert code between languages __—_\\n\\n¢ Write code documentation sora\\n\\n}\\n\\n¢ Automated testing\\n\\n@ max_sum-slice.py\\n\\n¢ Natural language to code generation eiting ghaetnentin tt\\n\\n¢ Virtual code assistant for learning to code\\n\\nImage Source: Github g\\n\\n© databricks\\n\\nAcademy\\n\\nFinding Success with Generative Al:\\n\\nLLM Applications\\n\\nDatabricks Academy 2023\\n\\nModeling techniques quickly commoditize...\\n\\nSaaS LLM models prices dropping exponentially (10X decrease YoY)\\n\\nHigh quality open-source Generative Al unlocks the models now available value of *your* data\\n\\nBuild the Al apps only\\n\\nyou can build :\\n\\nLLM Flavors\\n\\nThinking of building your own modern LLM application?\\n\\n$54 o = Mla Lae Open-Source Models Proprietary Models\\n\\n« Use as off-the-shelf or « Usually offered as\\n\\nfine-tune LLMs-as-a-service e Provides flexibility for e Some can be fine-tuned\\n\\ncustomizations e Restrictive licenses for « Can be smaller in size to usage and modification\\n\\nsave cost e Commercial / Non-commercial use\\n\\nOpen-source LLMs: Proprietary LLMs:\\n\\nNon-commercial Use Commercial Use ANTHROP\\\\C AA Gopenat OU Meta Al & databricks {\\\\ mosaic + S * LLaMA Dolly MPT + ChatPT PaLM 2\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nChoose the right LLM model flavor\\n\\nThere is no “perfect” model, trade-offs are required.\\n\\nLLM model decision criteria\\n\\n=\\n\\nLS J\\n\\n©):\\n\\nQuality Cost Latency\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nUsing Proprietary Models (LLMs-as-a-Service)\\n\\n¢ Speed of develooment\\n\\nQuick to get started and working.\\n\\nAs this is another API call, it will fit very easily into existing pipelines.\\n\\n¢ Quality\\n\\n« Can offer state-of-the-art results\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nCons\\n\\n° Cost\\n\\nPay for each token sent/received.\\n\\n¢ Data Privacy/Security\\n\\nYou may not know how your data is being used.\\n\\ne Vendor lock-in\\n\\nSusceptible to vendor outages, deprecated features, etc.\\n\\nUsing Open Source Models\\n\\n¢ Task-tailoring\\n\\nSelect and/or fine-tune a task-specific model for your use case.\\n\\ne Inference Cost\\n\\nMore tailored models often smaller, making them faster at inference time.\\n\\n¢ Control\\n\\n° All of the data and model information stays entirely within your locus of control.\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nCons\\n\\n¢ Upfront time investments\\n\\nNeeds time to select, evaluate, and possibly tune\\n\\n¢ Data Requirements\\n\\n° Fine-tuning or larger models require larger datasets.\\n\\n¢ Skill Sets\\n\\nRequire in-house expertise\\n\\nFine Tuned Models\\n\\nWhat is fine-tuning and how it works\\n\\nFine-tuning: The process of further training a pre-trained model on a specific task or dataset to adapt it for a particular application or domain.\\n\\nose Se ee ee ee ee ee ee ee ee ee ee ee ee ee ee ee ee ee ee ee ee ee ee ee ee ee ee\\n\\nLarge corpus of training data\\n\\na\\n\\nSmaller corpus of training data\\n\\n\\n\\n“yyy _ =F fo Fine-tuned Bro ee ————_—> Foundation ———— “Medel. =F ode Model ee ee\\n\\nComputationally expensive process Task specific training\\n\\nwere -,K\\n\\neex=-=-=- ¢\\n\\nModel Fine-Tuning\\n\\na a a el\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\nFine-tuning models\\n\\nFoundation models can be fine-tuned for specific tasks\\n\\n—_— = —_ —_——=— <= -_-=-™ —_—— -_=-™ ——\\n\\nSupervised training = Ess on smaller labeled =F = Question, Answer model model organization model Ls ~ Ls ~ 7 ; Task-specific Question Sentiment pauline fine-tuned models Answering Analysis ue\\n\\nRecognition\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\nFine-tuning models\\n\\nFoundation models can be fine-tuned for domain adaptation\\n\\n=—— = — — =—_—— = — — --7 _--~ --7 _-_\\n\\nSupervised training\\n\\non smaller labeled pound Senne datasets oundation oundation i ves model wo docs model\\n\\nLegal docs\\n\\n—— = = = -_-7 =\\n\\n~= = ~= = -_ = -_ — - -—_ ee = — ==\\n\\nDomain-specific fine-tuned models\\n\\nFinance\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nOpen Source quality is rapidly advancing — while fine tuning cost is rapidly decreasing\\n\\nDolly started the trend to open models with a commercially friendly license\\n\\nFacebook LLaMA Stanford Alpaca Databricks Dolly Mosaic MPT Til Falcon “Smaller, more performant models “Alpaca behaves qualitatively “Dolly willhelp democratize LLMs, “MPT-7B is trained from scratch on “Falcon significantly outperforms such as LLaMA...democratizes _ similarly to OpenAIl ... while being transforming them into a 1T tokens ... is open source, GPT-3 for ... 75% of the training access in this important, surprisingly small and easy /cheap commodity every company can available for commercial use, and compute budget—and ... a fifth of fast-changing field.” to reproduce” own and customize” matches the quality of LLaMA-7B” the compute at inference time.” February 24, 2023 March 13, 2023 March 24, 2023 May 5, 2023 May 24, 2023\\n\\nNon Commercial Use Only | Commercial Use Permitted\\n\\n©2023 Databricks Inc. — All rights reserved | Confidential and proprietary gY\\n\\nMixing LLM Flavors in a Workflow\\n\\nTypical applications are more than just a prompt-response system.\\n\\nTasks: Single interaction (_ with an LLM ( Ne ooeeene\\n\\nDirect LLM calls are just part of a full task/application workflow\\n\\nWorkflow: Applications with more than a single Interaction\\n\\n|=) Task 1\\n\\n(Summarization)\\n\\nWorkflow Initiated\\n\\neee ~\\n\\n-- eter eee ee ee ee ee ee ee ee ee ee ee Ke\\n\\nEnd-to-end workflow\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\nMixing LLM Flavors in a Workflow\\n\\nExample multi-LLM problem: get the sentiment of many articles on a topic\\n\\nArticle 1: “...” Initial solution Article 2:\"...\" Put all the articles together and have the LLM parse it all\\n\\nArticle 3: “...\" . Article 4: “...” Overall Article 5: “...” Sentiment Article 6: “...” Issue\\n\\nArticle 7: “...\" Can quickly overwhelm the model input\\n\\nlength\\n\\naw) Better solution Article 1: “...\\n\\nLa e Article 2: “...” — Summary 1 Overall A two-stage process to first 4 + Summar : : Article 3:\"..” os) pd Sata an summarize, then perform\\n\\nsentiment analysis.\\n\\nSummary LLM Sentiment LLM\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\n© databricks\\n\\nAcademy\\n\\nFinding Success with Generative Al:\\n\\nLakehouse Al\\n\\nDatabricks Academy 2023\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nDelivering business value from Gen Al is challenging. How do we...?\\n\\nfs Customize LLMs with 4a our data\\n\\nFSR Ensure LLMs deliver\\n\\nhigh quality answers\\n\\n: | Deploy LLMs without te O\\n\\nMaintain flexibility to new infrastructure O upgrade LLMs\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\nce securely connect our Integrate LLMs with data to LLMs g data governance\\n\\nLakehouse Al — a data-centric Al Platform\\n\\nDatasets Models Applications\\n\\nData Use Existing Model Collection and Model or Build serving and\\n\\nPreparation Your Own Monitoring\\n\\nUNITY CATALOG\\n\\n<i.\\n\\nLakehouse Al — optimized for Generative Al\\n\\nDatasets Models Applications\\n\\nCurated AI Models MLflow AI Gateway\\n\\nAutoML for Model Serving\\n\\nMlflow Evaluation Lakehouse Monitoring\\n\\nData Collection Use Existing Model\\n\\nModel Serving and Preparation or Build Your Own\\n\\nand Monitoring\\n\\nUNITY CATALOG\\n\\n<i.\\n\\nLakehouse Al capabilities\\n\\nUse Existing Model\\n\\nUnity Catalog + Features ———» | Or Build Your Own\\n\\n~ mijlow Features Delta Lake i sal Indexes ————> Indexes Models AI ml flow . —— MLFlow Prepare Enea Assets packaging Data Agents Serve Data Features Features Indexes Indexes ————> SQL Spark\\n\\nMonitor Data & Al\\n\\nDelta Live Tables\\n\\nGovernance & Lineage\\n\\nt— Metrics\\n\\nData Storage\\n\\nLakehouse Monitoring\\n\\nFeatures ————\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nAI\\n\\n“Assets\\n\\nw———— Logs\\n\\nServing in production\\n\\nModel Serving MLflow Al Gateway\\n\\nAPIs BI / SQL\\n\\n—@\\n\\nETL / streaming pipelines\\n\\noS\\n\\n43\\n\\nLakehouse Al works for all Al models\\n\\nClassic, deep, proprietary or open source Generative Al + LLMs\\n\\nDeep Classical ML Proprietary Open source Chains & learning algorithms LLMs generative Al agents models + LLMs\\n\\nOP mosaic™ 2, yTorch e (GJchatert — Li mos Re . LangChain tT ANTHROP\\\\C (©), Hugging Face TensorFlow XGBoost AA Gopenat stabilityv.ai Stable Diffusion txtal\\n\\n\\n\\nPaLM 2\\n\\nohh\\n\\nPick the best model for your use case\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nLLMOps, unified with DataOps + MLOps\\n\\nLLM Operations for end-to-end production\\n\\nDatabricks unifies LLMOps with traditional MLOps & DevOps\\n\\nTeams need to learn mental model of how LLMs coexist with traditional ML in operations\\n\\nDifferences to MLOps\\n\\nInternal/External Model Hub Fine-Tuned LLM\\n\\nVector Database\\n\\nModel Serving\\n\\nHuman Feedback in Monitoring & Evaluation\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nSource control\\n\\np release Cut release branch | Pull from release branch to production\\n\\nmiflow Unity Catalog ‘ Stage: None Stage: Staging Stage: Production ‘ Y ‘ oe ae Be _ULM API Request ay Fine-Tune LLM Vector Database Continuous Monitoring & Evaluation Update Deployment (CD) Ss | Q Pe os i — ve iii Internal/External Data tables Vector Database Metric Tables Human Feedback\\n\\nModel Hub\\n\\nLakehouse Al: A Data-Centric Al Platform Al = Generative Al, LLMs & Machine Learning\\n\\nSeparate AlPlatform | ManyAltools+ § —_—_ Lakehouse Al\\n\\n+ Data Platform | Data Platform a | * : “ | v | Unified data & Al governance Separate governance Some tools don’t have governance _ Centralized search and discovery | ~ | x | J — Data &Al Separate search interfaces _ Some tools don\\'t have search a x x Unified toolkit across data & Al Separate data / Al tools | Separate data / Al tools v ; x , x single =) of your data Copy of data in each platform Copy of data in each tool J Unified, automated lineage tracking ~ “s J ’ Only within each platform Not provided Performance and scale Jf J J . ~ xX Integration cost Costly effort to integrate platform Stitch together 10s of tools J\\n\\n©2023 Databricks Inc. — All rights reserved g 46\\n\\n© databricks Academy\\n\\nFinding Success with Generative Al:\\n\\nAl Adoption Preparation\\n\\nDatabricks Academy 2023\\n\\nHow to Prepare for Al Revolution Key Steps to Embrace the Al Revolution\\n\\n¢ Act with urgency to lead your organization in this watershed moment of Generative Al.\\n\\n¢ Understand Al fundamentals to identify business use cases. ¢ Develop a strategy for data and Al within your organization.\\n\\n¢ Identify the highest value use cases requiring LLMs.\\n\\n¢ Invest in innovation and create an organizational culture that embraces experimentation.\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\nHow to Prepare for Al Revolution Key Steps to Embrace the Al Revolution\\n\\n¢ Train people to promote Al-driven initiatives, consider reskilling / upskilling employees to work with Al effectively.\\n\\n¢ Address ethical and legal consideration. Stay informed about emerging ethical guidelines and regulations related to Al.\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nStrategic Roadmap for Al Adoption\\n\\nFormulate a strategy on how you will successfully integrate this technology into your business landscape\\n\\n() People & Adoption\\n\\ne Refine roles and responsibilities Operations & Monitoring C4) 2 tt Ek e Align your operation model Automation Gather feedback, continues interactive improvements\\n\\n@) Design & Architecture\\n\\ne Choose the right Al model architecture\\n\\n| e Integrate developed model into Business Use Cases @) a> existing business systems e Identify business objectives e Research use-cases and prioritize high value use cases e Data availability and alignment with use cases @ Define Gen Al Strategy Identify Al strategy Engage business units\\n\\nSetup ethical and legal policies Define success criteria\\n\\nOrganization\\'s Strategy & Mission How Al can be used for achieving or accelerating business objectives?\\n\\ng\\n\\n©2023 Databricks Inc. — All rights reserved | Confidential and pro\\n\\nWe are here to help you!\\n\\nDatabricks resources to help you get started\\n\\n& eS O\\n\\nProfessional Services Upskilling Your Team Solution Accelerators e Deliver customer e Upskill your team with e Jump-start your data specific Generative Al Databricks Academy and Al use cases using use cases » Work with Customer our purpose-built « Advising on building Enablement Specialists guides with LLMs to identify the most e Go from idea to proof of content and offerings little as two weeks\\n\\n(Self-paced, ILT, Private)\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\n© databricks Academy\\n\\nPotential Risks and Challenges\\n\\nDatabricks Academy 2023\\n\\nRisks and Challenges\\n\\nGenerative Al brings new risks and challenges for businesses and society\\n\\n¢ Legal issues\\n\\n1 Not exhaustive e 2) a | Va CY Challenging\\n\\n~\\\\_ Energy use and\\n\\n/\\\\\\\\ environmental harm Capabilty overhang\\n\\n° Security\\n\\n° Intellectual property protection Uy Bestouens ean , Copyright infringement Ease of mitigation © | Lack of a truth\\n\\ne e ° Ethical issues @| function ; Sophisticated phishing E] \" Leaks of proprietary data A and frai : ° Bias : ° e e f ° Simple M ISINTOrM at lon Low risk Severity of business risk amg NisK:\\n\\n¢ Social/Environmental issues ° Impact on workforce\\n\\n° Impact on the environment\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\n© databricks\\n\\nAcademy\\n\\nAssessing Potential Risks and Challenges:\\n\\nLegal Considerations\\n\\nDatabricks Academy 2023\\n\\nData Privacy in Generative Al\\n\\n¢ Current models don\\'t have “forgetting” feature for personal data.\\n\\n¢ Models are trained on large amounts of data, which may include personal information. This might violate a person’s privacy rights.\\n\\n¢ Businesses may be responsible for any violations resulting from use of Generative Al.\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nData Privacy in Generative Al\\n\\nConsiderations\\n\\nUse your existing data privacy strategy as the building block for your privacy in Al strategy.\\n\\n¢ Before using proprietary Off-Shelf Services:\\n\\n° What type of data will be collected?\\n\\nDefine what types of consent or\\n\\n¢ Will your data be used for training permission you may need.\\n\\nmodel or shared with 3rd parties?\\n\\n¢ Employee training - Do you have data lineage that enables\\n\\nof model develooment if needed? ¢ How can/can’‘t use GenAl tools? P gs ° Is user interaction history stored? Is It ° Violation plan secure?\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nData Privacy in Generative Al\\n\\nConsiderations\\n\\nData privacy best-practices: ¢ Ensure proper data anonymization, encryption, and access controls\\n\\n° Implement safeguards to access or disclosure of sensitive data during training/storage/inference\\n\\n¢ Establish data and model governance; version control, monitoring, auditing, data usage policy etc.\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\nData Security in Generative Al Data Leakage\\n\\nGenAl models have potential to jeaked memorize and reproduce training data. i\\n\\n_ yoyee> GP What if training data or prompt agamnsund me Chat includes sensitive or confidential compan’\\n\\ndata? nappe®\\n\\n8 Bans Stag ChatGpr Data Lea oe S AI Use After Spott; mn\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nData Security in Generative Al\\n\\nPrompt Injection\\n\\nDefinition: inserting a speci eet 1)\\n\\ninstruction or prompt within the input text to manipulate the normal i) aac caine un ene MLR aE\\n\\ne piracy. It is important to respect copyright laws behavior of LLMs.\\n\\nand engage in legal methods of acquiring content.\\n\\n¢ Other prompt injection cases: —— i lS . . are against copyright laws? * Generating malicious code » Instructing agent to give wrong Kap) q cereciryt 1 cn provide you uth 9 ist of information infringing or illegal content. It is important to\\n\\navoid these websites to comply with copyright laws and support content creators:\\n\\nRevealing confidential information\\n\\n1.0 XXXXXXX 2. XXXXXXX 3. XXXXXXX\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\nData Security in Generative Al\\n\\nEasy to facilitate fraud, censorship, surveillance, cyber attacks\\n\\n¢ GenAl can be used to access or generate harmful Technology vical “iby Hemi evi C O Nn t € Nn t ° At the start of the week, Liam Porr had only heard of GPT-3. By the end, the college student had used the AI model to produce an entirely fake blog under ¢ Potential security threats of LLMs”: a, It was meant as a fun experiment. But then one of his posts reached the ° ele, e ° number-one spot on Hacker News. Few people noticed that his blog was * Discover vulnerabilities and generate exploits for them completely Al-generated. Some even hit “Subscribe”\\n\\ne Automated fraud or scam attacks\\n\\nSource: MIT Technology Review\\n\\nPersonalized social engineering attacks\\n\\n: : 50 = the japan times Q Code generation tools might generate malicious code :\\n\\nNATIONAL\\n\\nChatGPT can be tricked to write malware if acting in developer mode\\n\\nEasy access to content for planning attacks or violence\\n\\nSource: OpenAl (2023)\\n\\nSource: The Japan Times\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nIntellectual Property Protection\\n\\n¢ GenAl models might be trained on proprietary or copyrighted data.\\n\\n¢ GenAl models and datasets, like other software, are subject to licenses that will tell you how you can or cant use the model or dataset.\\n\\n¢ GenAl models might have terms for not using output of the model for commercial purposes or creating a product competing with them.\\n\\nConsiderations:\\n\\n¢ Arrange legal agreements to protect intellectual property and ensure the output of the models is used appropriately.\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nLitigation and/or other Regulatory Risks\\n\\nExisting laws still apply to new and emerging technologies.\\n\\n¢ Automated-decision making processes that\\n\\nSource: The Brussels Times\\n\\ncauses bias or discrimination may subject the Belgian man dies by\\n\\n° suicide following developer or deployer to QUIENES) actions exchanges with chatbot or litigation - for example, in the employment Tuesday, 28 March 202 space.\\n\\n¢ Claiming a model or algorithm has certain functionality or results may trigger deceptive trade practices regulatory actions.\\n\\n¢ Products liability may also give rise to litigation. Th CharT cling streets hare\\n\\nA young Belgian man recently died by suicide after talking to a chatbot named ELIZA for several weeks, spurring calls for better protection of citizens and\\n\\n©2023 Databricks Inc. — All rights reserved the need to raise awareness.\\n\\nActive Regulatory Area\\n\\n° Al, similar to other emerging technologies, is subject to both existing and newly proposed regulations. ¢ A few examples of proposed Al regulations: ¢ EU Al Act ° US Algorithmic Accountability Act 2022 ¢ Japan Al regulation approach 2023 ¢ Biden-Harris Responsible Al Actions 2023 ¢ California Regulation of Automated Decision Tools\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\n© databricks\\n\\nAcademy\\n\\nAssessing Potential Risks and Challenges:\\n\\nEthical Considerations\\n\\nDatabricks Academy 2023\\n\\nFairness and Bias in Data Big data != Good data (Size doesn’t guarantee quality)\\n\\nHuman bias in data: Sentiment Across Models ,\\n\\n¢ Biases related to social perceptions, stereotypes, and historical factors\\n\\nSentiment Score\\n\\n¢ Stem from preconceived notions, cultural influences, and past experiences\\n\\n20 350M 760M 1.3B 2.7B 6.7B 13B 175B Model Size\\n\\n¢ Outdated data doesn’t capture social view changes Source: Brown etal 2020\\n\\n¢ Examples: stereotypical bias, historical unfairness, and implicit associations\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\nFairness and Bias in Data Big data != Good data (Size doesn’t guarantee quality)\\n\\nAnnotated human bias in data collection and annotation:\\n\\n« Models use annotated or fine-tuned with human feedback\\n\\n¢ This bias type reflect errors or limitations in human judgment and reasoning\\n\\n¢ Examples: Sampling error, Confirmation bias, Anecdotal fallacy.\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nBias Reinforcement Loop\\n\\nA loop between biased input and output\\n\\n—— ———_> a ®@-®\\n\\nTraining Data Al Model Learn from Model Generate Bias People Learn / Biased Data Decide Human bias in data Models learn biases present Models generate toxic, People learn and use biased in the training data. biased or discriminatory data > This is used as new outputs. data\\n\\nModel hallucinate\\n\\n| Reinforce existing bias |\\n\\nFeedback Loop\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\nReliability and Accuracy of Al Systems\\n\\nLLMs tend to hallucinate\\n\\n¢ Hallucination: Phenomenon when the model generates outputs that are plausible-sounding but inaccurate or nonsensical responses due to limitations in understanding.\\n\\n¢ Hallucination become dangerous when;\\n\\nModels become more convincing and people rely on them more\\n\\nModels lead to degradation of information quality\\n\\n©2023 Databricks Inc. — All rights reserved Source: Jiet al 2022, OpenAl (2023) g\\n\\nReliability and Accuracy of Al Systems\\n\\nLLMs tend to hallucinate\\n\\nTwo types of model hallucination:\\n\\nIntrinsic hallucination Extrinsic hallucination\\n\\nSource: Source:\\n\\nThe first Ebola vaccine was approved by the FDA in Alice won first prize in fencing last week.\\n\\n2019, five years after the initial outbreak in 2014.\\n\\nSummary output: Output:\\n\\nThe first Ebola vaccine was approved in 2021. Alice won first prize fencing for the first time last week\\n\\nand she was ecstatic.\\n\\n©2023 Databricks Inc. — All rights reserved Source: Set al 2022\\n\\nReliability and Accuracy of Al Systems\\n\\nAlgorithmic bias in Al systems\\n\\n¢ Generative Al models can produce family politics biased or stereotypical results ,\\n\\n¢ Lack of transparency of input data\\n\\nstory\\'s topic probability\\n\\n¢ Difficult to trace-back to original input data\\n\\n¢ Limited fact-checking process\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\nHow to Address Ethical Issues\\n\\nControls need to be incorporated at all levels\\n\\n. Assess data slices. | Update data.\\n\\nToxic, discriminatory, exclusive model\\n\\ne Curate data for fine- tuning e Fine-tune your model\\n\\n(Mis)information hazard\\n\\nMalicious uses —_—_—_P Regulation\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nHow to Address Ethical Issues\\n\\nRegulations need to incorporated at all levels D ‘ . Assess data slices. aes J Update data.\\n\\nToxic, discriminatory, exclusive model\\n\\ne Curate data for fine- tuning e Fine-tune your model\\n\\n(Mis)information hazard\\n\\nWY Malicious uses Regulation\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\nAuditing Generative Al Models\\n\\nAllocating responsibility and increasing model transparency\\n\\nGovernance Audit\\n\\n+ Model access\\n\\nIntended/prohibited use cases\\n\\n} Application | Audit\\n\\nOutput logs « Environmental data\\n\\nSource: Mokander et al 2023\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\n© databricks\\n\\nAcademy\\n\\nAssessing Potential Risks and Challenges:\\n\\nHuman-Al Interaction\\n\\nDatabricks Academy 2023\\n\\nHow will Al Impact Society\\n\\nImpact on the workforce\\n\\nPro Arguments\\n\\n¢ Personalization: Enables personalized .\\n\\nexperiences in our life\\n\\n¢ Automation and Efficiency: Al will be used for repetitive tasks > Increased efficiency and higher productivity\\n\\n¢ Accessibility: GenAl making technology more inclusive and accessible by generating alternative formats, providing real-time translations, and assisting individuals with disabilities\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nCounter Arguments\\n\\nJob Displacement: Al automation may lead to job losses or displacement of workers > economic inequalities and unemployment\\n\\nEthical Concerns: Entrench existing discrimination and biases.\\n\\nOverreliance: The increased trust and reliance on Al systems may lead to unnoticed mistakes and loss of important skills\\n\\nPrivacy & Security: Privacy concerns, cyber threats and malicious attacks, Al being used for political goals\\n\\ng\\n\\nAl and Workforce\\n\\nPotential impact of generative Al on workforce\\n\\ne Arou nd 80% of the U.S. workforce Fastest growing vs. fastest declining jobs\\n\\nTop 10 fastest growing jobs\\n\\nmay witness a minimum of 10% of their work responsibilities influenced by LLMs.*\\n\\nAl and Machine Learning Specialists Sustainability Specialists\\n\\nBusiness Intelligence Analysts Information Security Analysts Fintech Engineers\\n\\nData Analysts and Scientists Robotics Engineers\\n\\nBig Data Specialists\\n\\n8 | MS | SE | Bes | =\\n\\n¢ High-wage occupations are likely to expose more.”\\n\\nAgricultural Equipment Operators\\n\\n10. Digital Transformation Specialists\\n\\nWorld Economic Forum, Future of Jobs Report 2023.\\n\\nSource: Eloundou, T. Manning, S., Mishkin, P. & Rock, D. (2023)\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\n2.\\n\\n3\\n\\nWORLD ECONOMIC FORUM\\n\\nSo\\n\\nTop 10 fastest declining jobs\\n\\nBank Tellers and Related Clerks\\n\\nPostal Service Clerks\\n\\nCashiers and ticket Clerks\\n\\nData Entry Clerks\\n\\nAdministrative and Executive Secretaries Material-Recording and Stock-Keeping Clerks Accounting, Bookkeeping and Payroll Clerks Legislators and Officials\\n\\nStatistical, Finance and Insurance Clerks\\n\\nDoor-To-Door Sales Workers, News and Street Vendors, and Related Workers\\n\\nAl at Workplace\\n\\nGenerative Al and productivity\\n\\n¢« Around 60% of CEOs and CFOs plan to use Al and automation more.*\\n\\n¢ Accessing to Gen. Al tools increases productivity by 14% on average.**\\n\\ne Novice - and less-skilled workers benefits more\\n\\n¢ Companies see Al training as one of the highest strategic priorities from now until 2027.***\\n\\nSource: Brynjolfsson, E., Li, D. & Raymond, L. (2023) , **Source: Mercer Survey, *** Source: World Economic Forum\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nAl at Workplace\\n\\nInteracting with Al agents\\n\\n¢ Prompt Engineering: Designing and — crafting effective prompts or MESHING BSEIs instructions for generating desired\\n\\noutputs from a language model. % ° Prompt quality influence the quality and relevance of generated response\\n\\nWORLD ECONOMIC FORUM\\n\\n° Clear and intuitive prompts of workers’ core skills . are expected to change ¢ Soon most of the software we use will in the next five years\\n\\nintegrate Gen. Al features. Training employees to be able to leverage these tools is going to be critical.\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\n© databricks Academy\\n\\nGenerative Al Fundamentals:\\n\\nsummary and Next Steps\\n\\nDatabricks Academy 2023\\n\\n~ databricks\\n\\n©2023 Databricks Inc. — All rights reserved', metadata={'source': '../data/generative-ai-fundamentals-v1.pdf'})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unstructured_pdf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba5fc08b-6eec-488a-8671-b36230fe03c2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'© databricks Academy\\n\\nGenerative Al Fundamentals\\n\\nDatabricks Academy 2023\\n\\nAE ad PO TOL\\n\\nSe Everyone Asks\\n\\na Z Yj @ at 3a tei UD a i Mam is Generative 4 {+ How exactly | 4 om = Y ‘waa How can l use Prag 9 Alathreat or canluse : ~ y, | ‘ ; my data an Generative Al | = wo yt.» securely with |\\n\\ns \\\\ opportunity to gaina 7 “\\\\ Generative\\n\\nBy for my U), competitive Hh Al?\\n\\nbusiness? es advantage? :\\n\\ny) e —~ x\\n\\nsession goals\\n\\nUpon completion of this content, you should be able to:\\n\\nDescribe how generative artificial intelligence (Al) is being used to revolutionize practical Al applications\\n\\nDescribe how Generative Al models works and discuss their potential business uses cases\\n\\nDescribe how a data organization can find initial success with generative Al applications\\n\\nRecognize the potential legal and ethical considerations of utilizing generative Al for applications and within the workplace.\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\nAGENDA\\n\\nO01. Introducing Generative Al\\n\\nGenerative Al Basics\\n\\nLLMs and Generative Al\\n\\nO02. Finding Success with Generative Al LLM Applications\\n\\nGenerative Al with Databricks ML\\n\\nAl Adoption Preparation 03. Assessing Potential Risks and Challenges Legality\\n\\nEthical Considerations\\n\\nHuman-aAl Interaction\\n\\n© databricks\\n\\nAcademy\\n\\nIntroducing Generative AI:\\n\\nGenerative Al Basics\\n\\nDatabricks Academy 2023\\n\\nWhat is Generative Al?\\n\\nDeep Learning (DL)\\n\\nGenerative Al\\n\\nile ae\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nArtificial Intelligence:\\n\\nA multidisciplinary field of computer science that aims to create systems capable of emulating and surpassing human-level intelligence.\\n\\nMachine Learning:\\n\\nLearn from existing data and make predictions/prediction without being explicitly programmed.\\n\\nDeep Learning:\\n\\nUses “artificial neural networks” to learn from data.\\n\\ng\\n\\nWhat is Generative Al?\\n\\nDeep Learning (DL)\\n\\nGenerative Al\\n\\nal o\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nGenerative Artificial Intelligence:\\n\\nSub-field of Al that focuses on generating new content such as:\\n\\n¢ Images\\n\\n¢ Text\\n\\n¢ Audio/music « Video\\n\\n¢ Code\\n\\n¢ 3D objects\\n\\n¢ Synthetic data\\n\\nGenerative Models\\n\\nA branch of ML modeling which mathematically approximates the world\\n\\nData objects Deep Neural Network Tasks\\n\\n¢ Synthetic image ° Bird - generation\\n\\n[0.5, 14, -1, ..] ¢ Style transfer / edit\\n\\nTranslation Question Answering « Semantic search\\n\\n| “i\\n\\n[0.8, 1.4, -2.3, ....]\\n\\nSpeech-to-text Music transcription\\n\\nA\\n\\nq) —\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\n[1.8, 0.4, -1.5, ....]\\n\\nWhy Now?\\n\\nFactors making Generative Al possible now\\n\\nLarge Datasets\\n\\ne Availability of large and diverse datasets Al models learn patterns, correlations, and characteristics of large datasets Pre-trained state-of-the-art models\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nWhy Now?\\n\\nFactors making Generative Al possible now\\n\\nae\\n\\nLarge Datasets Computational Power e Availability of large and e Advancements in diverse datasets hardware; GPUs e Al models learn e Access to cloud patterns, correlations, computing and characteristics of e Open-source software,\\n\\nlarge datasets Hugging Face\\n\\ne Pre-trained state-of-the-art models\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nWhy Now?\\n\\nFactors making Generative Al possible now\\n\\nae\\n\\nLarge Datasets Computational Power\\n\\ne Advancements in hardware; GPUs\\n\\ne Access to cloud computing\\n\\ne Open-source software, Hugging Face\\n\\ne Availability of large and diverse datasets\\n\\ne Al models learn patterns, correlations, and characteristics of large datasets\\n\\ne Pre-trained state-of-the-art models\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\n&\\n\\nInnovative DL Models\\n\\ne Generative Adversarial Networks (GANs)\\n\\ne Transformers Architecture\\n\\ne Reinforcement learning from human feedback (RLHF)\\n\\nWhy should | care now?\\n\\nML/AI has been around for a while, why it matters now\\n\\nGenerative Al models’ accuracy and\\n\\neffectiveness have hit a tipping point Chegg shares drop more than\\n\\n40% after company says\\n\\n¢ Powerful enough to enable use cases not ChatGPT is killing its business\" feasible even a year ago ¢ Economical enough for use even by 50910212023\\n\\nnon-technical business users\\n\\nGenerative Al models and tooling are readily available\\n\\ne Many models are open source and customizable\\n\\ne Requires powerful GPUs, but are available in the cloud\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\nGenerative Al Use Cases\\n\\nIntelligent conversations, creative text creation, code generation\\n\\n¢ Content generation ¢ Question/answers\\n\\n¢ Virtual assistants ¢ Content personalization ¢ Language style transfer\\n\\n¢ Story telling, poetry, creative writing ¢ Translation\\n\\n¢ Code generation / auto-completion\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nHey! | am giving a presentation about Generative Al in Data +\\n\\nAl Summit. Write a short funny text to convince audiences\\n\\nthat Generative Al is the future. Max 100 words.\\n\\nStep into the future with Generative AI! It\\'s not just about flying cars and robot butlers. This mind-boggling technology can compose symphonies, craft witty jokes, and design cutting-edge fashion trends. It pushes the limits of human creativity, conjuring up entire worlds, unique characters, and mind-bending art. But it doesn\\'t stop there. Generative AI revolutionizes industries too, discovering new drugs and predicting market trends. So, get ready to be amazed. Embrace the future, where imagination knows no bounds, and Generative AI becomes our ultimate creative companion. The\\n\\nfuture is here, and it\\'s hilariously amazing!\\n\\ng\\n\\nGenerative Al Use Cases\\n\\nVisual/audio/3D generation: Unlocking creativity\\n\\nImage generation Video Synthesis a e Generate realistic/artistic Sy e Animation\\n\\nhigh-quality images e Scene generation e Virtual agent generation\\n\\n3D Generation Audio Generation\\n\\ne Object, character generation ZS) e Narration e Animations ae e Music composition\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nGenerative Al Use Cases\\n\\nsynthetic data generation\\n\\n¢ Synthetic dataset generation ° Increase size, diversity of dataset ° Privacy protection ° Simulate scenarios ° Fraud detection, network attack detection\\n\\n¢ Synthetic data for computer vision (e.g. autonomous Cars)\\n\\n° Object detection\\n\\n« Adversarial scenarios (weather, road condition)\\n\\n¢ Synthetic text for natural language processing\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nGenerative Al Use Cases\\n\\nGenerative design: Discover drugs, design unique systems\\n\\n¢ Drug discovery ¢ Product and material design\\n\\n¢ Chip design\\n\\n¢ Architectural design and urban planning\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\n© databricks Academy\\n\\nIntroducing Generative AI: Generative Al and LLMs\\n\\nDatabricks Academy 2023\\n\\n“Vicuna: an open-source chatbot impressing GPT-4 with 90%* ChatGPT quality”\\n\\n03/30/2023\\n\\n“GPT-4 beats 90% of lawyers trying to pass the bar”\\n\\nForbes 03/14/2023\\n\\n©2023 Databricks Inc. — All rights reserved | Confidential and proprietary\\n\\n“Smaller, more performant models such as LLaMA enable... further democratizing access in this important, fast-changing field...”\\n\\nOO Meta Al 02/24/2023\\n\\n“Falcon is now free of royalties for commercial and research use... Falcon 40B outperforms ... Meta’s LLaMA and Stability Al\\'s StableLM”\\n\\n05/31/2023\\n\\nLLMs are not hype—they change the Al game\\n\\nGenerative Al & LLMs are a once-in-a-generation shift in technology\\n\\nWhat is a LLM?\\n\\n: Large Language Model (LLM): Generative Al Model trained on massive datasets to achieve\\n\\nadvanced language processing capabilities\\n\\nBased on deep learning neural networks\\n\\nLarge Language Models (LLMs)\\n\\nFoundation Model:\\n\\nLarge ML model trained on vast amount of data & fine-tuned for more specific language understanding and generation tasks\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\nHow Do LLMs Work?\\n\\nA simplified version of LLM training process\\n\\n>. co Input > Tokenize Token Embeddings Shape Sea laae< Output Text\\n\\n(Encode text into numeric rep.) (Put words with similar meaning close in vector space)\\n\\nI\\n\\nI\\n\\nI\\n\\nDolly is a language model that can help Embedding Functions you generate text for a variety of tasks. (Pre-trained model) I 1\\n\\nI I v\\n\\nWikipedia\\n\\nDolly is a language model that can help you generate text for a variety of tasks.\\n\\n[0.2, 1.5, O06 .... 0.6]\\n\\nScientific Research Pre-Trained\\n\\nTransformer Model\\n\\nWhen done well, similar words will be [35, 5098, 318, 257, 3303, 2746, 326, 460, closer in these embedding/vector 1037, 345, 7716, 2420, 329, 257, 4996, 286, spaces. Example 2D representation; 8861, 13]\\n\\nPredicted next word\\n\\nCrawled data from the Internet\\n\\niN\\n\\nCustom Curated Billions of parameters Datasets ...\\n\\nTokens: 18, Characters: 81 (100 tokens ~= 75 words)\\n\\nEncoding Decoding\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\nAn Overview of Common LLMs\\n\\nOpen-source and Closed LLMs\\n\\nModel ormodel Model size License Created by Released family (# params) Falcon 7B-40B Apache 2.0 Technology 2023 A newer potentially state-of-the-art model Innovation Institute MPT 7B Apache 2.0 MosaicML 2023 Comes with various models for chat, writing etc. Dolly 12B MIT Databricks 2023 Instruction-tuned Pythia model Pythia 19M-12B Apache 2.0 EleutherAl 2023 Series of 8 models for comparisons across sizes GPT-3.5 175 B proprietary OpenAl 2022 ChatGPT model option; related models GPT-1/2/3/4 BLOOM 560 M - 176 B RAIL v1.0 BigScience 2022 46 languages FLAN-T5 80 M - 540 B Apache 2.0 Google 2021 methods to improve training for existing architectures BART 139 M - 406 M Apache 2.0 Meta 2019 derived from BERT, GPT, others BERT 109 M- 335M Apache 2.0 Google 2018 early breakthrough\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nFor up-to-date list of recommended LLMs: https://www.databricks.com\\n\\nPlease note: Databricks does not endorse any of these models - you should evaluate these if they meet your needs.\\n\\nLLMs Generate Outputs for NLP Tasks\\n\\nCommon LLM tasks\\n\\nweg\\n\\n</>\\n\\nContent Creation and Augmentation\\n\\nSummarization Question Answering Machine Translation\\n\\nClassification\\n\\nNamed Entity Recognition (NER)\\n\\nTone / Level of content\\n\\nCode generation\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nGenerating coherent and contextually relevant text. LLMs excel at tasks like text completion, creative writing, story generation, and dialogue generation.\\n\\nSummarizing long documents or articles into concise summaries. LLMs provide an efficient way to extract key information from large volumes of text.\\n\\nComprehend questions and provide relevant answers by extracting information from their pre-trained knowledge.\\n\\nAutomatically converting a text from one language to another. LLMs are also capable to explain language structure such as grammatical rules.\\n\\nCategorizing text into predefined classes or topics. LLMs are useful for tasks like topic classification, soam detection, or sentiment analysis.\\n\\nIdentifying and extracting named entities like names of persons, organizations, locations, dates, and more from text.\\n\\nAdjusting the text\\'s tone (professional, humorous, etc.) or complexity level (e.g., fourth-grade level).\\n\\nGenerating code in a specified programming language or converting code from one language to another.\\n\\ng\\n\\nLLMs Business Use Cases\\n\\nCustomer Engagement\\n\\n¢ Personalization and customer segmentation: provided data?\\n\\nWhat are the top 5 customer complaints based on the\\n\\nProvide personalized product/content recommendation based on customer nactoten complaints arenes follower behaviour and preferences 1. Shipping Delays - 25% of customers expressed\\n\\nfrustration with delayed deliveries. 2. Product Quality - 20% of customers reported\\n\\ne Feed back Analysis issues with the quality of the received\\n\\nproducts.\\n\\n. . 3. Customer Service Responsiveness - 18% of\\n\\ne Virtual assista nts customers felt that the response time from customer service was slow.\\n\\n4. Billing and Payment Errors - 15% of customers encountered errors and discrepancies in their bills and payments.\\n\\n5. Order Inaccuracies - 12% of customers received incorrect or incomplete orders.\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\nLLMs Business Use Cases\\n\\nContent Creation\\n\\nin . ole . . yoo ¢ Creative writing: Short stories, creative jaunches on” | . : icnatGPt oks 0” ae a, pEUTER? narratives, scripts etc. carter PS = ¢ Technical writing: Documentation, user . ope “4 manuals, simplifying content etc. a, alre , . a. “Online re) Iting p ¢ Translation and localization Cipes» © 200ks, Websip ° eo. ° ° > Iles ¢ Article writing for blogs/social media be Wass, Noten 2 NZ) will ChatGPT supplant ws writers, thinkers?” irvard Gazette\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\nLLMs Business Use Cases\\n\\nProcess automation and efficiency\\n\\nCheck customer order history,\\n\\nretrieve product details > generate personalized message.\\n\\n¢ Customer support augmentation =\\n\\nfreeform text\\n\\nand automated question answering lam angry! Your [Product Name] is a Customer Data\\n\\ncomplete disaster. It\\'s cheaply made, Order Data falling apart after just a few uses. It\\n\\n¢- Automated customer response eee eee\\n\\nresolution - either replace it with a\\n\\nO working product or refund my money ° Et Y 1a il immediately. This is unacceptable, and | won\\'t tolerate such poor quality. Fix this\\n\\nn n n now, or I\\'ll take my business elsewhere ° Soc | a | aa ed | a, p rod U ct revi ews and spread the word about your shoddy products.\\n\\n¢ Sentiment analysis, prioritization Expecting immeciate action\\n\\nSuggested Automated Message\\n\\nDear Jack Doe,\\n\\nWe understand your frustration with the TV you purchased. It seems that there are widespread issues with the manufacturing of these TVs (10% deficiency), affecting their performance. We apologize for any inconvenience caused and recommend submitting a refund request through\\n\\nthe following link: [Refund Request Form]. Rest assured, we are actively working with the manufacturer to address these problems. Thank you for your patience and understanding.\\n\\nSincerely,\\n\\nCustomer Support\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\nLLMs Business Use Cases\\n\\nCode generation and developer productivity\\n\\nsentiments.ts\\n\\n#!/usr/bin/env ts-node\\n\\n¢ Code completion, boilerplate code port { fetch } from \"fetch\"\\n\\ngeneration ee - Error detection and debugging iphone - Convert code between languages __—_\\n\\n¢ Write code documentation sora\\n\\n}\\n\\n¢ Automated testing\\n\\n@ max_sum-slice.py\\n\\n¢ Natural language to code generation eiting ghaetnentin tt\\n\\n¢ Virtual code assistant for learning to code\\n\\nImage Source: Github g\\n\\n© databricks\\n\\nAcademy\\n\\nFinding Success with Generative Al:\\n\\nLLM Applications\\n\\nDatabricks Academy 2023\\n\\nModeling techniques quickly commoditize...\\n\\nSaaS LLM models prices dropping exponentially (10X decrease YoY)\\n\\nHigh quality open-source Generative Al unlocks the models now available value of *your* data\\n\\nBuild the Al apps only\\n\\nyou can build :\\n\\nLLM Flavors\\n\\nThinking of building your own modern LLM application?\\n\\n$54 o = Mla Lae Open-Source Models Proprietary Models\\n\\n« Use as off-the-shelf or « Usually offered as\\n\\nfine-tune LLMs-as-a-service e Provides flexibility for e Some can be fine-tuned\\n\\ncustomizations e Restrictive licenses for « Can be smaller in size to usage and modification\\n\\nsave cost e Commercial / Non-commercial use\\n\\nOpen-source LLMs: Proprietary LLMs:\\n\\nNon-commercial Use Commercial Use ANTHROP\\\\C AA Gopenat OU Meta Al & databricks {\\\\ mosaic + S * LLaMA Dolly MPT + ChatPT PaLM 2\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nChoose the right LLM model flavor\\n\\nThere is no “perfect” model, trade-offs are required.\\n\\nLLM model decision criteria\\n\\n=\\n\\nLS J\\n\\n©):\\n\\nQuality Cost Latency\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nUsing Proprietary Models (LLMs-as-a-Service)\\n\\n¢ Speed of develooment\\n\\nQuick to get started and working.\\n\\nAs this is another API call, it will fit very easily into existing pipelines.\\n\\n¢ Quality\\n\\n« Can offer state-of-the-art results\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nCons\\n\\n° Cost\\n\\nPay for each token sent/received.\\n\\n¢ Data Privacy/Security\\n\\nYou may not know how your data is being used.\\n\\ne Vendor lock-in\\n\\nSusceptible to vendor outages, deprecated features, etc.\\n\\nUsing Open Source Models\\n\\n¢ Task-tailoring\\n\\nSelect and/or fine-tune a task-specific model for your use case.\\n\\ne Inference Cost\\n\\nMore tailored models often smaller, making them faster at inference time.\\n\\n¢ Control\\n\\n° All of the data and model information stays entirely within your locus of control.\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nCons\\n\\n¢ Upfront time investments\\n\\nNeeds time to select, evaluate, and possibly tune\\n\\n¢ Data Requirements\\n\\n° Fine-tuning or larger models require larger datasets.\\n\\n¢ Skill Sets\\n\\nRequire in-house expertise\\n\\nFine Tuned Models\\n\\nWhat is fine-tuning and how it works\\n\\nFine-tuning: The process of further training a pre-trained model on a specific task or dataset to adapt it for a particular application or domain.\\n\\nose Se ee ee ee ee ee ee ee ee ee ee ee ee ee ee ee ee ee ee ee ee ee ee ee ee ee ee\\n\\nLarge corpus of training data\\n\\na\\n\\nSmaller corpus of training data\\n\\n\\n\\n“yyy _ =F fo Fine-tuned Bro ee ————_—> Foundation ———— “Medel. =F ode Model ee ee\\n\\nComputationally expensive process Task specific training\\n\\nwere -,K\\n\\neex=-=-=- ¢\\n\\nModel Fine-Tuning\\n\\na a a el\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\nFine-tuning models\\n\\nFoundation models can be fine-tuned for specific tasks\\n\\n—_— = —_ —_——=— <= -_-=-™ —_—— -_=-™ ——\\n\\nSupervised training = Ess on smaller labeled =F = Question, Answer model model organization model Ls ~ Ls ~ 7 ; Task-specific Question Sentiment pauline fine-tuned models Answering Analysis ue\\n\\nRecognition\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\nFine-tuning models\\n\\nFoundation models can be fine-tuned for domain adaptation\\n\\n=—— = — — =—_—— = — — --7 _--~ --7 _-_\\n\\nSupervised training\\n\\non smaller labeled pound Senne datasets oundation oundation i ves model wo docs model\\n\\nLegal docs\\n\\n—— = = = -_-7 =\\n\\n~= = ~= = -_ = -_ — - -—_ ee = — ==\\n\\nDomain-specific fine-tuned models\\n\\nFinance\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nOpen Source quality is rapidly advancing — while fine tuning cost is rapidly decreasing\\n\\nDolly started the trend to open models with a commercially friendly license\\n\\nFacebook LLaMA Stanford Alpaca Databricks Dolly Mosaic MPT Til Falcon “Smaller, more performant models “Alpaca behaves qualitatively “Dolly willhelp democratize LLMs, “MPT-7B is trained from scratch on “Falcon significantly outperforms such as LLaMA...democratizes _ similarly to OpenAIl ... while being transforming them into a 1T tokens ... is open source, GPT-3 for ... 75% of the training access in this important, surprisingly small and easy /cheap commodity every company can available for commercial use, and compute budget—and ... a fifth of fast-changing field.” to reproduce” own and customize” matches the quality of LLaMA-7B” the compute at inference time.” February 24, 2023 March 13, 2023 March 24, 2023 May 5, 2023 May 24, 2023\\n\\nNon Commercial Use Only | Commercial Use Permitted\\n\\n©2023 Databricks Inc. — All rights reserved | Confidential and proprietary gY\\n\\nMixing LLM Flavors in a Workflow\\n\\nTypical applications are more than just a prompt-response system.\\n\\nTasks: Single interaction (_ with an LLM ( Ne ooeeene\\n\\nDirect LLM calls are just part of a full task/application workflow\\n\\nWorkflow: Applications with more than a single Interaction\\n\\n|=) Task 1\\n\\n(Summarization)\\n\\nWorkflow Initiated\\n\\neee ~\\n\\n-- eter eee ee ee ee ee ee ee ee ee ee ee Ke\\n\\nEnd-to-end workflow\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\nMixing LLM Flavors in a Workflow\\n\\nExample multi-LLM problem: get the sentiment of many articles on a topic\\n\\nArticle 1: “...” Initial solution Article 2:\"...\" Put all the articles together and have the LLM parse it all\\n\\nArticle 3: “...\" . Article 4: “...” Overall Article 5: “...” Sentiment Article 6: “...” Issue\\n\\nArticle 7: “...\" Can quickly overwhelm the model input\\n\\nlength\\n\\naw) Better solution Article 1: “...\\n\\nLa e Article 2: “...” — Summary 1 Overall A two-stage process to first 4 + Summar : : Article 3:\"..” os) pd Sata an summarize, then perform\\n\\nsentiment analysis.\\n\\nSummary LLM Sentiment LLM\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\n© databricks\\n\\nAcademy\\n\\nFinding Success with Generative Al:\\n\\nLakehouse Al\\n\\nDatabricks Academy 2023\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nDelivering business value from Gen Al is challenging. How do we...?\\n\\nfs Customize LLMs with 4a our data\\n\\nFSR Ensure LLMs deliver\\n\\nhigh quality answers\\n\\n: | Deploy LLMs without te O\\n\\nMaintain flexibility to new infrastructure O upgrade LLMs\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\nce securely connect our Integrate LLMs with data to LLMs g data governance\\n\\nLakehouse Al — a data-centric Al Platform\\n\\nDatasets Models Applications\\n\\nData Use Existing Model Collection and Model or Build serving and\\n\\nPreparation Your Own Monitoring\\n\\nUNITY CATALOG\\n\\n<i.\\n\\nLakehouse Al — optimized for Generative Al\\n\\nDatasets Models Applications\\n\\nCurated AI Models MLflow AI Gateway\\n\\nAutoML for Model Serving\\n\\nMlflow Evaluation Lakehouse Monitoring\\n\\nData Collection Use Existing Model\\n\\nModel Serving and Preparation or Build Your Own\\n\\nand Monitoring\\n\\nUNITY CATALOG\\n\\n<i.\\n\\nLakehouse Al capabilities\\n\\nUse Existing Model\\n\\nUnity Catalog + Features ———» | Or Build Your Own\\n\\n~ mijlow Features Delta Lake i sal Indexes ————> Indexes Models AI ml flow . —— MLFlow Prepare Enea Assets packaging Data Agents Serve Data Features Features Indexes Indexes ————> SQL Spark\\n\\nMonitor Data & Al\\n\\nDelta Live Tables\\n\\nGovernance & Lineage\\n\\nt— Metrics\\n\\nData Storage\\n\\nLakehouse Monitoring\\n\\nFeatures ————\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nAI\\n\\n“Assets\\n\\nw———— Logs\\n\\nServing in production\\n\\nModel Serving MLflow Al Gateway\\n\\nAPIs BI / SQL\\n\\n—@\\n\\nETL / streaming pipelines\\n\\noS\\n\\n43\\n\\nLakehouse Al works for all Al models\\n\\nClassic, deep, proprietary or open source Generative Al + LLMs\\n\\nDeep Classical ML Proprietary Open source Chains & learning algorithms LLMs generative Al agents models + LLMs\\n\\nOP mosaic™ 2, yTorch e (GJchatert — Li mos Re . LangChain tT ANTHROP\\\\C (©), Hugging Face TensorFlow XGBoost AA Gopenat stabilityv.ai Stable Diffusion txtal\\n\\n\\n\\nPaLM 2\\n\\nohh\\n\\nPick the best model for your use case\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nLLMOps, unified with DataOps + MLOps\\n\\nLLM Operations for end-to-end production\\n\\nDatabricks unifies LLMOps with traditional MLOps & DevOps\\n\\nTeams need to learn mental model of how LLMs coexist with traditional ML in operations\\n\\nDifferences to MLOps\\n\\nInternal/External Model Hub Fine-Tuned LLM\\n\\nVector Database\\n\\nModel Serving\\n\\nHuman Feedback in Monitoring & Evaluation\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nSource control\\n\\np release Cut release branch | Pull from release branch to production\\n\\nmiflow Unity Catalog ‘ Stage: None Stage: Staging Stage: Production ‘ Y ‘ oe ae Be _ULM API Request ay Fine-Tune LLM Vector Database Continuous Monitoring & Evaluation Update Deployment (CD) Ss | Q Pe os i — ve iii Internal/External Data tables Vector Database Metric Tables Human Feedback\\n\\nModel Hub\\n\\nLakehouse Al: A Data-Centric Al Platform Al = Generative Al, LLMs & Machine Learning\\n\\nSeparate AlPlatform | ManyAltools+ § —_—_ Lakehouse Al\\n\\n+ Data Platform | Data Platform a | * : “ | v | Unified data & Al governance Separate governance Some tools don’t have governance _ Centralized search and discovery | ~ | x | J — Data &Al Separate search interfaces _ Some tools don\\'t have search a x x Unified toolkit across data & Al Separate data / Al tools | Separate data / Al tools v ; x , x single =) of your data Copy of data in each platform Copy of data in each tool J Unified, automated lineage tracking ~ “s J ’ Only within each platform Not provided Performance and scale Jf J J . ~ xX Integration cost Costly effort to integrate platform Stitch together 10s of tools J\\n\\n©2023 Databricks Inc. — All rights reserved g 46\\n\\n© databricks Academy\\n\\nFinding Success with Generative Al:\\n\\nAl Adoption Preparation\\n\\nDatabricks Academy 2023\\n\\nHow to Prepare for Al Revolution Key Steps to Embrace the Al Revolution\\n\\n¢ Act with urgency to lead your organization in this watershed moment of Generative Al.\\n\\n¢ Understand Al fundamentals to identify business use cases. ¢ Develop a strategy for data and Al within your organization.\\n\\n¢ Identify the highest value use cases requiring LLMs.\\n\\n¢ Invest in innovation and create an organizational culture that embraces experimentation.\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\nHow to Prepare for Al Revolution Key Steps to Embrace the Al Revolution\\n\\n¢ Train people to promote Al-driven initiatives, consider reskilling / upskilling employees to work with Al effectively.\\n\\n¢ Address ethical and legal consideration. Stay informed about emerging ethical guidelines and regulations related to Al.\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nStrategic Roadmap for Al Adoption\\n\\nFormulate a strategy on how you will successfully integrate this technology into your business landscape\\n\\n() People & Adoption\\n\\ne Refine roles and responsibilities Operations & Monitoring C4) 2 tt Ek e Align your operation model Automation Gather feedback, continues interactive improvements\\n\\n@) Design & Architecture\\n\\ne Choose the right Al model architecture\\n\\n| e Integrate developed model into Business Use Cases @) a> existing business systems e Identify business objectives e Research use-cases and prioritize high value use cases e Data availability and alignment with use cases @ Define Gen Al Strategy Identify Al strategy Engage business units\\n\\nSetup ethical and legal policies Define success criteria\\n\\nOrganization\\'s Strategy & Mission How Al can be used for achieving or accelerating business objectives?\\n\\ng\\n\\n©2023 Databricks Inc. — All rights reserved | Confidential and pro\\n\\nWe are here to help you!\\n\\nDatabricks resources to help you get started\\n\\n& eS O\\n\\nProfessional Services Upskilling Your Team Solution Accelerators e Deliver customer e Upskill your team with e Jump-start your data specific Generative Al Databricks Academy and Al use cases using use cases » Work with Customer our purpose-built « Advising on building Enablement Specialists guides with LLMs to identify the most e Go from idea to proof of content and offerings little as two weeks\\n\\n(Self-paced, ILT, Private)\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\n© databricks Academy\\n\\nPotential Risks and Challenges\\n\\nDatabricks Academy 2023\\n\\nRisks and Challenges\\n\\nGenerative Al brings new risks and challenges for businesses and society\\n\\n¢ Legal issues\\n\\n1 Not exhaustive e 2) a | Va CY Challenging\\n\\n~\\\\_ Energy use and\\n\\n/\\\\\\\\ environmental harm Capabilty overhang\\n\\n° Security\\n\\n° Intellectual property protection Uy Bestouens ean , Copyright infringement Ease of mitigation © | Lack of a truth\\n\\ne e ° Ethical issues @| function ; Sophisticated phishing E] \" Leaks of proprietary data A and frai : ° Bias : ° e e f ° Simple M ISINTOrM at lon Low risk Severity of business risk amg NisK:\\n\\n¢ Social/Environmental issues ° Impact on workforce\\n\\n° Impact on the environment\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\n© databricks\\n\\nAcademy\\n\\nAssessing Potential Risks and Challenges:\\n\\nLegal Considerations\\n\\nDatabricks Academy 2023\\n\\nData Privacy in Generative Al\\n\\n¢ Current models don\\'t have “forgetting” feature for personal data.\\n\\n¢ Models are trained on large amounts of data, which may include personal information. This might violate a person’s privacy rights.\\n\\n¢ Businesses may be responsible for any violations resulting from use of Generative Al.\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nData Privacy in Generative Al\\n\\nConsiderations\\n\\nUse your existing data privacy strategy as the building block for your privacy in Al strategy.\\n\\n¢ Before using proprietary Off-Shelf Services:\\n\\n° What type of data will be collected?\\n\\nDefine what types of consent or\\n\\n¢ Will your data be used for training permission you may need.\\n\\nmodel or shared with 3rd parties?\\n\\n¢ Employee training - Do you have data lineage that enables\\n\\nof model develooment if needed? ¢ How can/can’‘t use GenAl tools? P gs ° Is user interaction history stored? Is It ° Violation plan secure?\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nData Privacy in Generative Al\\n\\nConsiderations\\n\\nData privacy best-practices: ¢ Ensure proper data anonymization, encryption, and access controls\\n\\n° Implement safeguards to access or disclosure of sensitive data during training/storage/inference\\n\\n¢ Establish data and model governance; version control, monitoring, auditing, data usage policy etc.\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\nData Security in Generative Al Data Leakage\\n\\nGenAl models have potential to jeaked memorize and reproduce training data. i\\n\\n_ yoyee> GP What if training data or prompt agamnsund me Chat includes sensitive or confidential compan’\\n\\ndata? nappe®\\n\\n8 Bans Stag ChatGpr Data Lea oe S AI Use After Spott; mn\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nData Security in Generative Al\\n\\nPrompt Injection\\n\\nDefinition: inserting a speci eet 1)\\n\\ninstruction or prompt within the input text to manipulate the normal i) aac caine un ene MLR aE\\n\\ne piracy. It is important to respect copyright laws behavior of LLMs.\\n\\nand engage in legal methods of acquiring content.\\n\\n¢ Other prompt injection cases: —— i lS . . are against copyright laws? * Generating malicious code » Instructing agent to give wrong Kap) q cereciryt 1 cn provide you uth 9 ist of information infringing or illegal content. It is important to\\n\\navoid these websites to comply with copyright laws and support content creators:\\n\\nRevealing confidential information\\n\\n1.0 XXXXXXX 2. XXXXXXX 3. XXXXXXX\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\nData Security in Generative Al\\n\\nEasy to facilitate fraud, censorship, surveillance, cyber attacks\\n\\n¢ GenAl can be used to access or generate harmful Technology vical “iby Hemi evi C O Nn t € Nn t ° At the start of the week, Liam Porr had only heard of GPT-3. By the end, the college student had used the AI model to produce an entirely fake blog under ¢ Potential security threats of LLMs”: a, It was meant as a fun experiment. But then one of his posts reached the ° ele, e ° number-one spot on Hacker News. Few people noticed that his blog was * Discover vulnerabilities and generate exploits for them completely Al-generated. Some even hit “Subscribe”\\n\\ne Automated fraud or scam attacks\\n\\nSource: MIT Technology Review\\n\\nPersonalized social engineering attacks\\n\\n: : 50 = the japan times Q Code generation tools might generate malicious code :\\n\\nNATIONAL\\n\\nChatGPT can be tricked to write malware if acting in developer mode\\n\\nEasy access to content for planning attacks or violence\\n\\nSource: OpenAl (2023)\\n\\nSource: The Japan Times\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nIntellectual Property Protection\\n\\n¢ GenAl models might be trained on proprietary or copyrighted data.\\n\\n¢ GenAl models and datasets, like other software, are subject to licenses that will tell you how you can or cant use the model or dataset.\\n\\n¢ GenAl models might have terms for not using output of the model for commercial purposes or creating a product competing with them.\\n\\nConsiderations:\\n\\n¢ Arrange legal agreements to protect intellectual property and ensure the output of the models is used appropriately.\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nLitigation and/or other Regulatory Risks\\n\\nExisting laws still apply to new and emerging technologies.\\n\\n¢ Automated-decision making processes that\\n\\nSource: The Brussels Times\\n\\ncauses bias or discrimination may subject the Belgian man dies by\\n\\n° suicide following developer or deployer to QUIENES) actions exchanges with chatbot or litigation - for example, in the employment Tuesday, 28 March 202 space.\\n\\n¢ Claiming a model or algorithm has certain functionality or results may trigger deceptive trade practices regulatory actions.\\n\\n¢ Products liability may also give rise to litigation. Th CharT cling streets hare\\n\\nA young Belgian man recently died by suicide after talking to a chatbot named ELIZA for several weeks, spurring calls for better protection of citizens and\\n\\n©2023 Databricks Inc. — All rights reserved the need to raise awareness.\\n\\nActive Regulatory Area\\n\\n° Al, similar to other emerging technologies, is subject to both existing and newly proposed regulations. ¢ A few examples of proposed Al regulations: ¢ EU Al Act ° US Algorithmic Accountability Act 2022 ¢ Japan Al regulation approach 2023 ¢ Biden-Harris Responsible Al Actions 2023 ¢ California Regulation of Automated Decision Tools\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\n© databricks\\n\\nAcademy\\n\\nAssessing Potential Risks and Challenges:\\n\\nEthical Considerations\\n\\nDatabricks Academy 2023\\n\\nFairness and Bias in Data Big data != Good data (Size doesn’t guarantee quality)\\n\\nHuman bias in data: Sentiment Across Models ,\\n\\n¢ Biases related to social perceptions, stereotypes, and historical factors\\n\\nSentiment Score\\n\\n¢ Stem from preconceived notions, cultural influences, and past experiences\\n\\n20 350M 760M 1.3B 2.7B 6.7B 13B 175B Model Size\\n\\n¢ Outdated data doesn’t capture social view changes Source: Brown etal 2020\\n\\n¢ Examples: stereotypical bias, historical unfairness, and implicit associations\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\nFairness and Bias in Data Big data != Good data (Size doesn’t guarantee quality)\\n\\nAnnotated human bias in data collection and annotation:\\n\\n« Models use annotated or fine-tuned with human feedback\\n\\n¢ This bias type reflect errors or limitations in human judgment and reasoning\\n\\n¢ Examples: Sampling error, Confirmation bias, Anecdotal fallacy.\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nBias Reinforcement Loop\\n\\nA loop between biased input and output\\n\\n—— ———_> a ®@-®\\n\\nTraining Data Al Model Learn from Model Generate Bias People Learn / Biased Data Decide Human bias in data Models learn biases present Models generate toxic, People learn and use biased in the training data. biased or discriminatory data > This is used as new outputs. data\\n\\nModel hallucinate\\n\\n| Reinforce existing bias |\\n\\nFeedback Loop\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\nReliability and Accuracy of Al Systems\\n\\nLLMs tend to hallucinate\\n\\n¢ Hallucination: Phenomenon when the model generates outputs that are plausible-sounding but inaccurate or nonsensical responses due to limitations in understanding.\\n\\n¢ Hallucination become dangerous when;\\n\\nModels become more convincing and people rely on them more\\n\\nModels lead to degradation of information quality\\n\\n©2023 Databricks Inc. — All rights reserved Source: Jiet al 2022, OpenAl (2023) g\\n\\nReliability and Accuracy of Al Systems\\n\\nLLMs tend to hallucinate\\n\\nTwo types of model hallucination:\\n\\nIntrinsic hallucination Extrinsic hallucination\\n\\nSource: Source:\\n\\nThe first Ebola vaccine was approved by the FDA in Alice won first prize in fencing last week.\\n\\n2019, five years after the initial outbreak in 2014.\\n\\nSummary output: Output:\\n\\nThe first Ebola vaccine was approved in 2021. Alice won first prize fencing for the first time last week\\n\\nand she was ecstatic.\\n\\n©2023 Databricks Inc. — All rights reserved Source: Set al 2022\\n\\nReliability and Accuracy of Al Systems\\n\\nAlgorithmic bias in Al systems\\n\\n¢ Generative Al models can produce family politics biased or stereotypical results ,\\n\\n¢ Lack of transparency of input data\\n\\nstory\\'s topic probability\\n\\n¢ Difficult to trace-back to original input data\\n\\n¢ Limited fact-checking process\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\nHow to Address Ethical Issues\\n\\nControls need to be incorporated at all levels\\n\\n. Assess data slices. | Update data.\\n\\nToxic, discriminatory, exclusive model\\n\\ne Curate data for fine- tuning e Fine-tune your model\\n\\n(Mis)information hazard\\n\\nMalicious uses —_—_—_P Regulation\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nHow to Address Ethical Issues\\n\\nRegulations need to incorporated at all levels D ‘ . Assess data slices. aes J Update data.\\n\\nToxic, discriminatory, exclusive model\\n\\ne Curate data for fine- tuning e Fine-tune your model\\n\\n(Mis)information hazard\\n\\nWY Malicious uses Regulation\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\nAuditing Generative Al Models\\n\\nAllocating responsibility and increasing model transparency\\n\\nGovernance Audit\\n\\n+ Model access\\n\\nIntended/prohibited use cases\\n\\n} Application | Audit\\n\\nOutput logs « Environmental data\\n\\nSource: Mokander et al 2023\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\n© databricks\\n\\nAcademy\\n\\nAssessing Potential Risks and Challenges:\\n\\nHuman-Al Interaction\\n\\nDatabricks Academy 2023\\n\\nHow will Al Impact Society\\n\\nImpact on the workforce\\n\\nPro Arguments\\n\\n¢ Personalization: Enables personalized .\\n\\nexperiences in our life\\n\\n¢ Automation and Efficiency: Al will be used for repetitive tasks > Increased efficiency and higher productivity\\n\\n¢ Accessibility: GenAl making technology more inclusive and accessible by generating alternative formats, providing real-time translations, and assisting individuals with disabilities\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nCounter Arguments\\n\\nJob Displacement: Al automation may lead to job losses or displacement of workers > economic inequalities and unemployment\\n\\nEthical Concerns: Entrench existing discrimination and biases.\\n\\nOverreliance: The increased trust and reliance on Al systems may lead to unnoticed mistakes and loss of important skills\\n\\nPrivacy & Security: Privacy concerns, cyber threats and malicious attacks, Al being used for political goals\\n\\ng\\n\\nAl and Workforce\\n\\nPotential impact of generative Al on workforce\\n\\ne Arou nd 80% of the U.S. workforce Fastest growing vs. fastest declining jobs\\n\\nTop 10 fastest growing jobs\\n\\nmay witness a minimum of 10% of their work responsibilities influenced by LLMs.*\\n\\nAl and Machine Learning Specialists Sustainability Specialists\\n\\nBusiness Intelligence Analysts Information Security Analysts Fintech Engineers\\n\\nData Analysts and Scientists Robotics Engineers\\n\\nBig Data Specialists\\n\\n8 | MS | SE | Bes | =\\n\\n¢ High-wage occupations are likely to expose more.”\\n\\nAgricultural Equipment Operators\\n\\n10. Digital Transformation Specialists\\n\\nWorld Economic Forum, Future of Jobs Report 2023.\\n\\nSource: Eloundou, T. Manning, S., Mishkin, P. & Rock, D. (2023)\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\n2.\\n\\n3\\n\\nWORLD ECONOMIC FORUM\\n\\nSo\\n\\nTop 10 fastest declining jobs\\n\\nBank Tellers and Related Clerks\\n\\nPostal Service Clerks\\n\\nCashiers and ticket Clerks\\n\\nData Entry Clerks\\n\\nAdministrative and Executive Secretaries Material-Recording and Stock-Keeping Clerks Accounting, Bookkeeping and Payroll Clerks Legislators and Officials\\n\\nStatistical, Finance and Insurance Clerks\\n\\nDoor-To-Door Sales Workers, News and Street Vendors, and Related Workers\\n\\nAl at Workplace\\n\\nGenerative Al and productivity\\n\\n¢« Around 60% of CEOs and CFOs plan to use Al and automation more.*\\n\\n¢ Accessing to Gen. Al tools increases productivity by 14% on average.**\\n\\ne Novice - and less-skilled workers benefits more\\n\\n¢ Companies see Al training as one of the highest strategic priorities from now until 2027.***\\n\\nSource: Brynjolfsson, E., Li, D. & Raymond, L. (2023) , **Source: Mercer Survey, *** Source: World Economic Forum\\n\\n©2023 Databricks Inc. — All rights reserved\\n\\nAl at Workplace\\n\\nInteracting with Al agents\\n\\n¢ Prompt Engineering: Designing and — crafting effective prompts or MESHING BSEIs instructions for generating desired\\n\\noutputs from a language model. % ° Prompt quality influence the quality and relevance of generated response\\n\\nWORLD ECONOMIC FORUM\\n\\n° Clear and intuitive prompts of workers’ core skills . are expected to change ¢ Soon most of the software we use will in the next five years\\n\\nintegrate Gen. Al features. Training employees to be able to leverage these tools is going to be critical.\\n\\n©2023 Databricks Inc. — All rights reserved g\\n\\n© databricks Academy\\n\\nGenerative Al Fundamentals:\\n\\nsummary and Next Steps\\n\\nDatabricks Academy 2023\\n\\n~ databricks\\n\\n©2023 Databricks Inc. — All rights reserved'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unstructured_pdf_data[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bbb10e5-1678-43a0-88a7-3a50a769eaba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='arXiv:2404.11018v2 [cs.LG] 22 May 2024\\n\\nGoogle DeepMind 2024-5-24\\n\\nMany-Shot In-Context Learning\\n\\nRishabh Agarwal’, Avi Singh”, Lei M. Zhang\", Bernd Bohnet\\', Luis Rosias\\', Stephanie Chan‘, Biao Zhang\", Ankesh Anand , Zaheer Abbas , Azade Nova , John D. Co-Reyes , Eric Chu , Feryal Behbahani , Aleksandra Faust and Hugo Larochelle\\n\\n“Contributed equally, ‘Key contribution\\n\\nLarge language models (LLMs) excel at few-shot in-context learning (ICL) — learning from a few input- output examples (“shots”) provided in context at inference, without any weight updates. Newly expanded context windows allow us to investigate ICL with hundreds or thousands of examples - the many-shot regime. Going from few-shot to many-shot, we observe significant performance gains across a wide variety of generative and discriminative tasks. While promising, many-shot ICL can be bottlenecked by the available amount of human-generated outputs. To mitigate this limitation, we explore two settings: (1) “Reinforced ICL” that uses model-generated chain-of-thought rationales in place of human rationales, and (2) “Unsupervised ICL” where we remove rationales altogether, and prompt the model only with domain-specific inputs. We find that both Reinforced and Unsupervised ICL can be effective in the many-shot regime, particularly on complex reasoning tasks. Furthermore, we demonstrate that, unlike few-shot learning, many-shot learning is effective at overriding pretraining biases, can learn high-dimensional functions with numerical inputs, and performs comparably to fine-tuning. Finally, we reveal the limitations of next-token prediction loss as an indicator of ICL performance.\\n\\n1. Introduction\\n\\n100 Few-Shot ICL {i Many-Shot ICL 19 +18.2 +7. xs +5.0 +10.9 <a 80 Pr FS +21.0 U s 4 60 +7.9 a +15.3 sé +150 +364 192 © Ee 40 ) . ov +5.0 rf x 8 2 L 3 ge . 5 3 3 & 5 @ 4 wv 20 4 3 4 z F & i <i i) a ec a % cd B g + ~ re) a oS 0 = 3 , <8 gga 330% wi go get ey oe ext IP ances) POE 0 eo) OD NET NY ger slower acne igey no Pre 2 xo) ae AVS gid (ge Sof 98S x0 © orrno oo\" yo oe Sas ool war eG Cy week? Be @ ® Fe aN\\n\\nFigure 1 | Many-shot vs Few-Shot In-Context Learning (ICL) across several tasks. Many-shot ICL consistently outperforms few-shot ICL, particularly on difficult non-natural language tasks. Optimal number of shots for many-shot ICL are shown inside the bar for each task. For few-shot ICL, we either use typical number of shots used on a benchmark, for example, 4-shot for MATH, or the longest prompt among the ones we tested with less than the GPT-3 context length of 2048 tokens. Reasoning-oriented tasks, namely MATH, GSM8K, BBH, and GPQA use chain-of-thought rationales. For translation, we report performance on English to Bemba, summarization uses XLSum, MATH corresponds to the MATH500 test set, and sentiment analysis results are reported with semantically-unrelated labels. See §2, §3, and §4 for more details.\\n\\nA limiting factor for in-context learning (ICL) in LLMs is the context window, restricting prior research to the few-shot ICL regime. Many-shot learning — ICL with a large number of shots, for example, hundreds or thousands — allows for better task specification, can reduce the need for fine- tuning, and potentially make LLMs more versatile and adaptable. Exploring many-shot ICL is now\\n\\n© 2024 Google DeepMind. All rights reserved\\n\\nMany-Shot In-Context Learning\\n\\nMany-Shot ICL: Context Length versus Number of Shots\\n\\n2 1x10° F\\n\\no lm Best-Performing Shots x\\n\\n(© —3%10°] mm Maximum Shots\\n\\nco 1 5\\n\\n=1n 1x10\\n\\nBa 4\\n\\nf= 3x10\\n\\nafé 4\\n\\nue 1x 10’\\n\\n£~3x 103 L Cc\\n\\n°\\n\\nOQ 1x10\\n\\nae\" gah NOP ote 0% ai 8) oe tO) cs yest! ON ete) at 225 25) ac ba go seu Se cob eco GR) Gor 0 co ret Qo Sey CaN a0 ye! se\"\\n\\nFigure 2 | Context Length for best-performing and the maximum number of shots tested for each task. The horizontal dashed line shows the context length of GPT-3 (2048 tokens), which is representative of typical few-shot prompts tested in the LLM literature. For several tasks, we observed the best-performing shots correspond to the maximum number of shots we tested, which was often limited by the number of available examples for in-context learning. On some tasks €.g., code verifier, planning), we did observe slight performance deterioration beyond a certain number of shots.\\n\\nfeasible, given the recent increase in context windows of publicly available LLMs by at least 100x: from only a few thousand tokens in GPT-3 [8] and Llama 2 [57] to 1M tokens in Gemini 1.5 Pro [16].\\n\\nIn this paper, we investigate how scaling the number of shots affects ICL performance on a wide variety of tasks (§2): problem solving using MATH [23] and GSM8K [10], question-answering [GPQA, 52], summarization using XSum [43] and XLSum [20], algorithmic reasoning [BBH, 56], reward modeling [Code Verifier, 24], low-resource machine translation [FLORES, 18], planning [Logistics, 54], and sentiment analysis [FP, 40]. Compared to few-shot ICL, many-shot learning performs significant better across these tasks, using several hundreds or thousands of shots (Figure 1). Furthermore, maximum performance is often achieved only once the number of shots reaches up to hundreds of thousands of tokens (Figure 2). Concurrent to our work, recent works explore many-shot ICL to jailbreak LLMs [2] (up to 256 shots) and tackle NLP classification tasks [6] (up to 80K tokens). In our work, we focus on a much wider range of tasks, use a lot more examples (up to 8192 shots), and much longer context lengths (up to 1M tokens). See §5 for a detailed discussion of related work.\\n\\nWhile many-shot ICL holds significant promise, it can be constrained by the need for high-quality, human-generated outputs. To overcome this, we introduce reinforced ICL and unsupervised ICL (§3). Inspired by the efficacy of model-generated solutions for fine-tuning [55], Reinforced ICL involves replacing human-written rationales with model-generated ones, filtered via answer correctness, for in-context learning. Inspired by task-recognition view of ICL [66], we also introduce Unsupervised ICL where we prompt the model with only problems instead of problem-solution pairs. On problem-solving tasks such as MATH, GPQA and Big-Bench Hard, we find that both reinforced and unsupervised ICL with many-shots can be more effective than few-shot ICL with human-generated rationales, with reinforced ICL being more broadly effective.\\n\\nFinally, we empirically study how the learning dynamics of in-context learning changes from few-shot to the many-shot regime (84). We find that with sufficient examples, ICL can overcome pre- training biases, perform comparably to full fine-tuning, and solve high-dimensional prediction tasks with numerical inputs, namely sequential parity prediction and linear classification. This suggests the potential of many-shot ICL to adapt to unseen tasks and domains that might be misaligned with an LLM’s training data. Surprisingly, the order of examples can influence many-shot performance (§4.3) Finally, we demonstrate that long-context scaling laws [2, 68, 27] based on next-token prediction loss may not reliably predict ICL performance on problem-solving and reasoning tasks.\\n\\nMany-Shot In-Context Learning\\n\\nOur key contributions are as follows:\\n\\nScaling ICL (§2): We systematically evaluate ICL performance at different scales of in-context examples for a wide range of tasks with Gemini 1.5 Pro. Our results indicate large performance jumps when transitioning from few-shot to many-shot regime.\\n\\n¢ Reinforced and Unsupervised ICL (§3): We find that using model-generated rationales or only problems can reduce the dependence of many-shot ICL on human-generated data.\\n\\n¢ Analysing ICL (§4): We show that many-shot ICL can overcome pre-training biases, perform comparably to fine-tuning, and learn non-NLP prediction tasks, where few-shot ICL struggles. We also reveal that next-token prediction loss may not be a good predictor of ICL performance.\\n\\n2. Scaling In-Context Learning\\n\\nDuring in-context learning (ICL), the LLM receives a prompt containing a set of input-output examples, also called shots, that illustrate the desired task. At the end of the prompt, we append a test input and allow the LM to make a prediction just by conditioning on the prompt and predicting the next tokens auto-regressively. Recent increase in context windows of LLMs allow using many more shots for ICL than typically used. Exposure to many more shots can lead to better generalization, handle more complex problems than what is possible with few-shot ICL, make fine-tuning less essential, and greater control over model outputs, potentially reducing biases stemming from pre-training.\\n\\nEvaluation We evaluate the many-shot performance of Gemini 1.5 Pro! [16] model with 1 million token context length, the largest publicly available so far. Unless specified otherwise, we use greedy decoding. For reliable results, we randomly sample in-context examples for each K-shot prompt multiple times using different random seeds and report average performance, along with some visualization for performance on individual seeds. To ensure that using more shots provides additional information, any K-shot prompt in our setup includes all in-context examples from prompts with less than K examples. To reduce the inference cost, we use KV caching [49]. Next, we study many-shot ICL on typical LLM use-cases (also see §2.4).\\n\\n2.1. Machine Translation\\n\\nWe consider translation from English to a low-resource target language, where many-shot ICL can complement the existing knowledge within the LLM. We use the target languages with the largest gap reported between LLMs and state-of-the-art systems [53], namely Bemba and Kurdish, from FLORES-200 benchmark [45]. We modify the default 1-shot MT prompt from Gemini Team [15] to include multiple translation pairs as shots from the FLORES dev split (containing 997 examples). We evaluate performance on the first 150 sentences from the test set using chrF2++ [50], a standard metric based on character and word n-gram overlap between generated and reference translation.\\n\\nSee Figure 3 for results. Similar to Robinson et al. [53], we observed small gains in the few-shot regime from 1-shot to 10-shot, particularly on Kurdish. However, when using the entire dev set for many-shot ICL, we observe improvements of 15.3% on Bemba and 4.5% on Kurdish, relative to the 1-shot Gemini prompt. Overall, these results establish the new-state-of-art for these language pairs.\\n\\n1This corresponds to original version in the Gemini 1.5 Tech Report, released in February 2024. We note that the Gemini 1.5 Pro API now serves a newer version starting from April 2024.\\n\\nMany-Shot In-Context Learning\\n\\nMany-shot ICL: Machine Translation Many-shot ICL: Summarization Beef PEGASUS (Fine-tuned on XSum) = —e— English > Bemba S45) English > Kurdish 30) pee nS (iinetuned on ) t o + _ =) gy 40 025 = ina G 35 NLLB (SOTA) a o eee NB sorAy | 2 2 F 20 —— GEM-XSum Fr 30 —— XLSum (Transfer) 2° 27 22 23 2% 2° 2 27 28 QF QW 27 2 2 27 2\\n\\nNumber of Shots (K) Number of Shots (K)\\n\\nFigure 3 | Machine Translation (MT). Test Performance im- Figure 4 | Summarization. As we increase the number of proves monotonically as we increase the number of MT pairs shots from XSum dev set, XSum test performance improves provided as in-context examples during inference. Notably, up to 50 shots and then deteriorates. In contrast, XLSum many-shot ICL outperforms state-of-the-art chRF2++ scores performance typically improves with more shots from XSum. of 35% (NLLB) on Bemba and 40% (Google Translate) on The 500-shot prompt corresponds to 205K tokens. PEGA- Kurdish [53]. We note that 997-shot prompt corresponds to SUS [71] and mTS5 [20] are specialized models fine-tuned around 85K tokens. See an example prompt in Figure A.l. for summarization. See an example prompt in Figure A.2.\\n\\n2.2. Abstractive Summarization\\n\\nTo investigate how scaling ICL examples can impact the comprehension ability of LLMs, we now consider abstractive news summarization using XSum dataset from the GEM benchmark [1]. Using XSum dev set examples containing news articles and summaries, we also evaluate how many-shot ICL generalizes to XLSum [20]. We report performance on 150 test articles using ROUGE-L [35], which measures the longest common subsequence between reference and generated summaries.\\n\\nAs depicted in Figure 4, peak performance with many-shot ICL is remarkably close to specialized models fine-tuned on XSum and XLSum. However, XSum performance declines with more than 50 in-context examples. Surprisingly, we observed the many-shot prompted model occasionally generating summaries with fabricated dates and times (§A.8), despite the absence of such data in the in-context summaries. Nonetheless, performance on XLSum monotonically improves with more shots, demonstrating positive transfer from many-shot learning to a related task.\\n\\n2.3. Planning: Logistics Domain\\n\\nRecent work has highlighted shortcomings in planning abilities of LLMs [59]. To this 40.0% end, we evaluate whether many-shot ICL can improve their ability to generate sim- ple plans on the Logistics domain, a widely used benchmark. The objective in this do- main is to transport packages within cities\\n\\nPlanning: Logistics Domain (600 instances)\\n\\nSuccess Rate br N Ww Oo Oo Oo 8 ge8g BS BS BS\\n\\n: ee . . —e— Gemini 1.5 Pro via trucks, and between cities via airplanes. ---+ GPT-4 (Valmeekam et. al, 2024)\\n\\nWe generate a set of planning problems 0.0%\\n\\nwith 2-3 cities, 1-2 packages, 1 truck and airplane per city using a formal planning language (PDDL) generator [54], result- Figure 5 | In-context Planning. Success rate quickly improves\\n\\ning in 1.3K problems for learning and 600 with up to 10 shots (37K tokens), followed by saturation up to 400\\n\\n: : shots and a sudden performance jump at 800 shots. As a reference, for evaluation. To compute optimal solu- we report 1-shot GPT-4 results from Valmeekam et al. [59]. See tions for each problem, we use the Fast- Figure A.3 for an example 1-shot prompt.\\n\\nDownward planner [21].\\n\\n2° 27 22 2% 27 2> 2° 27 2F 2% QU Number of Shots (K)\\n\\nMany-Shot In-Context Learning\\n\\nAs shown in Figure 5, we observe significant improvement in success rate with increasing numbers of ICL shots. While far from state-of-the-art planning approaches (e.g., Fast-Downward), our results demonstrate the potential of many-shot ICL to improve the commonsense planning abilities of LLMs.\\n\\n2.4. Reward Modelling with Many-Shot ICL: Learning Code Verifiers\\n\\nCode Verifier: Best-of-4 Performance 05 Code Verifier: Conditional Probabilities 82.0% . ° > ~~ P(Yes | Correct) S 2 —|=— P(Yes | Incorrect) = 80.0% 504 > oO 5 78.0% a7 Bo ppp g 0.2\\n\\n4 76.0% 5 wz --- Pass@1 >\\n\\n74.0% —- Best-of-4 tol a\\n\\n0% 1 2 4 8 16 32 64 128 256 512 1 2 4 8 16 32 64 128 256 512 Number of Shots (K) Number of Shots (K)\\n\\nFigure 6 | Learning Verifiers In-Context for checking correctness of GSM8K code solutions. Error bars denotes standard error of mean over 3 seeds. See Figure A.5 for a 2-shot prompt. Best-of-N accuracy. (Left) Average accuracy of top-ranked code solutions (among 4 solutions) based on the verifier score on 200 GSMBK test problems. Best-of-4 selection with 128-shot bridges the gap between Pass@1 accuracy of 77.25% and Pass@4 accuracy of 90% with Gemini 1.0 Pro model. Verifier Confidence. (Right) Conditional Probabilities of the Yes token P(Yes) from the verifier, averaged over all correct and incorrect solutions on test problems.\\n\\nA standard approach to improve LLM reasoning is to use test-time verification [10, 44, 24]. Specifically, an LLM generates multiple candidate solutions for a given problem and a verifier, also known as an outcome reward model, ranks these solutions and selects the best one. Here, we focus on learning such verifiers in-context for code verification.\\n\\nTo create in-context verification examples, we utilize correct and incorrect code solutions in Python generated using Gemini 1.0 Pro [15] on the GSMS8kK train set. In the prompt, each (problem, solution) pair is appended with the question “Is the solution correct?” followed by the Yes or No token according to ground truth correctness. At inference, we modify each test (problem, solution) pair in the same way and record the logit of the Yes and No tokens (denoted by Ly,;, Lyo). To compute the verifier score, we use the normalized probability of the Yes token: P(Yes) = exp(Lyes)/( €XP(Lyes) + €XP(Lno)). We evaluate verifier performance using best-of-4 selection based on the verifier score on 200 problems from GSMBK test set with Gemini 1.0 solutions.\\n\\nAs shown in Figure 6 (left), best-of-4 accuracy with the few-shot prompted verifier significantly improves above pass@1 accuracy with 16 or more in-context examples. Along with an accuracy improvement, the probabilities of the Yes token conditioned on ground-truth correct and incorrect solutions separate with increasing the number of shots up to 256, as shown in Figure 6 (right). Overall, these results show a proof-of-concept that the Gemini model becomes better at verifying correctness of solutions with many-shot ICL.\\n\\n3. Many-shot Learning without Human-Written Rationales\\n\\nMany-shot ICL could potentially be limited by the availability of high-quality human-generated rationales or demonstrations. This is particularly challenging for complex reasoning tasks, such as GPQA [52], where human-generated rationales require significant resources and expert knowledge. In this work, we explore two simple approaches for addressing this issue.\\n\\nMany-Shot In-Context Learning\\n\\nReinforced ICL Recent work [55] proposed a simplified version of Reinforced Self-Training [19], demonstrating that fine-tuning using model-generated rationales can be more effective than human- generated rationales for problem-solving tasks. Inspired by their work, we introduce Reinforced ICL, where we use model-generated rationales for in-context learning. To do so, we use a zero-shot or few-shot chain-of-thought [62] prompt as a starting point to sample multiple rationales for each training problem. Then, we select rationales that obtain the correct final answer (we assume access to ground truth final answers or correctness checks), and arrange them into in-context examples containing (problem, rationale) pairs.\\n\\nOne potential issue with model-generated rationales is that of false positives: it is possible for an incorrect reasoning chain to lead to the correct final answer, and fine-tuning or prompting using such a reasoning chain would typically harm performance. Nevertheless, as we discuss in later sections, we often find model-generated rationales to be at least as effective human-written rationales.\\n\\nUnsupervised ICL We now go one step further than Reinforced ICL: what if we removed rationales from the many-shot prompt altogether, and prompt the model only with inputs? Specifically, the Unsupervised ICL prompt consists of: 1) a preamble, such as, “You will be provided questions similar to the ones below:”, 2) a list of unsolved inputs or problems, and 3) a zero-shot instruction or a few-shot prompt with outputs for the desired output format. See §A.2 for the exact prompts we use.\\n\\nOne hypothesis for how many-shot unsupervised ICL might surpass few-shot learning with human demonstrations is that, when the LLM already possesses the required knowledge to solve a task, any information inserted in the prompt that can narrow down what knowledge is needed for the task becomes helpful. This would be consistent with the view that ICL simply “locates” latent concepts (e.g., math problem-solving) the LLM acquired during pre-training [66, 22, 61]. As such, any of the prompt components — inputs, outputs, and their mapping — can help locate such concepts. While Unsupervised ICL is broadly applicable, it may not perform well, for example, when outputs are critical for specifying the task (Figure 9 and A.11).\\n\\n3.1. Problem-solving: Hendrycks MATH & GSM8K\\n\\nooo-- 4-shot InnerMono. MATH Prompt Mm ICL (Ground-Truth) lm Unsupervised ICL Mm Reinforced ICL\\n\\n60.0% MATH500 GSMB8K (Transfer using MATH prompts) “ew ° Se é 2 95.0% ° 3 eo . 8 e ° oe + e ° J57.5% a a oe ofl 3 S57.5% 8 90.0% \\\\ > 8 8 55.0% é 8 85.0%| ¢ fe 52.5% 80.0% ua 50.0% 75.0% . 47.5% 70.0%! © = Z 7 = =“! 25 50 125 250 500 4 10 25 50 125 250 500 Number of Shots (K) Number of Shots (K)\\n\\nFigure 7 | Many-shot Reinforced and Unsupervised ICL for problem-solving generally outperform ICL with ground-truth MATH solutions. MATH. (Left) The bar plots depict the average performance across five random seeds on the MATH500 test set. Each random seed (denoted by the dots) corresponds to a different subset of problems along with ground truth or model-generated solutions (if any) in the prompt. Transfer to GSM8K. (Right) We see that the prompt obtained from MATH transfers well to the GSM8K test split containing 500 problems. Our results with many-shot ICL outperform the 4-shot Minerva prompt, which obtains a test accuracy of 55.7% on MATHS00 and 90.6% on GSM8K.\\n\\nMany-Shot In-Context Learning\\n\\nWe evaluate Reinforced and Unsupervised ICL on Hendrycks MATH [23], which consists of challenging high school competition-level mathematics problems. We use the MATHS0O0 test set from Lightman et al. [33] to report performance, and our 4-shot MATH prompt for data generation can be found in Figure A.6. For Unsupervised ICL, we append this 4-shot prompt after the unsolved problems (see Figure A.8). For comparison, we also evaluate ICL with human-written solutions (ground-truth) from the MATH training set, with the same problems used for many-shot prompts.\\n\\nOur results are shown in the Figure 7 (left). On MATH500, both Reinforced and Unsupervised ICL outperforms ICL with ground-truth solutions in both the few-shot and many-shot regime. For ICL, we observe that the performance improves with more examples in the prompt up to a point, and then declines (with the peak being at about 125 examples). Performance for Reinforced ICL also improves with the number of examples, and reaches a plateau at around 25 examples (while being about 5% higher than ICL), and unlike ICL, we don’t see a significant drop in performance even for a very large number of examples in the context. Notably, many-shot ICL achieves comparable or superior performance when using only problems compared to using problems with solutions. This suggests solutions may be redundant for eliciting problem-solving via in-context learning on this domain, potentially due to extensive math-related data seen during pretraining.\\n\\nCan many-shot ICL enable out-of-distribution generalization? Singh et al. [55] found that fine- tuning a model on model-generated solutions from MATH resulted in improved test performance on GSM8K [10], which has a different distribution of problems than MATH. Here, we investigate whether many-shot ICL also improves transfer performance on GSMBK, indicating an improvement in general problem-solving abilities from in-context learning. Our results in Figure 7 (right) show that this is indeed the case — Reinforced ICL with MATH prompts excels on GSMB8K, outperforming ICL with ground truth MATH solutions as well as Unsupervised ICL in the many-shot setting with at least 25 shots. This indicates that model-generated solutions can enable better generalization than just using problems or combining them with ground-truth solutions for ICL.\\n\\n3.2. Question Answering: Google-Proof QA (GPQA)\\n\\nGoogle-Proof QA (GPQA): Diamond Split\\n\\n° ° ¥ 45.0% ° ee * el > Ps 0-Shot Prompt fa . @ mmm ICL (Ground-Truth) 3 40.0% hd Mmm Unsupervised ICL < a | | ---- ill Reinforced ICL 3 o\\n\\n35.0%\\n\\n5 10 25 50 125 250 Number of Shots (K)\\n\\nFigure 8 | Many-shot Reinforced and Unsupervised ICL for GPQA. The baseline zero-shot prompt, which is used for generating rationales for Reinforced ICL and appended to the prompt for Unsupervised ICL, obtains a performance of 38.8%. The average test accuracy with 125-shot prompt with both ground-truth or model-generated rationales surpass the 40.4% obtained by Claude-3 Sonnet. As we vary the number of shots, while Unsupervised ICL matches or outperforms the zero-shot prompt, Reinforced ICL consistently outperforms it.\\n\\nGPQA [52] is a multiple-choice QA benchmark, with difficult questions focused on graduate-level reasoning in biology, physics, and chemistry. Following Claude-3 [3], we use the diamond split (198 problems) for evaluation. This split focuses on questions where domain experts agree but experts in other domains struggle despite extended effort and internet access. Remaining 250 questions in non-\\n\\nMany-Shot In-Context Learning\\n\\ndiamond split are used for many-shot ICL with and without human-written rationales. For Reinforced ICL, we use a zero-shot prompt (Figure A.4) to generate multiple rationales on the non-diamond split, solving 129 problems. We also append this zero-shot prompt after the GPQA problems for specifying output format for Unsupervised ICL.\\n\\nAs shown in Figure 8, average test accuracy with ground-truth rationales improves substantially from 5 shots to 125 shots, with the best-performing 125-shot prompt nearly matching the accuracy of the state-of-the-art Claude-3 Opus. However, we do observe a performance degradation with 250 shots. Moreover, Reinforced ICL results indicate that model-generated rationales on GPQA seem to be better than ground-truth rationales up to 25 shots, while resulting in similar performance with more shots. Additionally, Unsupervised ICL does not follow any systematic trend: it sometimes performs better ICL with ground-truth rationales depending on the number of shots, but generally underperforms Reinforced ICL. As noted in Anthropic [3], GPQA is a small evaluation dataset and has an inherent higher variance across different runs, which might explain the non-systematic trends.\\n\\n3.3. Algorithmic and Symbolic Reasoning: Big-Bench Hard\\n\\nMultistep Arithmetic Two Logical Deduction [Seven] Geometric Shapes Salient Translation Error Detection\\n\\n3 10 425 50 100 3 10 25 50 100 3 10 425 50 100 3 10 25 50 100\\n\\nObject Count Reasoning About Colored Objects Dyck Languages Word Sort\\n\\n1,\\n\\n@ 1.00 00 e 0.9 fom vs ‘toto E do<<l. 5 0.50 0.90 0.8 a\\n\\n3 10 25 50 100 3 10 25 50 100 3 10 25 50 100 3 10 25 50 100\\n\\nNumber of Shots Number of Shots Number of Shots Number of Shots\\n\\n===: 3-shot CoT (Human-written) —}— Reinforced ICL —}~ Unsupervised ICL\\n\\nFigure 9 | BIG-Bench Hard. Reinforced and Unsupervised ICL with varying number of shots, averaged across five random seeds. We evaluate test performance on a held-out set of 100 problems. The error bars denote standard deviation. Reinforced ICL outperforms Unsupervised ICL for all tasks, which in turns outperforms the human-written chain-of-thought (CoT) prompt. Averaged across tasks, CoT prompting using human-written rationales gets a success rate of 72.1%, Unsupervised ICL obtains 77.1%, while Reinforced ICL gets 83%.\\n\\nWe now evaluate Reinforced ICL and Unsupervised ICL on BIG-Bench Hard [56], a suite of challenging algorithmic reasoning tasks. To reduce the impact of false positives, we select 8 tasks out of 23 in BIG-Bench Hard for which the likelihood of getting a false positive is low: either the answer string is long, or the number of options for each question is large (at least 6). For Reinforced ICL, we use the standard 3-shot CoT prompt from Suzgun et al. [56] to sample 10 rationales per problem from a training set of 150 problem at a temperature of 1.0. We filter the rationales based on final answer correctness and arrange them into prompts containing 3 to 100 (problem, rationale) pairs.\\n\\nAs shown in Figure 9, Reinforced ICL strongly outperforms Unsupervised ICL for almost all tasks, which in turn outperforms the standard 3-shot CoT prompt. Performance for Reinforced ICL generally improves monotonically with the number of prompts for 7 out of 8 tasks. These results indicate the Reinforced ICL is a more robust technique than Unsupervised ICL, especially for tasks in which the demonstrations contain crucial information about the task. For a few tasks, Reinforced ICL outperforms the human-written 3-shot prompt even in the 3-shot setting. This result suggests that model-generated rationales can sometimes outperform human-written rationales even when controlling for the amount of data, mirroring the results reported by Singh et al. [55] for fine-tuning.\\n\\nMany-Shot In-Context Learning\\n\\n4. Analyzing Many-Shot ICL\\n\\n4.1. Overcoming Pre-training Biases with Many-Shot ICL\\n\\nSentiment Analysis (FP): Replacement Labels\\n\\nSentiment Analysis (FP): Replacement Labels\\n\\n100%\\n\\nPr °o\\n\\nA —-- Abstract labels 7 —— Default labels\\n\\nY —-~ Flipped labels\\n\\n. —-- Abstract labels Lf —— Default labels ~~ Flipped labels\\n\\nTest Accuracy (%) [or] 3 x \\\\ ™ Confidence (Label Probability) ° a —— a ~.\\n\\n2 2 2 2 2 2 2 27 27 2 Number of Shots (K) Number of Shots (K)\\n\\nFigure 10 | Overcoming Pre-Training Bias with Many-Shot ICL. (Left) Many-shot ICL overcomes label flips: Test accuracy for sentiment analysis typically improves with more training shots. Flipped and abstract labels eventually approaching the performance of default labels. (Right) Confidence shift in overcoming bias. For flipped and abstract labels, model confidence in its predicted sentiment labels initially drops, then sharply increases with more training shots to similar value, suggesting a period of overcoming pre-training bias.\\n\\nWhile LLMs demonstrate in-context learning of novel tasks, Kossen et al. [30] suggest that ICL may have difficulty unlearning biases derived from pre-training data. Their experiments, however, focused mainly on few-shot ICL due to LLM context length limitations. Here, we revisit their study using many-shot ICL on the Financial PhraseBank (FP) sentiment analysis dataset [40]. Like Kossen et al. [30], we study label relationships that affect pre-training biases:\\n\\n¢ Flipped Labels: Default labels are rotated, that is, [‘negative’, ‘neutral’, ‘positive’] becomes [‘neutral’, ‘positive’, ‘negative’]. This conflicts with sentiment biases an LLM might have learned.\\n\\n¢ Abstract Labels: We use [‘’, ‘B’, ‘C’], removing any pre-existing sentiment association [63].\\n\\nFor ICL shots, we sample examples from the validation set (with replaced labels) to exhibit the input-label relationship and report the results in Figure 10. With few shots, test accuracy with replacement labels is much lower than with default labels. This suggests that with few-shot ICL, the model struggles to overcome its pre-existing biases from pre-training. However, as the number of shots increases, performance on flipped and abstract labels dramatically improves, approaching that of default labels. For default labels, confidence in predicted labels steadily increases with more shots, as shown in Figure 10 (right). In contrast, for flipped labels, confidence initially drops then sharply increases before reaching a plateau, suggesting a period of overcoming pre-training bias.\\n\\nWe posit that the initial drop in performance and confidence in the few-shot regime may be attributed to the “early ascent” phenomenon [47, 36]: a small number of shots may lead to the retrieval of an incorrect skill, which eventually diminishes as task learning takes effect in the many- shot regime. Overall, these results indicate that many-shot ICL can overcome pre-training biases.\\n\\n4.2. Learning Non-Natural Language Tasks: High-Dimensional Functions\\n\\nWe now test many-shot ICL’s ability to learn abstract mathematical functions with numerical inputs, which let us stress test its generality and applicability to possibly unseen tasks.\\n\\nBinary Linear Classification in High Dimensions Following the setup from Wei et al. [63], we create datasets with N-dimensional inputs vectors and their binary class labels, where each dimension\\n\\nMany-Shot In-Context Learning\\n\\n—— Gemini 1.5 Pro —-—- k-Nearest Neighbors — ----- Random Classification (N = 16) Classification (N = 32) Classification (N = 64) _ 90.0% S > 80.0% g 5 & 70.0% < 2 60.0% 50.0% - 2= 25 27 2° 2° 27 28 2 Qu gm 2 25 27 2 2° 27 28 2 QW QT 22 25 27 2° 2° 27 2 29 Qi QTt Number of Shots Per Class (K) Number of Shots Per Class (K) Number of Shots Per Class (K)\\n\\nFigure 11 | In-Context Classification. Test accuracy for 16, 32 and 64 dimensional linear classification problems, averaged across 5 randomly-generated datasets with 25 points per class for each dataset (250 evaluation points total). As we increase the number of shots, the accuracy improves and approximately tracks the performance of the nearest-neighbor baseline trained from scratch on the same data. We use the default implementation of k-nearest neighbours (with k = 5) from scikit-learn [48]. See Figure A.7 for an example prompt.\\n\\nParity Sequence: 20 Digits\\n\\nInput: 10110001110000100111 Label: Odd Odd Even Odd Odd Odd Odd Even Odd Even Even Even Even Even Odd Odd Odd Even Odd Even\\n\\n—— Gemini 1.5 Pro » Random Chance\\n\\nInput:01100110110011000111 GPT-2 Med (Scratch): 20x data\\n\\nLabel:\\n\\nTest Accuracy (%) (Exact Seq. Match) N oO oO x\\n\\n27 27 2% 2 27 28 29 2 QM gt pg\\n\\nNumber of Shots (K) Figure 12 | Learning Sequential Parity Function In-context. We report test accuracy over 200 unseen inputs, averaged across 3 seeds. Error bars denote standard error of the mean. Task Prompt. (Left) Example prompt with input and output labels of the 20-digit Sequential Parity Function. Test accuracy (Right) Many-shot ICL performance improves almost\\n\\nmonotonically with the number of shots, surpassing performance of GPT-2 Medium sized transformer trained from scratch for 1 forward-backward pass per example on 20x more data.\\n\\nis a random integer in [1, 1000]. See more details in 8A.5. While Wei et al. [63] used only 16 shots per class, we scale ICL up to 2048 shots per class. As shown in Figure 11, while 2048 shots per class perform best when N = 16, we observe slight accuracy decrease beyond 512 shots for higher values of N (Figure 11 C, R). Moreover, many-shot ICL substantially outperforms random-chance accuracy and nearly matches the accuracy of a strong baseline, namely k-nearest neighbors, indicating that many-shot ICL can implement nearest-neighbour search over inputs. This is reminiscent of induction heads that implement prefix matching over sequences [46], a plausible mechanism for ICL abilities.\\n\\nSequential Parity Parity is a fundamental Boolean function that determines if a binary input sequence contains an even or odd number of 1s. Despite their power, transformers trained specifically for in-context learning, struggle to learn the Parity function over 20-digit sequences [7]. In this work, we evaluate how well many-shot ICL performs with a pretrained LLM to learn the sequential parity function f(x) = [f1(x), fa(x),---, fa(x)], where x € {0,1}\" and f;(x) = x1 @xg---@x;, Vie [1,n]. We report the results in Figure 12. We see consistent improvement in test accuracy as we increase the number of shots to 8192. Performance surpasses a GPT-2 Medium sized transformer [51] trained from scratch on 20x more input-output examples (with no repeated examples; §A.6). This result indicates many-shot ICL can implement computations analogous to gradient descent [60].\\n\\n10\\n\\nMany-Shot In-Context Learning\\n\\n4.3. Is Many-Shot ICL Sensitive to Example Ordering?\\n\\nIn few-shot in-context learning (ICL), the order of examples within the prompt can significantly impact model performance [38, 65]. Here, we investigate whether such sensitivity to prompt ordering ob-\\n\\nserved in few-shot ICL persists in many-shot _ scenarios, which remains\\n\\nlargely unexplored. Specifically, we evalu-\\n\\nate ten different random orderings of fixed MATH500: 50-shot Ordering Sensitivity\\n\\n50 in-context examples from MATH training 37° r\\n\\nsplit and evaluate performance on the held-out 1 05.0% \" + |\\n\\nMATHS0O0 test set [33]. 560.0% ° ° : § t—s F As Figure 13 reveals, performance varies sig- 3 55.0% Q ° e g¢ 2 °\\n\\nnificantly across different subareas in MATH500. 8 50.0%] ¢\\n\\nStrikingly, an ordering that that excels in one 45.0%! °\\n\\nsubarea may perform poorly in another, for ex- 9 cae oe et eS 6% (eo\\n\\nample, the best Geometry ordering yields weak e iS cores\\n\\nresults on Number Theory. This fluctuation re-\\n\\nsults in a smaller variation in average perfor- Figure 13 | Many-Shot Sensitivity To Example Ordering. see : Each colored data point represents a different random order-\\n\\nmance compared to individual subareas. Onein- . . . a\\n\\n. . i. ing of 50 in-context examples provided to Gemini 1.5 Pro.\\n\\nteresting extension would be to optimize many-\\n\\nshot prompts using frameworks like DSPy [28] that has been successfully applied for optimizing\\n\\nfew-shot prompts based a given metric. Overall, these findings highlight a key challenge in ensuring\\n\\nreliable results with many-shot ICL for long-context models.\\n\\n4.4. Many-Shot ICL vs. Supervised Fine-Tuning\\n\\nBase Model (@@m_ Supervised FT (@m_ ~Many-Shot ICL\\n\\n~ English > Bemba English > Kurdish\\n\\nBS\\n\\n+ 40 4 4\\n\\n+ a a\\n\\nta ™ ™ “ “\\n\\nE20] & N\\n\\nU\\n\\ni)\\n\\neee eee\\n\\n250 997 250 997\\n\\nNumber of Examples Number of Examples\\n\\nFigure 14 | Comparing SFT with Many-Shot ICL on low-resource translation. We plot mean performance across 3 seeds. The standard deviation is between 0.1% to 0.5%. Base model corresponds to 1-shot performance of Gemini 1.5 Pro.\\n\\nMany-shot ICL could make task-specific fine-tuning less essential or, in some cases, even un- necessary, allowing LLMs to tackle a wider range of tasks without specialization. While supervised fine-tuning (SFT) is the dominant LLM paradigm when making use of hundreds or thousands of examples, it is computationally expensive in terms of training. In contrast, many-shot ICL does not require any training, however it has a larger inference cost, which can be substantially reduced with KV caching [49, 64], which might be available off-the-shelf with context caching [12].\\n\\nHere, we compare many-shot ICL to full fine-tuning for machine translation (§2.1). We run two sets of experiments: one using 250 examples, and another using the entire dev set (997 examples). Our results in Figure 14 show that SFT and ICL performance is quite close for Bemba, while SFT has a slight edge for Kurdish. Overall, these results demonstrate that many-shot ICL can be a viable alternative to SFT for some tasks.\\n\\n11\\n\\nMany-Shot In-Context Learning 4.5. Comparing Many-Shot Abilities of Frontier LLMs\\n\\n—e— Gemini 1.5 Pro GPT-4-Turbo —e— Claude-3-Opus\\n\\nTranslation: English - Bemba Translation: English > Kurdish\\n\\niN o\\n\\nw ua\\n\\nTest chrF2++ (%) WwW uw\\n\\nTest chrF2++ (%)\\n\\n2° 2? 22 23 27 2 2% 27 2F 29 2 2° 2t 22 23 27 2° 2© 27 2% 29 2 Number of Shots (K) Number of Shots (K)\\n\\nFigure 15 | Many-shot ICL with GPT-4-Turbo and Claude-3-Opus [3] on low-resource machine translation (§2.1).\\n\\nThe strong many-shot results with Gemini 1.5 Pro raises the question of whether other long-context frontier LLMs also benefit from many-shot ICL. To do so, we evaluate GPT-4-Turbo (128K context length) and Claude-3-Opus [3] (200K context length) on the low-resource translation (§2.1). For both these models, many-shot ICL scales favorably on Bemba but do not exhibit much improvement on Kurdish. Notably, 1.5 Pro starts lower than Claude-3 on Bemba but improves more rapidly, achieving much higher performance at 997 shots. It also outperforms GPT-4 in few-shot learning and improves\\n\\nfurther with more examples. Overall, these results indicate that frontier LLMs exhibit varying degree of many-shot ICL capability.\\n\\n4.6. Long-context scaling laws may not predict ICL performance\\n\\nNegative Log-Likelinood on Ground-Truth Solutions\\n\\n8 GPQA MATH GSMBK co} £ oe & y 2 $A, gv 0.70 0.8 Lo - - = 7 = Lt 313 s 1D 0.65 0.6 F ee 0.60 eT S44 0.4 ov Zz 5 40 275 50 425 750 4 10 25 50 425750500 4 10 25 50 425750500 Number of Shots (K) Number of Shots (K) Number of Shots (K) —— Reinforced ICL —e— ICL (Ground-Truth) —a— Unsupervised ICL\\n\\nFigure 16 | Negative Log-Likelihood (NLL) as a function of number of shots. We plot NLL on ground truth test set solutions for GPQA, MATH and GSMBK. For GPQA and MATH, questions for Reinforced ICL and Unsupervised ICL comes from the training splits of those datasets. We study GSMB8K in the transfer setting, i.e. questions for Reinforced and Unsupervised ICL come from MATH. The absolute NLL for ICL and Reinforced ICL are not directly comparable to Unsupervised ICL, since they use different prompt formats.\\n\\nPrior works [68, 2, 27] have found that the negative log-likelihood (NLL) for ground-truth test outputs decreases predictably as the context length increases. We confirm this finding for GPQA, Hendrycks MATH and GSM8K with many-shot ICL, and report our results in Figure 16. However, we note that NLL trends are not a strong predictor for downstream task performance. For example, the success rate for both MATH and GPQA with ICL decreases after 125 shots (Figure 7,8), but we do not observe a corresponding increase in the NLL in Figure 16.\\n\\nWe also plot NLL curves for Reinforced and Unsupervised ICL, and find them to generally have a\\n\\n12\\n\\nMany-Shot In-Context Learning\\n\\nsmaller slope when compared to supervised ICL. Interestingly, NLL curves for ICL with ground-truth outputs is lower than with model-generated outputs, even though the latter often performs better. In the GSM8K transfer setting (using MATH problems and solutions to score GSM8K solutions), the change in NLL is close to nil. However, this doesn’t reflect transfer performance on GSM8K, which continues to improve with more examples (Figure 7).\\n\\nOverall, our results demonstrate that NLL is not a reliable proxy when attempting to predict ICL performance for problem-solving domains. This makes intuitive sense: for any given problem, there are a large number of potentially correct CoT solutions that the model can generate, and calculating the log-likelihood on only one such solution may not provide a clear picture for overall model capability. We also explore computing NLL on a diverse set of model-generated outputs on MATH, and our findings are presented in §A.7.\\n\\n5. Related Work\\n\\nScaling in-context learning Brown et al. [8] reported improved performance as you increase the number of examples (up to 64) for in-context learning in LLMs , and later works corroborated this finding [39]. However, very few works have explored using a large number of examples (1000 or above) in the prompt. This is likely due to the fact the context lengths in large language models have been quite limited until recently [16, 3]. One closely related work to ours is from Li et al. [31], who scale the number of examples for in-context learning to 2000. However, Li et al. [31] use a custom model architecture [74] to achieve long context lengths, and only evaluate models of up to 1.3B parameters, which is several orders of magnitude smaller than state-of-the-art language models, and are ineffective for complex tasks, such as GPQA [52].\\n\\nConcurrently to our work, Anil et al. [2] used many-shot prompting (upto 256 shots) to jailbreak language models. In our work, we focus on a much wider range of tasks, use a lot more examples (up to 8192 shots) and use models with much longer context lengths (up to 1M tokens). Also, we explore mitigations for needing many human-generated examples with many-shot ICL. Furthermore, while Anil et al. [2] use many-shot learning to override preferences learned during RLHF phase to elicit the biases stemming from pretraining, our results in §4.1 demonstrate that we can also override pre-training biases themselves. Bertsch et al. [6] also concurrently shows benefits of scaling up in-context learning to many demonstrations on several classification datasets with up to 151 labels, albeit also using smaller context windows of up to 80k tokens (using Llama2-80k [13]).\\n\\nLong-context scaling laws Prior works [68, 2, 27, 16] have reported smaller next-token prediction loss with longer contexts, which Jeon et al. [25] also show using theoretical analysis. Our findings confirm this trend for even longer context lengths, but our analysis reveals some of the limitations of using next-token prediction loss as a metric for evaluating long-context performance, as next-token prediction loss continues to go down even as overall performance plateaus.\\n\\nLearning from self-generated data Numerous recent works [19, 70, 55] propose fine-tuning language models on self-generated data to improve performance. Their approach consists of (1) generate samples from the model and filter them using binary feedback, (2) fine-tune the model on these samples, and (3) repeat this process a few times. In this work, we extend this idea to in-context learning, and study the efficacy of Reinforced ICL in reasoning and problem-solving domains.\\n\\nMany-Shot In-Context Learning\\n\\nSelf-generated data and in-context learning Kim et al. [29] propose using self-generated data for few-shot ICL on classification problems, where they generate demonstrations using the LLM conditioned on the test input for each possible class label, and including these demonstrations in the context when performing the final prediction. Li et al. [32] extend this approach to reasoning and language understanding tasks, where they also generate demonstrations conditioned on the test input. Consistent with our findings, these works show that model-generated demonstrations can outperform human-generated demonstrations in the few-shot regime. Another related approach is AutoCoT [73] that uses a zero-shot CoT prompt to produce model-generated demonstrations for few-shot ICL. To do so, AutoCoT samples diverse questions one-by-one based on embedding-based clustering followed by heuristics-based post-processing for selecting demonstrations.\\n\\nDifferent from above approaches, Reinforced ICL generates demonstrations using the same pro- cedure as Singh et al. [55], does not require clustering, post-processing heuristics, or access to the test inputs for generating demonstrations, and can be applied to any problem for which we can obtain reliable reward signals. Moreover, our work mainly focuses on the utility of randomly-sampled model-generated demonstrations for many-shot ICL.\\n\\nLearning Input-Output Relationships with ICL Numerous works [41, 30, 69, 36] have investigated whether LLMs truly learn input-output relationships during in-context learning. Min et al. [41] found that replacing the ground truth labels in in-context examples with random labels barely effected final performance. Further investigations by Yoo et al. [69] and Kossen et al. [30] found that this finding does not necessarily hold across tasks and model sizes. In particular, Kossen et al. [30], Lin and Lee [36] showed that LLMs can indeed learn input-output relationships via in-context learning, but require more examples in order to do so well. In our work, we extrapolate the trend found in those works to much longer context lengths, showing that pre-training biases can be mostly overcome given enough training examples.\\n\\nLearning Mathematical Functions with LLMs Several prior works investigate whether mathemati- cal functions can be learned with transformers [14, 72, 67, 7]. All these works train transformers specifically to perform in-context learning for such functions. In contrast, we demonstrate that many- shot ICL can learn high-dimensional functions even with pre-trained LLMs. Concurrent to our work, Vacareanu et al. [58] demonstrate that pretrained LLMs are able to perform regression tasks, with performance rivaling that of traditional supervised methods with 500 in-context examples. Our work complement their findings to other synthetic tasks with a much larger number of in-context examples. Dinh et al. [11] fine-tuned GPT-3 on synthetic classification tasks and observed similarities in the decision boundaries learned by the fine-tuned model and kNNs. Our results in Figure 11 show that many-shot ICL also performs comparably to kNNs on high-dimensional classification tasks.\\n\\nComparing ICL with fine-tuning Contrary to task-specific fine-tuning, ICL does not require op- timizing any model weights, allowing LLMs to perform a variety of tasks at inference. As such, several prior works compare fine-tuning with ICL but in few-shot regime. Liu et al. [37] proposed a parameter-efficient few-shot fine-tuning (FT) approach for TO that outperforms few-shot ICL with GPT-3. However, Awadalla et al. [5] argue that few-shot ICL is more robust to distribution shifts than fine-tuning for question answering tasks. Similarly, Asai et al. [4] show better transfer with ICL compared to fine-tuning on some tasks. Mosbach et al. [42] fairly compare ICL with FT by using the same model for both approaches and show that full fine-tuning (FT) generally outperforms ICL in the few-shot regime with 16 examples. More recently, Lin et al. [34] show that few-shot ICL can outperform fine-tuning based approaches for aligning LLMs.\\n\\nMany-Shot In-Context Learning\\n\\nComplementary to prior works, we compare full fine-tuning with many-shot ICL with the same number of examples for low-resource translation. Notably, we find that many-shot ICL performs comparably to FT. Aligned with our findings, Bertsch et al. [6] concurrently show that many-shot ICL generally outperforms parameter-efficient fine-tuning (LoRA) on classification tasks. Overall, many-shot ICL and FT can exhibit comparable behaviors, which we leave for further investigation.\\n\\nExemplar vs. Rule-based ICL generalization Chan et al. [9] indicate that ICL tends to generalize in a more exemplar-based way, compared to rule-based generalization during in-weights learning. Using a clever experiment with blocked attention, Bertsch et al. [6] also argue that the benefits of many in-context demonstrations arise from having access to more similar examples. While our results on in-context linear classification agree with this conclusion, our sequential parity results seem to contradict it. Strikingly, sequential parity was the task on which we saw the most improvement, whereas it should be a task that benefits least from seeing similar examples — after all, the nearest neighbor is always going to give the wrong answer (off by 1 bit). Chan et al. [9] do show that a transformer’s inductive biases towards exemplar-based generalization can be shifted both by the training data and the model size, with larger models being less exemplar-based — perhaps this explains the contradictory findings, given that our work used a larger and much more capable model, though this remains an open question.\\n\\n6. Discussion, Limitations and Future Work\\n\\nWe found significant gains in performance when going from few-shot to many-shot ICL on a wide range of tasks, including translation, summarization, planning, reward modeling, mathematical problem solving, question-answering, algorithmic reasoning, and sentiment analysis. To overcome the challenges of obtaining a large number of high-quality human-written rationales for many-shot ICL, we introduced two regimes: Reinforced ICL and Unsupervised ICL. Moreover, we demonstrate that, unlike few-shot ICL, many-shot ICL is effective at overriding pretraining biases, can learn high-dimensional functions with numerical inputs, and performs comparably to SFT.\\n\\nOne limitation of our work is that it mainly evaluates many-shot ICL with Gemini 1.5 Pro. That said, concurrent works [2, 6] as well as our preliminary results with GPT-4-Turbo and Claude-3- Opus (84.5) indicate that other LLMs can also benefit from many-shot ICL. Future work should focus on evaluating the many-shot abilities of a wide range of long context models, as they become available. Furthermore, many-shot performance can likely serve as a valuable metric for evaluating the quality of long-context models, going beyond the prevalent needle-in-a-haystack test [26].\\n\\nAnother limitation of our work is that we don’t completely understand why performance can sometimes degrades with more examples in the prompt (for example, for MATH). Our analysis found that negative log-likelihood trends are insufficient to explain this degradation, and future work should investigate new directions to shed light on the matter and improving many-shot ICL capabilities. Overall, we hope that this work lays a foundation for understanding and optimizing the use of long-context models for ICL, opening up a new frontier of LLM capabilities.\\n\\nAcknowledgements We would like to thank Gheorghe Comanici for reviewing an early draft of this work. We are also\\n\\ngrateful to Doina Precup, Aviral Kumar, Dale Schuurmans, Ankit Anand, Ross Goroshin, Urvashi Singh, Jannik Kossen, Charline Le Lan, and Daniel Toyoma for helpful discussions.\\n\\nMany-Shot In-Context Learning\\n\\nContribution Statement\\n\\nRA initiated and led the project, ran majority of the many-shot experiments and analysis, came up with reinforced ICL, on-boarded collaborators, wrote the initial draft. AS contributed initial infra for experiments on MATH and GSMB8kK, ran BBH experiments, co-led the fine-tuning experiments, conducted NLL analysis on problem-solving tasks, and wrote several sections.\\n\\nLZ contributed results for in-context verifier. BB contributed the planning logistics task. LR led the fine-tuning experiments. BZ contributed the many-shot results for GPT-4 and Claude-3. AA helped with GPQA, SC contributed the baseline for parity task and both helped edit the paper. AF and HL provided feedback on an early draft. HL also suggested the unsupervised ICL experiments. Others were involved in project discussions and minor edits to the paper.\\n\\nReferences\\n\\n(1]\\n\\n[2]\\n\\n[3]\\n\\n[4]\\n\\n[5\\n\\nfa\\n\\n[6\\n\\nfa\\n\\n[7]\\n\\n[8]\\n\\nS.N. Akter, Z. Yu, A. Muhamed, T. Ou, A. Bauerle, A. A. Cabrera, K. Dholakia, C. Xiong, and G. Neubig. An in-depth look at gemini’s language abilities. arXiv preprint arXiv:2312.11444, 2023.\\n\\nC. Anil, E. Durmus, M. Sharma, J. Benton, S. Kundu, J. Batson, N. Rimsky, M. Tong, J. Mu, D. Ford, F. Mosconi, R. Agrawal, R. Schaeffer, N. Bashkansky, S. Svenningsen, M. Lambert, A. Radhakrishnan, C. Denison, E. J. Hubinger, Y. Bai, T. Bricken, T. Maxwell, N. Schiefer, J. Sully, A. Tamkin, T. Lanham, K. Nguyen, T. Korbak, J. Kaplan, D. Ganguli, S. R. Bowman, E. Perez, R. Grosse, and D. Duvenaud. Many-shot jailbreaking. Technical report, Anthropic, 2024.\\n\\nAnthropic. The claude 3 model family: Opus, sonnet, haiku. Technical Report, 2024.\\n\\nA. Asai, S. Kudugunta, X. V. Yu, T. Blevins, H. Gonen, M. Reid, Y. Tsvetkov, S. Ruder, and H. Hajishirzi. Buffet: Benchmarking large language models for few-shot cross-lingual transfer. arXiv preprint arXiv:2305.14857, 2023.\\n\\nA. Awadalla, M. Wortsman, G. Ilharco, S. Min, I. Magnusson, H. Hajishirzi, and L. Schmidt. Exploring the landscape of distributional robustness for question answering models. arXiv preprint arXiv:2210.12517, 2022.\\n\\nA. Bertsch, M. Ivgi, U. Alon, J. Berant, M. R. Gormley, and G. Neubig. In-Context Learning with Long-Context Models: An In-Depth Exploration, Apr. 2024. URL http: //arxiv.org/abs/ 2405 .00200. arXiv:2405.00200 [cs].\\n\\nS. Bhattamishra, A. Patel, P. Blunsom, and V. Kanade. Understanding in-context learning in transformers and Ilms by learning to learn discrete functions. arXiv preprint arXiv:2310.03016, 2023.\\n\\nT. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Lan- guage models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. Bal- can, and H. Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6- 12, 2020, virtual, 2020. URL https: //proceedings.neurips.cc/paper/2020/hash/ 1457cO0d6bf cb4967418bfb8ac142f64a-Abstract .html.\\n\\n[9]\\n\\nKe) fi]\\n\\n[20]\\n\\n[21]\\n\\n[22]\\n\\nMany-Shot In-Context Learning\\n\\nS. C. Y. Chan, I. Dasgupta, J. Kim, D. Kumaran, A. K. Lampinen, and F. Hill. Transformers generalize differently from information stored in context vs in weights, Oct. 2022. URL http: //arxiv.org/abs/2210.05675. arXiv:2210.05675 [cs].\\n\\nK. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.\\n\\nT. Dinh, Y. Zeng, R. Zhang, Z. Lin, M. Gira, S. Rajput, J.-y. Sohn, D. Papailiopoulos, and K. Lee. Lift: Language-interfaced fine-tuning for non-language machine learning tasks. Advances in Neural Information Processing Systems, 35:11763-11784, 2022.\\n\\nG. A. for Developers. Context caching guide, 2024. URL https://ai.google.dev/ gemini-api/docs/caching.\\n\\nY. Fu, R. Panda, X. Niu, X. Yue, H. Hajishirzi, Y. Kim, and H. Peng. Data Engineering for Scaling Language Models to 128K Context, Feb. 2024. URL http: //arxiv.org/abs/2402.10171. arXiv:2402.10171 [es].\\n\\nS. Garg, D. Tsipras, P. S. Liang, and G. Valiant. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 35: 30583-30598, 2022.\\n\\nG. Gemini Team. Gemini: A family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.\\n\\nG. Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arxiv:2403.05530, 2024.\\n\\nM. Ghallab, A. Howe, C. Knoblock, D. Mcdermott, A. Ram, M. Veloso, D. Weld, and D. Wilkins. PDDL—The Planning Domain Definition Language, 1998.\\n\\nN. Goyal, C. Gao, V. Chaudhary, P. Chen, G. Wenzek, D. Ju, S. Krishnan, M. Ranzato, F. Guzman, and A. Fan. The flores-101 evaluation benchmark for low-resource and multilingual machine translation. Trans. Assoc. Comput. Linguistics, 10:522-538, 2022.\\n\\nC. Gulcehre, T. L. Paine, S. Srinivasan, K. Konyushkova, L. Weerts, A. Sharma, A. Siddhant, A. Ahern, M. Wang, C. Gu, et al. Reinforced self-training (rest) for language modeling. arXiv preprint arXiv:2308.08998, 2023.\\n\\nT. Hasan, A. Bhattacharjee, M. S. Islam, K. S. Mubasshir, Y. Li, Y. Kang, M. S. Rahman, and R. Shahriyar. Xl-sum: Large-scale multilingual abstractive summarization for 44 languages. In C. Zong, F. Xia, W. Li, and R. Navigli, editors, Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021, volume ACL/IJCNLP 2021 of Findings of ACL, pages 4693-4703. Association for Computational Linguistics, 2021.\\n\\nM. Helmert. The fast downward planning system. Journal of Artificial Intelligence Research, 26: 191-246, July 2006. ISSN 1076-9757. doi: 10.1613/jair.1705. URL http: //dx.doi.org/ 10.1613/jair.1705.\\n\\nR. Hendel, M. Geva, and A. Globerson. In-context learning creates task vectors. arXiv preprint arXiv:2310.15916, 2023.\\n\\n[23]\\n\\n[24]\\n\\n[25]\\n\\n[26]\\n\\n[27]\\n\\n[28]\\n\\n[29]\\n\\n[30]\\n\\n[31]\\n\\n[32]\\n\\n[33]\\n\\n[34]\\n\\n[35]\\n\\n[36]\\n\\nMany-Shot In-Context Learning\\n\\nD. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Mea- suring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.\\n\\nA. Hosseini, X. Yuan, N. Malkin, A. Courville, A. Sordoni, and R. Agarwal. V-star: Training verifiers for self-taught reasoners. arXiv preprint arXtv:2402.06457, 2024.\\n\\nH. J. Jeon, J. D. Lee, Q. Lei, and B. Van Roy. An information-theoretic analysis of in-context learning. arXiv preprint arXiv:2401.15530, 2024.\\n\\nG. Kamradt. LLMTest_NeedleInAHaystack. https://github.com/gkamradt/LLMTest_ NeedleInAHaystack, 2023. Accessed: 2024-04-16.\\n\\nJ. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.\\n\\nO. Khattab, A. Singhvi, P. Maheshwari, Z. Zhang, K. Santhanam, S. V. A, S. Haq, A. Sharma, T. T. Joshi, H. Moazam, H. Miller, M. Zaharia, and C. Potts. DSPy: Compiling declarative language model calls into state-of-the-art pipelines. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=sY5N0zY50d.\\n\\nH. J. Kim, H. Cho, J. Kim, T. Kim, K. M. Yoo, and S. Lee. Self-generated in-context learning: Lever- aging auto-regressive language models as a demonstration generator. CoRR, abs/2206.08082, 2022. doi: 10.48550/ARXIV.2206.08082. URL https: //doi.org/10.48550/arXiv.2206. 08082.\\n\\nJ. Kossen, Y. Gal, and T. Rainforth. In-context learning learns label relationships but is not conventional learning. In The Twelfth International Conference on Learning Representations, 2023.\\n\\nM. Li, S. Gong, J. Feng, Y. Xu, J. Zhang, Z. Wu, and L. Kong. In-context learning with many demonstration examples. CoRR, abs/2302.04931, 2023. doi: 10.48550/ARXIV.2302.04931. URL https: //doi.org/10.48550/arXiv .2302.04931.\\n\\nR. Li, G. Wang, and J. Li. Are human-generated demonstrations necessary for in-context learning? In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=frRDT6EOhg.\\n\\nH. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe. Let’s verify step by step. CoRR, abs/2305.20050, 2023. doi: 10.48550/ARXIV.2305.20050. URL https: //doi.org/10.48550/arXiv.2305.20050.\\n\\nB. Y. Lin, A. Ravichander, X. Lu, N. Dziri, M. Sclar, K. Chandu, C. Bhagavatula, and Y. Choi. The unlocking spell on base Ilms: Rethinking alignment via in-context learning. arXiv preprint arXiv:2312.01552, 2023.\\n\\nC.-Y. Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summariza- tion Branches Out, pages 74-81, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https: //aclanthology.org/W04-1013.\\n\\nZ. Lin and K. Lee. Dual operating modes of in-context learning. arXiv preprint arXiv:2402.18819, 2024.\\n\\n[37]\\n\\n[38]\\n\\n[39]\\n\\n[40]\\n\\n[41]\\n\\n[42]\\n\\n[43]\\n\\n[44]\\n\\n[45]\\n\\n[46]\\n\\n[47]\\n\\n[48]\\n\\nMany-Shot In-Context Learning\\n\\nH. Liu, D. Tam, M. Mugeeth, J. Mohta, T. Huang, M. Bansal, and C. A. Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in Neural Information Processing Systems, 35:1950-1965, 2022.\\n\\nY. Lu, M. Bartolo, A. Moore, S. Riedel, and P. Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786, 2021.\\n\\nY. Lu, M. Bartolo, A. Moore, S. Riedel, and P. Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In S. Muresan, P. Nakov, and A. Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 8086-8098. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022. ACL-LONG.556. URL https: //doi.org/10.18653/v1/2022.acl-long.556.\\n\\nP. Malo, A. Sinha, P. Korhonen, J. Wallenius, and P. Takala. Good debt or bad debt: Detecting semantic orientations in economic texts. Journal of the Association for Information Science and Technology, 65(4):782-796, 2014.\\n\\nS. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Hajishirzi, and L. Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In Y. Goldberg, Z. Kozareva, and Y. Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 11048-11064. Association for Computational Linguistics, 2022.\\n\\nM. Mosbach, T. Pimentel, S. Ravfogel, D. Klakow, and Y. Elazar. Few-shot fine-tuning vs. in-context learning: A fair comparison and evaluation. arXiv preprint arXiv:2305.16938, 2023.\\n\\nS. Narayan, S. B. Cohen, and M. Lapata. Don’t give me the details, just the summary! topic- aware convolutional neural networks for extreme summarization. In E. Riloff, D. Chiang, J. Hockenmaier, and J. Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 1797-1807. Association for Computational Linguistics, 2018.\\n\\nA. Ni, S. Iyer, D. Radev, V. Stoyanov, W.-t. Yih, S. Wang, and X. V. Lin. Lever: Learning to verify language-to-code generation with execution. In International Conference on Machine Learning, pages 26106-26128. PMLR, 2023.\\n\\nM. A. NLLB Team. No language left behind: Scaling human-centered machine translation. arXiv preprint, 2022.\\n\\nC. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, S. Johnston, A. Jones, J. Kernion, L. Lovitt, K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish, and C. Olah. In-context learning and induction heads. Transformer Circuits Thread, 2022. https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.\\n\\nJ. Pan. What in-context learning “learns” in-context: Disentangling task recognition and task learning. PhD thesis, Princeton University, 2023.\\n\\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pret- tenhofer, R. Weiss, V. Dubourg, et al. Scikit-learn: Machine learning in python. the Journal of machine Learning research, 12:2825-2830, 2011.\\n\\n[49]\\n\\n[50]\\n\\n[51]\\n\\n[52]\\n\\n[53]\\n\\n[54]\\n\\n[55]\\n\\n[56]\\n\\n[57]\\n\\n[58]\\n\\n[59]\\n\\n[60]\\n\\n[61]\\n\\n[62]\\n\\nMany-Shot In-Context Learning\\n\\nR. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, J. Heek, K. Xiao, S. Agrawal, and J. Dean. Efficiently scaling transformer inference. Proceedings of Machine Learning and Systems, 5, 2023.\\n\\nM. Popovic. chrf++: words helping character n-grams. In Proceedings of the second conference on machine translation, pages 612-618, 2017.\\n\\nA. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\\n\\nD. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman. Gpqa: A graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023.\\n\\nN. R. Robinson, P. Ogayo, D. R. Mortensen, and G. Neubig. Chatgpt mt: Competitive for high-(but not low-) resource languages. arXiv preprint arXiv:2309.07423, 2023.\\n\\nJ. Seipp, A. Torralba, and J. Hoffmann. PDDL generators. https://doi.org/10.5281/ zenodo . 6382173, 2022.\\n\\nA. Singh, J. D. Co-Reyes, R. Agarwal, A. Anand, P. Patil, X. Garcia, P. J. Liu, J. Harrison, J. Lee, K. Xu, A. T. Parisi, A. Kumar, A. A. Alemi, A. Rizkowsky, A. Nova, B. Adlam, B. Bohnet, G. F. Elsayed, H. Sedghi, I. Mordatch, I. Simpson, I. Gur, J. Snoek, J. Pennington, J. Hron, K. Kenealy, K. Swersky, K. Mahajan, L. A. Culp, L. Xiao, M. Bileschi, N. Constant, R. Novak, R. Liu, T. Warkentin, Y. Bansal, E. Dyer, B. Neyshabur, J. Sohl-Dickstein, and N. Fiedel. Beyond human data: Scaling self-training for problem-solving with language models. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https: //openreview.net/ forum?id=1NAyUngGFK. Expert Certification.\\n\\nM. Suzgun, N. Scales, N. Scharli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. H. Chi, D. Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.\\n\\nH. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\\n\\nR. Vacareanu, V.-A. Negru, V. Suciu, and M. Surdeanu. From words to numbers: Your large language model is secretly a capable regressor when given in-context examples. arXiv preprint arXiv:2404.07544, 2024.\\n\\nK. Valmeekam, M. Marquez, S. Sreedharan, and S. Kambhampati. On the planning abilities of large language models-a critical investigation. Advances in Neural Information Processing Systems, 36, 2024.\\n\\nJ. von Oswald, E. Niklasson, E. Randazzo, J. Sacramento, A. Mordvintsev, A. Zhmoginov, and M. Vladymyrov. Transformers learn in-context by gradient descent, Dec. 2022. URL http://arxiv.org/abs/2212.07677. arXiv:2212.07677 [cs].\\n\\nX. Wang, W. Zhu, M. Saxon, M. Steyvers, and W. Y. Wang. Large language models are latent variable models: Explaining and finding good demonstrations for in-context learning. Advances in Neural Information Processing Systems, 36, 2024.\\n\\nJ. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-of- thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824—24837, 2022.\\n\\n20\\n\\nMany-Shot In-Context Learning\\n\\n[63]\\n\\n[64]\\n\\n[65]\\n\\n[66]\\n\\n[67]\\n\\n[68]\\n\\n[69]\\n\\n[70]\\n\\n[71]\\n\\n[72]\\n\\n[73]\\n\\n[74]\\n\\nJ. Wei, J. Wei, Y. Tay, D. Tran, A. Webson, Y. Lu, X. Chen, H. Liu, D. Huang, D. Zhou, et al. Larger language models do in-context learning differently. arXiv preprint arXtv:2303.03846, 2023.\\n\\nH. Wu and K. Tu. Layer-condensed kv cache for efficient inference of large language models. arXiv preprint arXiv:2405.10637, 2024.\\n\\nY. Xiang, H. Yan, L. Gui, and Y. He. Addressing order sensitivity of in-context demonstration examples in causal language models. arXiv preprint arXiv:2402.15637, 2024.\\n\\nS. M. Xie, A. Raghunathan, P. Liang, and T. Ma. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021.\\n\\nY. Xing, X. Lin, N. Suh, Q. Song, and G. Cheng. Benefits of transformer: In-context learning in linear regression tasks with unstructured data. arXiv preprint arXiv:2402.00743, 2024.\\n\\nW. Xiong, J. Liu, I. Molybog, H. Zhang, P. Bhargava, R. Hou, L. Martin, R. Rungta, K. A. Sankararaman, B. Oguz, M. Khabsa, H. Fang, Y. Mehdad, S. Narang, K. Malik, A. Fan, S. Bhosale, S. Edunov, M. Lewis, S. Wang, and H. Ma. Effective long-context scaling of foundation models. CoRR, abs/2309.16039, 2023. doi: 10.48550/ARXIV.2309.16039. URL https://doi.org/ 10.48550/arXiv.2309.16039.\\n\\nK. M. Yoo, J. Kim, H. J. Kim, H. Cho, H. Jo, S. Lee, S. Lee, and T. Kim. Ground-truth labels matter: A deeper look into input-label demonstrations. In Y. Goldberg, Z. Kozareva, and Y. Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 2422-2437. Association for Computational Linguistics, 2022.\\n\\nZ. Yuan, H. Yuan, C. Li, G. Dong, C. Tan, and C. Zhou. Scaling relationship on learning mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825, 2023.\\n\\nJ. Zhang, Y. Zhao, M. Saleh, and P. Liu. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. In International conference on machine learning, pages 11328-11339. PMLR, 2020.\\n\\nR. Zhang, S. Frei, and P. L. Bartlett. Trained transformers learn linear models in-context. arXiv preprint arXiv:2306.09927, 2023.\\n\\nZ. Zhang, A. Zhang, M. Li, and A. Smola. Automatic chain of thought prompting in large language models. In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id=5NTt8GF jUHkr.\\n\\nL. Zheng, J. Yuan, C. Wang, and L. Kong. Efficient attention via control variates. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https: //openreview.net/pdf?id=G-uNfHKrj46.\\n\\n21\\n\\nMany-Shot In-Context Learning\\n\\nA. Appendix\\n\\nA.1. Example Prompts\\n\\nYou are an expert translator. I am going to give you one or more example pairs of text snippets where the first is in English and the second is a translation of the first snippet into Kurdish. The sentences will be written\\n\\nEnglish: <first sentence>\\n\\nKurdish: <translated first sentence>\\n\\nAfter the example pairs, I am going to provide another sentence in English and I want you to translate it into Kurdish. Give only the translation, and no extra commentary, formatting, or chattiness. Translate the text from English to Kurdish.\\n\\nEnglish: Its remnants produced showers across most of the islands, though as of yet, no damage or flooding has been reported.\\n\\nKurdish: Li heréma Serengetiyé, Parka Neteweyi ya Serengeti ya Tanzanyayé, Cihé Parastina Ngorongoro i Cihé Parastina Giyanewerén Néciré Maswa ti Cihé Parastina Neteweyi ya Masai Mara ya Kendyayé hene.\\n\\nEnglish: - -- Kurdish:\\n\\nFigure A.1 | Example prompt with a test input for translation from English to Kurdish on FLORES-MT benchmark in §2.1.\\n\\nI will first show a news article and then provide a very short one sentence long summary of it in fluent English.\\n\\nSummarize the following article: Burberry reported pre-tax profits of £166m for the year to March.\\n\\nA year ago it made a loss of £16.1m, hit by charges at its Spanish operations.\\n\\nIn the past year it has opened 21 new stores and closed nine. It plans to open 20-30 stores this year worldwide.\\n\\nThe group has also focused on promoting the Burberry brand online.\\n\\nSales rose 7% to £1.28bn, with the company recording double-digit sales growth in Europe and Asia Pacific.\\n\\nAdjusted profit rose 23% to £215m, taking into account one-off items and a favourable exchange rate.\\n\\nStores in London in particular benefited from favourable currency movements and increased tourism.\\n\\n“Looking forward, while mindful of the economic environment, Burberry plans to build on its strong financial position by accelerating investment in growth initiatives in retail, digital and new markets, while continuing to enhance the brand,” said chief executive Angela Ahrendts. Burberry shares were up 7.6% at 659 pence in afternoon trading.\\n\\nSummary: Luxury fashion designer Burberry has returned to profit after opening new stores and spending more on online marketing\\n\\nFigure A.2 | Example 1-shot prompt used for summarization on XSum and XLSum in §2.2.\\n\\n22\\n\\nMany-Shot In-Context Learning\\n\\nPlease solve the problem: (define (problem logistics-c2-s1-p1-a2) (:domain logistics-strips) (objects\\n\\na0 al\\n\\nc0 cl\\n\\ntO tl\\n\\n10-0 11-0\\n\\npo\\n\\n)\\n\\nGinit\\n\\n(AIRPLANE a0) (AIRPLANE a1) (CITY c0)\\n\\n(CITY c1) (TRUCK t0) (TRUCK t1) (LOCATION 10-0) (in-city 10-0 c0) (LOCATION 11-0) (in-city 11-0 ¢1) (AIRPORT 10-0) (AIRPORT 11-0) (OBJ pO)\\n\\n(at tO 10-0)\\n\\n(at t1 11-0)\\n\\n(at pO 11-0)\\n\\n(at a0 10-0)\\n\\n(at al 11-0)\\n\\n)\\n\\n(goal\\n\\n(and\\n\\n(at pO 10-0)\\n\\n)\\n\\n)\\n\\n)\\n\\nYour plan as plain text without formatting: (load-airplane pO al 11-0)\\n\\n(fly-airplane al 11-0 10-0)\\n\\n(unload-airplane pO a1 10-0)\\n\\ndone.\\n\\nPlease solve the problem: (define (problem -- -)\\n\\nYour plan as plain text without formatting:\\n\\nFigure A.3 | An example 1-shot PDDL [17] prompt, with a test example for the Logistics domain in §2.3. Within a city, the locations are directly linked, allowing trucks to travel between any two of these locations. Similarly, cities are directly connected to each other allowing airplanes to travel between any two cities. Each city is equipped with one truck and has a designated location that functions as an airport\\n\\nYou will be given a multiple choice question with different choices such as (A), (B), (C), (D). Think step by step before giving a final answer to this question. Always finish your answer with Final Answer: (X)’, where X is the correct answer choice. If none of the options match, choose the closest option as the final answer.\\n\\nFigure A.4 | Zero-shot prompt for GPQA.\\n\\n23\\n\\nMany-Shot In-Context Learning\\n\\n# problem: It starts raining at 7:00\\n\\nCalculate the total time\\n\\n# solution: def solution():\\n\\nCalculate the total time\\n\\nsecond_day_rain_d\\n\\nreturn result\\n\\nYes\\n\\n# problem: She spends two hours s!\\n\\nWhat percentage of the rounded to the nearest\\n\\n# solution: def solution():\\n\\nShe spends two hours s!\\n\\nWhat percentage of the rounded to the nearest\\n\\nfirst_day_rain_duration = 17 - 7 # 10 hours\\n\\nthird_day_rain_duration = second_day_rain_duration * 2 # 24 hours total_rain_duration = first_day_rain_duration + second_day_rain_duration + third_day_rain_duration result = total_rain_duration\\n\\nand pours heavily until its stops at 17:00 on a particular day.\\n\\nOn the second day, the rain takes 2 more hours than it took on the first day to stop. On the third day, the rain pours for twice the amount of time it took on the second day.\\n\\nit was raining in the three days.\\n\\n\"Tt starts raining at 7:00 and pours heavily until its stops at 17:00 on a particular day. On the second day, the rain takes 2 more hours than it took on the first day to stop. On the third day, the rain pours for twice the amount of time it took on the second day.\\n\\nme\\n\\nit was raining in the three days.\\n\\nuration = first_day_rain_duration + 2 # 12 hours\\n\\n# is the solution correct?\\n\\nHaley is getting ready to watch a comet fly over her house.\\n\\nhopping for a telescope, half an hour getting everything set up in the backyard,\\n\\nthree times the setup time making snacks, and 20 minutes watching the comet.\\n\\ntotal time she spent on all those activities was spent watching the comet, ercent?\\n\\n\"Haley is getting ready to watch a comet fly over her house.\\n\\nhopping for a telescope, half an hour getting everything set up in the backyard,\\n\\nthree times the setup time making snacks, and 20 minutes watching the comet.\\n\\ntotal time she spent on all those activities was spent watching the comet, ercent?\"\"\"\\n\\nshopping time = 2 # hours setup_time = 0.5 # hours\\n\\nsnack_time = setuj\\n\\nreturn result\\n\\nNo\\n\\n) time * 3 # hours\\n\\ncomet_time = 20 # minutes\\n\\ntotal_time = shopping time + setup_time + snack_time + comet_time percentage_comet_time = (comet_time / total_time) * 100 rounded_percentage_comet_time = round(percentage_comet_time) result = rounded_percentage_comet_time\\n\\n# is the solution correct?\\n\\nFigure A.5 | An example 2-shot code correctness verifier prompt on GSMS8K. Here, the solutions are generated using the Gemini 1.0 Pro [15] model. See results in §2.4.\\n\\n24\\n\\nMany-Shot In-Context Learning\\n\\nVx=2\\n\\nProblem: Find the domain of the expression Vex =x\\n\\nSolution:\\n\\nI need to find the domain of this expression.\\n\\nThis expression has two square root expressions.\\n\\nI know the values under each square root must be non-negative. So let me start by ensuring that.\\n\\nI set x — 2 > 0, which simplifies to x > 2.\\n\\nThen, I set 5 — x > 0, which simplifies to x < 5.\\n\\nBut I need to make sure the denominator is not zero.\\n\\nSo, 5 — x must also be greater than zero, which gives me x < 5. Therefore, the domain of the expression is | [2, 5) |. Answer: [2,5). Final Answer: The final answer is [2,5). I hope it is correct.\\n\\nProblem: If det A = 2 and det B = 12, then find det(AB).\\n\\nSolution:\\n\\nI need to find the determinant of the product of matrices A and B.\\n\\nI remember that the determinant of the product of two matrices equals the product of their determinants. So, det(AB) = (det A)(det B) = (2)(12) = [24]. Answer: 24.\\n\\nFinal Answer: The final answer is 24. I hope it is correct.\\n\\nProblem: Terrell usually lifts two 20-pound weights 12 times. If he uses two 15-pound weights instead, how many times must Terrell lift them in order to lift the same total weight?\\n\\nSolution:\\n\\nOkay, so Terrell lifts a total of 2 - 12 - 20 = 480 pounds of weight with the 20-pound weights.\\n\\nWell, if he switches to 15-pound weights, the total weight lifted will be 2-15 -n = 30n pounds, where n is the number of lifts.\\n\\nI want to find the number of lifts, n, for the total weight lifted to be the same.\\n\\nI equate 30n to 480 pounds and solve for n.\\n\\n30n = 480\\n\\n=> n=480/30 =| 16\\n\\nAnswer: 16. Final Answer: The final answer is 16. I hope it is correct.\\n\\nProblem: If the system of equations\\n\\n6x - 4y =a, 6y - 9x =b.\\n\\nhas a solution (x, y) where x and y are both nonzero, find ¢, assuming b is nonzero.\\n\\nSolution:\\n\\nI’m given a system of two equations.\\n\\nI see that if I multiply the first equation by -3, Til get another equation that has the same left-hand side as the second equation, 6y — 9x.\\n\\nLet me try that\\n\\n3 6y - 9x = ~3t Ah, I also know that 6y — 9x = b, so I can equate these two equations. So, 3 a 2 ~3% =b> B= 73h\\n\\n~ 2\\n\\nAnswer: —3.\\n\\nFinal Answer: The final answer is -3. I hope it is correct.\\n\\nFigure A.6 | 4-Shot Inner Monologue prompt used for MATH and GSMB8K.\\n\\n25\\n\\nMany-Shot In-Context Learning\\n\\nnput: 255 378 650 363 42 447 898 211 104 145 975 6 827 769 977 901 Output: Foo\\n\\nnput: 111 677 874 692 540 800 771 325 295 106 980 148 275 882 246 136 Output: Foo\\n\\nnput: 136 215 529 65 265 475 45 639 678 95 460 902 746 919 181 838 Output: Foo\\n\\nnput: 62 583 498 50 198 277 519 22 935 351 142 369 349 272 880 125 Output: Bar\\n\\nnput: 101 99 830 735 732 76 243 703 564 3 225 20 136 333 195 441 Output: Bar\\n\\nnput: 242 430 80 153 39 269 898 6 530 524 89 377 238 697 212 539 Output: Bar\\n\\nnput: 261 83 244 37 170 277 161 779 544 272 893 535 71 394 64 607 Output: Bar\\n\\nnput: 402 863 114 193 413 905 894 143 193 288 174 646 411 938 212 285 Output: Bar\\n\\nnput: 869 365 622 671 191 780 492 836 381 450 184 388 604 79 924 926 Output: Foo\\n\\nnput: 548 823 66 658 380 81 779 449 641 673 94 130 258 229 299 278 Output: Bar\\n\\nnput: 700 409 398 375 236 745 32 33 333 173 902 399 176 95 851 897 Output: Foo\\n\\nnput: 673 211 14 221 508 752 147 309 338 23 827 980 373 861 980 946 Output: Foo\\n\\nnput: 528 608 334 210 228 186 559 20 302 93 84 436 726 114 785 865 Output: Bar\\n\\nnput: 117 190 66 628 31 838 183 687 598 11 187 226 381 979 171 39 Output: Bar\\n\\nnput: 802 730 854 392 529 95 15 987 800 266 551 816 145 390 419 686 Output: Foo\\n\\nnput: 723 701 860 30 217 633 226 477 720 839 548 880 277 178 512 585 Output: Foo\\n\\nmput: ---\\n\\nOutput:\\n\\nFigure A.7 | Example prompt with 8 shots per class for the linear classification in 16 dimensions, discussed in §4.2. Here, we use semantically-unrelated labels (‘Foo’ and ‘Bar’) following Wei et al. [63].\\n\\n26\\n\\nMany-Shot In-Context Learning\\n\\nA.2. Prompts for Unsupervised ICL\\n\\nis (—oo, -3] U Problem: Let\\n\\nProblem: Let\\n\\nFind f (29).\\n\\nrespond only\\n\\nSolution: I need to find\\n\\nBut I need to So, 5 — x musi\\n\\nFinal Answer:\\n\\nSolution: I need to fin\\n\\nAnswer: 24. Final Answer:\\n\\nProblem: Eva\\n\\nProblem: Find the domain of the expression\\n\\nTherefore, the domain of the expression is | [2, 5) |. Answer: [2,5).\\n\\nYou will be provided Problems similar to the ones below: Problem: What is the remainder when 369,963 is divided by 6? Problem: The solution to the inequality\\n\\ny= x? +ax+b <0\\n\\n[5, oo). Find the vertex of the parabola y = -x2 +ax +b.\\n\\nx be an angle such that tanx = ¢ and tan2x = as- Then the least positive value of x equals\\n\\ntan7!k. Compute k. Problem: Compute sin 0°.\\n\\nFe) 9x+4 if x is an integer, x) = [x]+5 ifx is not an integer.\\n\\nNow, I am going to give you a series of demonstrations of math Problems and Solutions. When you respond,\\n\\nwith the Solution of the final Problem, thinking step by step.”\\n\\nVx-2 V5-x\"\\n\\nthe domain of this expression.\\n\\nThis expression has two square root expressions.\\n\\nI know the values under each square root must be non-negative. So let me start by ensuring that.\\n\\nI set x — 2 > 0, which simplifies to x > 2.\\n\\nThen, I set 5 — x > 0, which simplifies to x < 5.\\n\\nmake sure the denominator is not zero. t also be greater than zero, which gives me x < 5.\\n\\nThe final answer is [2,5). I hope it is correct.\\n\\nProblem: If det A = 2 and det B = 12, then find det(AB).\\n\\nthe determinant of the product of matrices A and B.\\n\\nI remember that the determinant of the product of two matrices equals the product of their determinants. So, det(AB) = (det A)(det B) = (2)(12) =| 24].\\n\\nThe final answer is 24. I hope it is correct.\\n\\nluate (x + y)(x —y) when x = 13 andy =5.\\n\\nFigure A.8 | Prompt\\n\\nused for Unsupervised ICL with MATH and GSMB8K. We first start with a preamble saying that we are\\n\\ngoing to list a number of problems, and then we list the problems. We then give another pre-amble to specify the output\\n\\nformat, and include\\n\\nup to 4 examples to fully describe this output format. As we go to the many-shot setting with hun\\n\\nof examples, we on\\n\\nly increase the number of problems in the prompt, not the problem-solution pairs at the end.\\n\\nreds\\n\\n27\\n\\nMany-Shot In-Context Learning\\n\\nYou will be provided questions similar to the ones below:\\n\\nQuestion:\\n\\nA large gene has dozens of exons, of which the central ones code for folded triple helical repeats that connect the cytoskeleton with sarcolemma and extracellular space. Each exon usually codes for one folded triple alpha helix. The most common mutations of the gene are central exon deletions that create out-of-frame peptides and progressive degenerative organ waste. A solution is to deliver a Morpholino that recognizes the 5’ end of the out-of-frame exon in pre-mRNA. The molecule prevents binding of the spliceosome and creates exon skipping and in-frame joining. Several missing exons are well tolerated by an organism. Which structure below is not involved in the proposed therapy?\\n\\n(A) antisense\\n\\n(B) polyA tail\\n\\n(C) R-loops\\n\\n(D) lariat\\n\\nQuestion:\\n\\nYou will be given a multiple choice question with different choices such as (A), (B), (C), (D). Think step by step before giving a final answer to this question. Always finish your answer with ’Final Answer: (X)’, where X is the correct answer choice. If none of the options match, choose the closest option as the final answer.\\n\\nFigure A.9 | Unsupervised ICL Prompt for GPQA. We first start with a preamble saying that we are going to list a number of questions, and then we list the questions. We then give another preamble to specify the output format. As we go to the many-shot setting with hundreds of examples, we only increase the number of questions in the prompt.\\n\\nYou will be provided source sentences in English to translate in into Kurdish similar to the ones below:\\n\\nEnglish: Its remnants produced showers across most of the islands, though as of yet, no damage or flooding has been reported.\\n\\nYou are an expert translator. I am going to give you one or more example pairs of text snippets where the first is in English and the second is a translation of the first snippet into Kurdish. The sentences will be written\\n\\nEnglish: <first sentence>\\n\\nKurdish: <translated first sentence>\\n\\nAfter the example pairs, I am going to provide another sentence in English and I want you to translate it into Kurdish. Give only the translation, and no extra commentary, formatting, or chattiness. Translate the text from English to Kurdish.\\n\\nEnglish: Its remnants produced showers across most of the islands, though as of yet, no damage or flooding\\n\\nhas been reported.\\n\\nKurdish: Li heréma Serengetiyé, Parka Neteweyi ya Serengeti ya Tanzanyayé, Cihé Parastina Ngorongoro i\\n\\nCihé Parastina Giyanewerén Néciré Maswa ti Cihé Parastina Neteweyi ya Masai Mara ya Kendyayé hene. English: - -- Kurdish:\\n\\nFigure A.10 | Unsupervised ICL Prompt for the low-resource MT task. We first start with a preamble saying that we are going to list a number of source sentences, and then we list the sentences. We then give another preamble with 1 input-output example to specify the output format. As we go to the many-shot setting with hundreds of examples, we only increase the number of source sentences in the prompt.\\n\\n28\\n\\nMany-Shot In-Context Learning A.3. Unsupervised ICL on Machine Translation\\n\\nTranslation: English > Kurdish\\n\\nS 45} -* ICL (Source and Target) . —e— Unsupervised ICL (Source Only) a 40 uw c rc » 35 wn & 30\\n\\n1 5 10 25 50 125250500997 Number of Shots (K)\\n\\nFigure A.11 | Unsupervised ICL does not work for low-resource machine translation. This is expected as providing\\n\\nonly source sentences for translation task doesn’t improve the task specification. See Figure A.10 for the prompt used for unsupervised ICL for this experiment.\\n\\nA.4. Reinforced ICL: Data-collection Prompt Sensitivity and Iteration 2\\n\\nIter 1 (Minerva) Mm Iter1 Mmm Iter 2 MATHS500: Reinforced ICL 60.0%\\n\\n_ re i °\\n\\n257.5% 2 i om eS\\n\\n>. Ht th i\\n\\n% 55.0% 4- Mini P,\\n\\na\\n\\no 52.5%\\n\\n252.\\n\\nra 0,\\n\\n90.0% shot InnerMono. Prompt 47.5%\\n\\n25 50 125 250 500 Number of Shots (K)\\n\\nFigure A.12 | Reinforced ICL Hendrycks MATH. We find the performance of model-generated rationales with 4-shot Minerva prompt is generally better or comparable to the ones generated by 4-shot InnerMono. MATH prompt. Furthermore, another iteration of Reinforced ICL - generating rationales from the best performing 25-shot prompt (with model-generated\\n\\nrationales) on the MATH training set and using the problems which were not solved in first iteration - seem to further improve many-shot performance.\\n\\nA.5. Linear Classification: Data Generation\\n\\nFor each classification dataset, we randomly sample another N-dimensional vector as the decision boundary and a decision threshold. We then provide K N-dimensional points above this threshold and K points below that same threshold as in-context exemplars, and the model must determine whether unseen N-dimensional points are above or below the threshold (we do not tell the model the equation or the threshold). We provide the python code for date generation below.\\n\\nimport numpy as np def _generate_dataset(minv, maxv, N, k, a, t):\\n\\nxtrain, ytrain = [], [] count_pos, count_neg = 0, 0\\n\\n29\\n\\nMany-Shot In-Context Learning\\n\\nwhile (count_pos < k) or (count_neg < k): x_ex = np.random.randint(minv, maxv, size=N) label = 1 if np.dot(x_ex, a) >t:\\n\\nif count_pos >= k:\\n\\ncontinue count_pos += 1 else: if count_neg >= k: continue count_neg += 1 label = -1\\n\\nxtrain .append(x_ex) ytrain . append (label) return np.array(xtrain).astype(str), np.array(ytrain)\\n\\ndef GENERATEEVAL(N, k, seed): \"\"\"Generates one evaluation example for N-dimensional linear classification.\\n\\nArgs: N: Dimensionality of the data. k: Number of in-context exemplars per class.\\n\\nReturns: xtrain: A list of 2k training examples (k positive, k negative) ytrain: A list of corresponding labels for training examples. xeval: A list of evaluation examples (25 positive, 25 negative) yeval: Ground-truth labels for evaluation examples.\\n\\n# Step 2: Generate ground-truth coefficients np.random. seed (seed) minv, maxv = 1, 1000 a = np.random.randint(minv, maxv, size=N) # Random integer coefficients\\n\\n# Step 3: Generate a pivot point p = np.random.randint(minv, maxv, size=N)\\n\\n# Step 4: Calculate the classification threshold t = np.dot(a, p)\\n\\n# Steps 5: Generate training examples xtrain, ytrain = generate _dataset(minv, maxv, N, k, a, t)\\n\\n# Steps 6: Generate the evaluation example xeval, yeval = _generate_dataset(minv, maxv, N, 25, a, t)\\n\\nreturn xtrain, ytrain, (xeval, yeval)\\n\\nListing 1 | Code for Generating Sythetic datasets for Linear Classification in High Dimensions.\\n\\nA.6. Training GPT-2 from scratch on the sequential parity task\\n\\n: Max num examples\\n\\n10 | in many-shot ICL. 1 1 08 |} 1 1 > 1 Joe | 3 I Performance of @ 04 -}--------------------} AP ---------------- many-shot ICL 1 1 0.2 | | — best small 1 — 0.0 best medium 0.0 0.5 1.0 15 2.0 2.5 3.0 Num examples seen 1e5\\n\\nFigure A.13 | For the sequential parity task, training a transformer from scratch does not meet 8192-shot ICL performance (dashed lines) until 20x the number of examples. We trained two transformers on the sequential parity task (from §4.2). The smaller model was the size of GPT-2 Small, with 12 layers and 768 embedding dimension. The larger model was the size of GPT-2 Medium, with 24 layers and 1024 embedding dimension. We trained using a linear warmup and square root decay schedule, sweeping max learning rate values [le-5, 5e-5, le-4, 5e-4, 1-e3] and num warmup steps [50, 100, 500, 1000, 5000]. The best values for both models (fastest learning) were max_lr=1e-4, warmup_steps=1000.\\n\\n30\\n\\nMany-Shot In-Context Learning\\n\\nA.7. Negative Log-Likelihood on Model-Generated Data\\n\\nNegative Log Likelihood (NLL) on Model Generated Test Solutions with L-shot prompt\\n\\nL=4 L=10 L=25 L=50 0.125 0.10 0.11 a 30.10 a a = 0.100 2 2 3 0.10 0.09 0.075 0.08 0.08 TS Pe PPS TS PS Pes Ss PS PSS Ss PS PSS Num Shots in Scoring Prompt Num Shots in Scoring Prompt Num Shots in Scoring Prompt Num Shots in Scoring Prompt L=125 L=250 L=500 0.14 0.150 0.12 oro yet Z 0.125 = = o10 = 0.100 0.08 RS pS OS RS po pS RS pO PP So LS PP PPS SPP PES PS VPP PES Num Shots in Scoring Prompt Num Shots in Scoring Prompt Num Shots in Scoring Prompt —— Correct —— Incorrect\\n\\nFigure A.14 | NLL For Model-Generated Data. We compute negative log-likelihood on model-generated solutions for Hendrycks MATH. We generate solutions using prompts of varying lengths, starting from (model-generated) 4-shot prompts, and going up to 500-shot prompts. We then score the solutions with (model-generated) prompts of different lengths, from 4-shot to 500-shot prompts. We see that, for the solutions generated using the 4-shot prompt, the NLL is lowest for 4-shot and 10-shot scoring prompts, and increases as the length of the scoring prompt increases. Note that the generation and scoring prompts contain different subsets of examples from the training sets (and are randomized for each sample). Generally speaking, we see that the NLL is lowest when the generation and the scoring prompts are of similar lengths, and higher when they are further apart. This reveals to us that prompts of similar lengths induce similar distributions over the outputs, even when the actual contents of the prompt are different. Further, we see that correct and incorrect solutions follow similar curves, and the gap between NLL for correct and incorrect solutions does not follow any specific pattern.\\n\\nA.8. Many-Shot ICL: Hallucinations on Summarization\\n\\nHere is an example on XSum showing summaries with halluncinated date and time when increasing the number of shots for many-shot ICL, discussed in §2.2.\\n\\nReference Summary: Russia’s leading independent polling agency has been labelled a \"foreign agent\" by the justice ministry and says it cannot now work.\\n\\n1-shot: The Levada Centre, a Russian organization that conducts political opinion surveys, has been designated as a \"foreign agent\" by the government. This label restricts the organization’s activities and effectively prevents it from conducting independent polls, particularly in relation to the upcoming parliamentary elections. Critics view this move as political censorship and an attempt to suppress dissent.\\n\\n5-shot: Russia’s leading independent pollster has been labelled a \"foreign agent\" by the justice ministry, two weeks before parliamentary elections.\\n\\n10-shot: A leading Russian opinion pollster has been labelled a \"foreign agent\" by the authorities, effectively barring it from conducting any work linked to the upcoming parliamentary elections.\\n\\n25-shot: Russia’s leading independent pollster has been labelled a \"foreign agent\" by the justice ministry, effectively barring it from conducting election-related work.\\n\\n50-shot: 12 September 2016 Last updated at 14:32 GMT One of Russia’s last independent pollsters has been designated a \"foreign agent\" by the justice ministry.\\n\\n125-shot: 12 September 2016 Last updated at 14:43 GMT\\n\\nOne of Russia’s last independent pollsters has been branded a \"foreign agent\" by the justice ministry. 250-shot: 2016-09-06 Last updated at 14:33 GMT\\n\\nRussia’s only independent pollster has been branded a \"foreign agent\" by the justice ministry.\\n\\n500-shot: 12 September 2016 Last updated at 14:22 GMT One of Russia’s last independent pollsters has been branded a \"foreign agent\" by the government.\\n\\n31', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "online_loader = OnlinePDFLoader(\"https://arxiv.org/pdf/2404.11018.pdf\")\n",
    "\n",
    "online_pdf_data = online_loader.load()\n",
    "online_pdf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97c868c2-9799-4cd8-af41-6770a6ba2254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95180"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(online_pdf_data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f42f51e9-cfcf-4b57-bc50-86242256fa1e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arXiv:2404.11018v2 [cs.LG] 22 May 2024\\n\\nGoogle DeepMind 2024-5-24\\n\\nMany-Shot In-Context Learning\\n\\nRishabh Agarwal’, Avi Singh”, Lei M. Zhang\", Bernd Bohnet\\', Luis Rosias\\', Stephanie Chan‘, Biao Zhang\", Ankesh Anand , Zaheer Abbas , Azade Nova , John D. Co-Reyes , Eric Chu , Feryal Behbahani , Aleksandra Faust and Hugo Larochelle\\n\\n“Contributed equally, ‘Key contribution\\n\\nLarge language models (LLMs) excel at few-shot in-context learning (ICL) — learning from a few input- output examples (“shots”) provided in context at inference, without any weight updates. Newly expanded context windows allow us to investigate ICL with hundreds or thousands of examples - the many-shot regime. Going from few-shot to many-shot, we observe significant performance gains across a wide variety of generative and discriminative tasks. While promising, many-shot ICL can be bottlenecked by the available amount of human-generated outputs. To mitigate this limitation, we explore two settings: (1) “Reinforced ICL” that uses model-generated chain-of-thought rationales in place of human rationales, and (2) “Unsupervised ICL” where we remove rationales altogether, and prompt the model only with domain-specific inputs. We find that both Reinforced and Unsupervised ICL can be effective in the many-shot regime, particularly on complex reasoning tasks. Furthermore, we demonstrate that, unlike few-shot learning, many-shot learning is effective at overriding pretraining biases, can learn high-dimensional functions with numerical inputs, and performs comparably to fine-tuning. Finally, we reveal the limitations of next-token prediction loss as an indicator of ICL performance.\\n\\n1. Introduction\\n\\n100 Few-Shot ICL {i Many-Shot ICL 19 +18.2 +7. xs +5.0 +10.9 <a 80 Pr FS +21.0 U s 4 60 +7.9 a +15.3 sé +150 +364 192 © Ee 40 ) . ov +5.0 rf x 8 2 L 3 ge . 5 3 3 & 5 @ 4 wv 20 4 3 4 z F & i <i i) a ec a % cd B g + ~ re) a oS 0 = 3 , <8 gga 330% wi go get ey oe ext IP ances) POE 0 eo) OD NET NY ger slower acne igey no Pre 2 xo) ae AVS gid (ge Sof 98S x0 © orrno oo\" yo oe Sas ool war eG Cy week? Be @ ® Fe aN\\n\\nFigure 1 | Many-shot vs Few-Shot In-Context Learning (ICL) across several tasks. Many-shot ICL consistently outperforms few-shot ICL, particularly on difficult non-natural language tasks. Optimal number of shots for many-shot ICL are shown inside the bar for each task. For few-shot ICL, we either use typical number of shots used on a benchmark, for example, 4-shot for MATH, or the longest prompt among the ones we tested with less than the GPT-3 context length of 2048 tokens. Reasoning-oriented tasks, namely MATH, GSM8K, BBH, and GPQA use chain-of-thought rationales. For translation, we report performance on English to Bemba, summarization uses XLSum, MATH corresponds to the MATH500 test set, and sentiment analysis results are reported with semantically-unrelated labels. See §2, §3, and §4 for more details.\\n\\nA limiting factor for in-context learning (ICL) in LLMs is the context window, restricting prior research to the few-shot ICL regime. Many-shot learning — ICL with a large number of shots, for example, hundreds or thousands — allows for better task specification, can reduce the need for fine- tuning, and potentially make LLMs more versatile and adaptable. Exploring many-shot ICL is now\\n\\n© 2024 Google DeepMind. All rights reserved\\n\\nMany-Shot In-Context Learning\\n\\nMany-Shot ICL: Context Length versus Number of Shots\\n\\n2 1x10° F\\n\\no lm Best-Performing Shots x\\n\\n(© —3%10°] mm Maximum Shots\\n\\nco 1 5\\n\\n=1n 1x10\\n\\nBa 4\\n\\nf= 3x10\\n\\nafé 4\\n\\nue 1x 10’\\n\\n£~3x 103 L Cc\\n\\n°\\n\\nOQ 1x10\\n\\nae\" gah NOP ote 0% ai 8) oe tO) cs yest! ON ete) at 225 25) ac ba go seu Se cob eco GR) Gor 0 co ret Qo Sey CaN a0 ye! se\"\\n\\nFigure 2 | Context Length for best-performing and the maximum number of shots tested for each task. The horizontal dashed line shows the context length of GPT-3 (2048 tokens), which is representative of typical few-shot prompts tested in the LLM literature. For several tasks, we observed the best-performing shots correspond to the maximum number of shots we tested, which was often limited by the number of available examples for in-context learning. On some tasks €.g., code verifier, planning), we did observe slight performance deterioration beyond a certain number of shots.\\n\\nfeasible, given the recent increase in context windows of publicly available LLMs by at least 100x: from only a few thousand tokens in GPT-3 [8] and Llama 2 [57] to 1M tokens in Gemini 1.5 Pro [16].\\n\\nIn this paper, we investigate how scaling the number of shots affects ICL performance on a wide variety of tasks (§2): problem solving using MATH [23] and GSM8K [10], question-answering [GPQA, 52], summarization using XSum [43] and XLSum [20], algorithmic reasoning [BBH, 56], reward modeling [Code Verifier, 24], low-resource machine translation [FLORES, 18], planning [Logistics, 54], and sentiment analysis [FP, 40]. Compared to few-shot ICL, many-shot learning performs significant better across these tasks, using several hundreds or thousands of shots (Figure 1). Furthermore, maximum performance is often achieved only once the number of shots reaches up to hundreds of thousands of tokens (Figure 2). Concurrent to our work, recent works explore many-shot ICL to jailbreak LLMs [2] (up to 256 shots) and tackle NLP classification tasks [6] (up to 80K tokens). In our work, we focus on a much wider range of tasks, use a lot more examples (up to 8192 shots), and much longer context lengths (up to 1M tokens). See §5 for a detailed discussion of related work.\\n\\nWhile many-shot ICL holds significant promise, it can be constrained by the need for high-quality, human-generated outputs. To overcome this, we introduce reinforced ICL and unsupervised ICL (§3). Inspired by the efficacy of model-generated solutions for fine-tuning [55], Reinforced ICL involves replacing human-written rationales with model-generated ones, filtered via answer correctness, for in-context learning. Inspired by task-recognition view of ICL [66], we also introduce Unsupervised ICL where we prompt the model with only problems instead of problem-solution pairs. On problem-solving tasks such as MATH, GPQA and Big-Bench Hard, we find that both reinforced and unsupervised ICL with many-shots can be more effective than few-shot ICL with human-generated rationales, with reinforced ICL being more broadly effective.\\n\\nFinally, we empirically study how the learning dynamics of in-context learning changes from few-shot to the many-shot regime (84). We find that with sufficient examples, ICL can overcome pre- training biases, perform comparably to full fine-tuning, and solve high-dimensional prediction tasks with numerical inputs, namely sequential parity prediction and linear classification. This suggests the potential of many-shot ICL to adapt to unseen tasks and domains that might be misaligned with an LLM’s training data. Surprisingly, the order of examples can influence many-shot performance (§4.3) Finally, we demonstrate that long-context scaling laws [2, 68, 27] based on next-token prediction loss may not reliably predict ICL performance on problem-solving and reasoning tasks.\\n\\nMany-Shot In-Context Learning\\n\\nOur key contributions are as follows:\\n\\nScaling ICL (§2): We systematically evaluate ICL performance at different scales of in-context examples for a wide range of tasks with Gemini 1.5 Pro. Our results indicate large performance jumps when transitioning from few-shot to many-shot regime.\\n\\n¢ Reinforced and Unsupervised ICL (§3): We find that using model-generated rationales or only problems can reduce the dependence of many-shot ICL on human-generated data.\\n\\n¢ Analysing ICL (§4): We show that many-shot ICL can overcome pre-training biases, perform comparably to fine-tuning, and learn non-NLP prediction tasks, where few-shot ICL struggles. We also reveal that next-token prediction loss may not be a good predictor of ICL performance.\\n\\n2. Scaling In-Context Learning\\n\\nDuring in-context learning (ICL), the LLM receives a prompt containing a set of input-output examples, also called shots, that illustrate the desired task. At the end of the prompt, we append a test input and allow the LM to make a prediction just by conditioning on the prompt and predicting the next tokens auto-regressively. Recent increase in context windows of LLMs allow using many more shots for ICL than typically used. Exposure to many more shots can lead to better generalization, handle more complex problems than what is possible with few-shot ICL, make fine-tuning less essential, and greater control over model outputs, potentially reducing biases stemming from pre-training.\\n\\nEvaluation We evaluate the many-shot performance of Gemini 1.5 Pro! [16] model with 1 million token context length, the largest publicly available so far. Unless specified otherwise, we use greedy decoding. For reliable results, we randomly sample in-context examples for each K-shot prompt multiple times using different random seeds and report average performance, along with some visualization for performance on individual seeds. To ensure that using more shots provides additional information, any K-shot prompt in our setup includes all in-context examples from prompts with less than K examples. To reduce the inference cost, we use KV caching [49]. Next, we study many-shot ICL on typical LLM use-cases (also see §2.4).\\n\\n2.1. Machine Translation\\n\\nWe consider translation from English to a low-resource target language, where many-shot ICL can complement the existing knowledge within the LLM. We use the target languages with the largest gap reported between LLMs and state-of-the-art systems [53], namely Bemba and Kurdish, from FLORES-200 benchmark [45]. We modify the default 1-shot MT prompt from Gemini Team [15] to include multiple translation pairs as shots from the FLORES dev split (containing 997 examples). We evaluate performance on the first 150 sentences from the test set using chrF2++ [50], a standard metric based on character and word n-gram overlap between generated and reference translation.\\n\\nSee Figure 3 for results. Similar to Robinson et al. [53], we observed small gains in the few-shot regime from 1-shot to 10-shot, particularly on Kurdish. However, when using the entire dev set for many-shot ICL, we observe improvements of 15.3% on Bemba and 4.5% on Kurdish, relative to the 1-shot Gemini prompt. Overall, these results establish the new-state-of-art for these language pairs.\\n\\n1This corresponds to original version in the Gemini 1.5 Tech Report, released in February 2024. We note that the Gemini 1.5 Pro API now serves a newer version starting from April 2024.\\n\\nMany-Shot In-Context Learning\\n\\nMany-shot ICL: Machine Translation Many-shot ICL: Summarization Beef PEGASUS (Fine-tuned on XSum) = —e— English > Bemba S45) English > Kurdish 30) pee nS (iinetuned on ) t o + _ =) gy 40 025 = ina G 35 NLLB (SOTA) a o eee NB sorAy | 2 2 F 20 —— GEM-XSum Fr 30 —— XLSum (Transfer) 2° 27 22 23 2% 2° 2 27 28 QF QW 27 2 2 27 2\\n\\nNumber of Shots (K) Number of Shots (K)\\n\\nFigure 3 | Machine Translation (MT). Test Performance im- Figure 4 | Summarization. As we increase the number of proves monotonically as we increase the number of MT pairs shots from XSum dev set, XSum test performance improves provided as in-context examples during inference. Notably, up to 50 shots and then deteriorates. In contrast, XLSum many-shot ICL outperforms state-of-the-art chRF2++ scores performance typically improves with more shots from XSum. of 35% (NLLB) on Bemba and 40% (Google Translate) on The 500-shot prompt corresponds to 205K tokens. PEGA- Kurdish [53]. We note that 997-shot prompt corresponds to SUS [71] and mTS5 [20] are specialized models fine-tuned around 85K tokens. See an example prompt in Figure A.l. for summarization. See an example prompt in Figure A.2.\\n\\n2.2. Abstractive Summarization\\n\\nTo investigate how scaling ICL examples can impact the comprehension ability of LLMs, we now consider abstractive news summarization using XSum dataset from the GEM benchmark [1]. Using XSum dev set examples containing news articles and summaries, we also evaluate how many-shot ICL generalizes to XLSum [20]. We report performance on 150 test articles using ROUGE-L [35], which measures the longest common subsequence between reference and generated summaries.\\n\\nAs depicted in Figure 4, peak performance with many-shot ICL is remarkably close to specialized models fine-tuned on XSum and XLSum. However, XSum performance declines with more than 50 in-context examples. Surprisingly, we observed the many-shot prompted model occasionally generating summaries with fabricated dates and times (§A.8), despite the absence of such data in the in-context summaries. Nonetheless, performance on XLSum monotonically improves with more shots, demonstrating positive transfer from many-shot learning to a related task.\\n\\n2.3. Planning: Logistics Domain\\n\\nRecent work has highlighted shortcomings in planning abilities of LLMs [59]. To this 40.0% end, we evaluate whether many-shot ICL can improve their ability to generate sim- ple plans on the Logistics domain, a widely used benchmark. The objective in this do- main is to transport packages within cities\\n\\nPlanning: Logistics Domain (600 instances)\\n\\nSuccess Rate br N Ww Oo Oo Oo 8 ge8g BS BS BS\\n\\n: ee . . —e— Gemini 1.5 Pro via trucks, and between cities via airplanes. ---+ GPT-4 (Valmeekam et. al, 2024)\\n\\nWe generate a set of planning problems 0.0%\\n\\nwith 2-3 cities, 1-2 packages, 1 truck and airplane per city using a formal planning language (PDDL) generator [54], result- Figure 5 | In-context Planning. Success rate quickly improves\\n\\ning in 1.3K problems for learning and 600 with up to 10 shots (37K tokens), followed by saturation up to 400\\n\\n: : shots and a sudden performance jump at 800 shots. As a reference, for evaluation. To compute optimal solu- we report 1-shot GPT-4 results from Valmeekam et al. [59]. See tions for each problem, we use the Fast- Figure A.3 for an example 1-shot prompt.\\n\\nDownward planner [21].\\n\\n2° 27 22 2% 27 2> 2° 27 2F 2% QU Number of Shots (K)\\n\\nMany-Shot In-Context Learning\\n\\nAs shown in Figure 5, we observe significant improvement in success rate with increasing numbers of ICL shots. While far from state-of-the-art planning approaches (e.g., Fast-Downward), our results demonstrate the potential of many-shot ICL to improve the commonsense planning abilities of LLMs.\\n\\n2.4. Reward Modelling with Many-Shot ICL: Learning Code Verifiers\\n\\nCode Verifier: Best-of-4 Performance 05 Code Verifier: Conditional Probabilities 82.0% . ° > ~~ P(Yes | Correct) S 2 —|=— P(Yes | Incorrect) = 80.0% 504 > oO 5 78.0% a7 Bo ppp g 0.2\\n\\n4 76.0% 5 wz --- Pass@1 >\\n\\n74.0% —- Best-of-4 tol a\\n\\n0% 1 2 4 8 16 32 64 128 256 512 1 2 4 8 16 32 64 128 256 512 Number of Shots (K) Number of Shots (K)\\n\\nFigure 6 | Learning Verifiers In-Context for checking correctness of GSM8K code solutions. Error bars denotes standard error of mean over 3 seeds. See Figure A.5 for a 2-shot prompt. Best-of-N accuracy. (Left) Average accuracy of top-ranked code solutions (among 4 solutions) based on the verifier score on 200 GSMBK test problems. Best-of-4 selection with 128-shot bridges the gap between Pass@1 accuracy of 77.25% and Pass@4 accuracy of 90% with Gemini 1.0 Pro model. Verifier Confidence. (Right) Conditional Probabilities of the Yes token P(Yes) from the verifier, averaged over all correct and incorrect solutions on test problems.\\n\\nA standard approach to improve LLM reasoning is to use test-time verification [10, 44, 24]. Specifically, an LLM generates multiple candidate solutions for a given problem and a verifier, also known as an outcome reward model, ranks these solutions and selects the best one. Here, we focus on learning such verifiers in-context for code verification.\\n\\nTo create in-context verification examples, we utilize correct and incorrect code solutions in Python generated using Gemini 1.0 Pro [15] on the GSMS8kK train set. In the prompt, each (problem, solution) pair is appended with the question “Is the solution correct?” followed by the Yes or No token according to ground truth correctness. At inference, we modify each test (problem, solution) pair in the same way and record the logit of the Yes and No tokens (denoted by Ly,;, Lyo). To compute the verifier score, we use the normalized probability of the Yes token: P(Yes) = exp(Lyes)/( €XP(Lyes) + €XP(Lno)). We evaluate verifier performance using best-of-4 selection based on the verifier score on 200 problems from GSMBK test set with Gemini 1.0 solutions.\\n\\nAs shown in Figure 6 (left), best-of-4 accuracy with the few-shot prompted verifier significantly improves above pass@1 accuracy with 16 or more in-context examples. Along with an accuracy improvement, the probabilities of the Yes token conditioned on ground-truth correct and incorrect solutions separate with increasing the number of shots up to 256, as shown in Figure 6 (right). Overall, these results show a proof-of-concept that the Gemini model becomes better at verifying correctness of solutions with many-shot ICL.\\n\\n3. Many-shot Learning without Human-Written Rationales\\n\\nMany-shot ICL could potentially be limited by the availability of high-quality human-generated rationales or demonstrations. This is particularly challenging for complex reasoning tasks, such as GPQA [52], where human-generated rationales require significant resources and expert knowledge. In this work, we explore two simple approaches for addressing this issue.\\n\\nMany-Shot In-Context Learning\\n\\nReinforced ICL Recent work [55] proposed a simplified version of Reinforced Self-Training [19], demonstrating that fine-tuning using model-generated rationales can be more effective than human- generated rationales for problem-solving tasks. Inspired by their work, we introduce Reinforced ICL, where we use model-generated rationales for in-context learning. To do so, we use a zero-shot or few-shot chain-of-thought [62] prompt as a starting point to sample multiple rationales for each training problem. Then, we select rationales that obtain the correct final answer (we assume access to ground truth final answers or correctness checks), and arrange them into in-context examples containing (problem, rationale) pairs.\\n\\nOne potential issue with model-generated rationales is that of false positives: it is possible for an incorrect reasoning chain to lead to the correct final answer, and fine-tuning or prompting using such a reasoning chain would typically harm performance. Nevertheless, as we discuss in later sections, we often find model-generated rationales to be at least as effective human-written rationales.\\n\\nUnsupervised ICL We now go one step further than Reinforced ICL: what if we removed rationales from the many-shot prompt altogether, and prompt the model only with inputs? Specifically, the Unsupervised ICL prompt consists of: 1) a preamble, such as, “You will be provided questions similar to the ones below:”, 2) a list of unsolved inputs or problems, and 3) a zero-shot instruction or a few-shot prompt with outputs for the desired output format. See §A.2 for the exact prompts we use.\\n\\nOne hypothesis for how many-shot unsupervised ICL might surpass few-shot learning with human demonstrations is that, when the LLM already possesses the required knowledge to solve a task, any information inserted in the prompt that can narrow down what knowledge is needed for the task becomes helpful. This would be consistent with the view that ICL simply “locates” latent concepts (e.g., math problem-solving) the LLM acquired during pre-training [66, 22, 61]. As such, any of the prompt components — inputs, outputs, and their mapping — can help locate such concepts. While Unsupervised ICL is broadly applicable, it may not perform well, for example, when outputs are critical for specifying the task (Figure 9 and A.11).\\n\\n3.1. Problem-solving: Hendrycks MATH & GSM8K\\n\\nooo-- 4-shot InnerMono. MATH Prompt Mm ICL (Ground-Truth) lm Unsupervised ICL Mm Reinforced ICL\\n\\n60.0% MATH500 GSMB8K (Transfer using MATH prompts) “ew ° Se é 2 95.0% ° 3 eo . 8 e ° oe + e ° J57.5% a a oe ofl 3 S57.5% 8 90.0% \\\\ > 8 8 55.0% é 8 85.0%| ¢ fe 52.5% 80.0% ua 50.0% 75.0% . 47.5% 70.0%! © = Z 7 = =“! 25 50 125 250 500 4 10 25 50 125 250 500 Number of Shots (K) Number of Shots (K)\\n\\nFigure 7 | Many-shot Reinforced and Unsupervised ICL for problem-solving generally outperform ICL with ground-truth MATH solutions. MATH. (Left) The bar plots depict the average performance across five random seeds on the MATH500 test set. Each random seed (denoted by the dots) corresponds to a different subset of problems along with ground truth or model-generated solutions (if any) in the prompt. Transfer to GSM8K. (Right) We see that the prompt obtained from MATH transfers well to the GSM8K test split containing 500 problems. Our results with many-shot ICL outperform the 4-shot Minerva prompt, which obtains a test accuracy of 55.7% on MATHS00 and 90.6% on GSM8K.\\n\\nMany-Shot In-Context Learning\\n\\nWe evaluate Reinforced and Unsupervised ICL on Hendrycks MATH [23], which consists of challenging high school competition-level mathematics problems. We use the MATHS0O0 test set from Lightman et al. [33] to report performance, and our 4-shot MATH prompt for data generation can be found in Figure A.6. For Unsupervised ICL, we append this 4-shot prompt after the unsolved problems (see Figure A.8). For comparison, we also evaluate ICL with human-written solutions (ground-truth) from the MATH training set, with the same problems used for many-shot prompts.\\n\\nOur results are shown in the Figure 7 (left). On MATH500, both Reinforced and Unsupervised ICL outperforms ICL with ground-truth solutions in both the few-shot and many-shot regime. For ICL, we observe that the performance improves with more examples in the prompt up to a point, and then declines (with the peak being at about 125 examples). Performance for Reinforced ICL also improves with the number of examples, and reaches a plateau at around 25 examples (while being about 5% higher than ICL), and unlike ICL, we don’t see a significant drop in performance even for a very large number of examples in the context. Notably, many-shot ICL achieves comparable or superior performance when using only problems compared to using problems with solutions. This suggests solutions may be redundant for eliciting problem-solving via in-context learning on this domain, potentially due to extensive math-related data seen during pretraining.\\n\\nCan many-shot ICL enable out-of-distribution generalization? Singh et al. [55] found that fine- tuning a model on model-generated solutions from MATH resulted in improved test performance on GSM8K [10], which has a different distribution of problems than MATH. Here, we investigate whether many-shot ICL also improves transfer performance on GSMBK, indicating an improvement in general problem-solving abilities from in-context learning. Our results in Figure 7 (right) show that this is indeed the case — Reinforced ICL with MATH prompts excels on GSMB8K, outperforming ICL with ground truth MATH solutions as well as Unsupervised ICL in the many-shot setting with at least 25 shots. This indicates that model-generated solutions can enable better generalization than just using problems or combining them with ground-truth solutions for ICL.\\n\\n3.2. Question Answering: Google-Proof QA (GPQA)\\n\\nGoogle-Proof QA (GPQA): Diamond Split\\n\\n° ° ¥ 45.0% ° ee * el > Ps 0-Shot Prompt fa . @ mmm ICL (Ground-Truth) 3 40.0% hd Mmm Unsupervised ICL < a | | ---- ill Reinforced ICL 3 o\\n\\n35.0%\\n\\n5 10 25 50 125 250 Number of Shots (K)\\n\\nFigure 8 | Many-shot Reinforced and Unsupervised ICL for GPQA. The baseline zero-shot prompt, which is used for generating rationales for Reinforced ICL and appended to the prompt for Unsupervised ICL, obtains a performance of 38.8%. The average test accuracy with 125-shot prompt with both ground-truth or model-generated rationales surpass the 40.4% obtained by Claude-3 Sonnet. As we vary the number of shots, while Unsupervised ICL matches or outperforms the zero-shot prompt, Reinforced ICL consistently outperforms it.\\n\\nGPQA [52] is a multiple-choice QA benchmark, with difficult questions focused on graduate-level reasoning in biology, physics, and chemistry. Following Claude-3 [3], we use the diamond split (198 problems) for evaluation. This split focuses on questions where domain experts agree but experts in other domains struggle despite extended effort and internet access. Remaining 250 questions in non-\\n\\nMany-Shot In-Context Learning\\n\\ndiamond split are used for many-shot ICL with and without human-written rationales. For Reinforced ICL, we use a zero-shot prompt (Figure A.4) to generate multiple rationales on the non-diamond split, solving 129 problems. We also append this zero-shot prompt after the GPQA problems for specifying output format for Unsupervised ICL.\\n\\nAs shown in Figure 8, average test accuracy with ground-truth rationales improves substantially from 5 shots to 125 shots, with the best-performing 125-shot prompt nearly matching the accuracy of the state-of-the-art Claude-3 Opus. However, we do observe a performance degradation with 250 shots. Moreover, Reinforced ICL results indicate that model-generated rationales on GPQA seem to be better than ground-truth rationales up to 25 shots, while resulting in similar performance with more shots. Additionally, Unsupervised ICL does not follow any systematic trend: it sometimes performs better ICL with ground-truth rationales depending on the number of shots, but generally underperforms Reinforced ICL. As noted in Anthropic [3], GPQA is a small evaluation dataset and has an inherent higher variance across different runs, which might explain the non-systematic trends.\\n\\n3.3. Algorithmic and Symbolic Reasoning: Big-Bench Hard\\n\\nMultistep Arithmetic Two Logical Deduction [Seven] Geometric Shapes Salient Translation Error Detection\\n\\n3 10 425 50 100 3 10 25 50 100 3 10 425 50 100 3 10 25 50 100\\n\\nObject Count Reasoning About Colored Objects Dyck Languages Word Sort\\n\\n1,\\n\\n@ 1.00 00 e 0.9 fom vs ‘toto E do<<l. 5 0.50 0.90 0.8 a\\n\\n3 10 25 50 100 3 10 25 50 100 3 10 25 50 100 3 10 25 50 100\\n\\nNumber of Shots Number of Shots Number of Shots Number of Shots\\n\\n===: 3-shot CoT (Human-written) —}— Reinforced ICL —}~ Unsupervised ICL\\n\\nFigure 9 | BIG-Bench Hard. Reinforced and Unsupervised ICL with varying number of shots, averaged across five random seeds. We evaluate test performance on a held-out set of 100 problems. The error bars denote standard deviation. Reinforced ICL outperforms Unsupervised ICL for all tasks, which in turns outperforms the human-written chain-of-thought (CoT) prompt. Averaged across tasks, CoT prompting using human-written rationales gets a success rate of 72.1%, Unsupervised ICL obtains 77.1%, while Reinforced ICL gets 83%.\\n\\nWe now evaluate Reinforced ICL and Unsupervised ICL on BIG-Bench Hard [56], a suite of challenging algorithmic reasoning tasks. To reduce the impact of false positives, we select 8 tasks out of 23 in BIG-Bench Hard for which the likelihood of getting a false positive is low: either the answer string is long, or the number of options for each question is large (at least 6). For Reinforced ICL, we use the standard 3-shot CoT prompt from Suzgun et al. [56] to sample 10 rationales per problem from a training set of 150 problem at a temperature of 1.0. We filter the rationales based on final answer correctness and arrange them into prompts containing 3 to 100 (problem, rationale) pairs.\\n\\nAs shown in Figure 9, Reinforced ICL strongly outperforms Unsupervised ICL for almost all tasks, which in turn outperforms the standard 3-shot CoT prompt. Performance for Reinforced ICL generally improves monotonically with the number of prompts for 7 out of 8 tasks. These results indicate the Reinforced ICL is a more robust technique than Unsupervised ICL, especially for tasks in which the demonstrations contain crucial information about the task. For a few tasks, Reinforced ICL outperforms the human-written 3-shot prompt even in the 3-shot setting. This result suggests that model-generated rationales can sometimes outperform human-written rationales even when controlling for the amount of data, mirroring the results reported by Singh et al. [55] for fine-tuning.\\n\\nMany-Shot In-Context Learning\\n\\n4. Analyzing Many-Shot ICL\\n\\n4.1. Overcoming Pre-training Biases with Many-Shot ICL\\n\\nSentiment Analysis (FP): Replacement Labels\\n\\nSentiment Analysis (FP): Replacement Labels\\n\\n100%\\n\\nPr °o\\n\\nA —-- Abstract labels 7 —— Default labels\\n\\nY —-~ Flipped labels\\n\\n. —-- Abstract labels Lf —— Default labels ~~ Flipped labels\\n\\nTest Accuracy (%) [or] 3 x \\\\ ™ Confidence (Label Probability) ° a —— a ~.\\n\\n2 2 2 2 2 2 2 27 27 2 Number of Shots (K) Number of Shots (K)\\n\\nFigure 10 | Overcoming Pre-Training Bias with Many-Shot ICL. (Left) Many-shot ICL overcomes label flips: Test accuracy for sentiment analysis typically improves with more training shots. Flipped and abstract labels eventually approaching the performance of default labels. (Right) Confidence shift in overcoming bias. For flipped and abstract labels, model confidence in its predicted sentiment labels initially drops, then sharply increases with more training shots to similar value, suggesting a period of overcoming pre-training bias.\\n\\nWhile LLMs demonstrate in-context learning of novel tasks, Kossen et al. [30] suggest that ICL may have difficulty unlearning biases derived from pre-training data. Their experiments, however, focused mainly on few-shot ICL due to LLM context length limitations. Here, we revisit their study using many-shot ICL on the Financial PhraseBank (FP) sentiment analysis dataset [40]. Like Kossen et al. [30], we study label relationships that affect pre-training biases:\\n\\n¢ Flipped Labels: Default labels are rotated, that is, [‘negative’, ‘neutral’, ‘positive’] becomes [‘neutral’, ‘positive’, ‘negative’]. This conflicts with sentiment biases an LLM might have learned.\\n\\n¢ Abstract Labels: We use [‘’, ‘B’, ‘C’], removing any pre-existing sentiment association [63].\\n\\nFor ICL shots, we sample examples from the validation set (with replaced labels) to exhibit the input-label relationship and report the results in Figure 10. With few shots, test accuracy with replacement labels is much lower than with default labels. This suggests that with few-shot ICL, the model struggles to overcome its pre-existing biases from pre-training. However, as the number of shots increases, performance on flipped and abstract labels dramatically improves, approaching that of default labels. For default labels, confidence in predicted labels steadily increases with more shots, as shown in Figure 10 (right). In contrast, for flipped labels, confidence initially drops then sharply increases before reaching a plateau, suggesting a period of overcoming pre-training bias.\\n\\nWe posit that the initial drop in performance and confidence in the few-shot regime may be attributed to the “early ascent” phenomenon [47, 36]: a small number of shots may lead to the retrieval of an incorrect skill, which eventually diminishes as task learning takes effect in the many- shot regime. Overall, these results indicate that many-shot ICL can overcome pre-training biases.\\n\\n4.2. Learning Non-Natural Language Tasks: High-Dimensional Functions\\n\\nWe now test many-shot ICL’s ability to learn abstract mathematical functions with numerical inputs, which let us stress test its generality and applicability to possibly unseen tasks.\\n\\nBinary Linear Classification in High Dimensions Following the setup from Wei et al. [63], we create datasets with N-dimensional inputs vectors and their binary class labels, where each dimension\\n\\nMany-Shot In-Context Learning\\n\\n—— Gemini 1.5 Pro —-—- k-Nearest Neighbors — ----- Random Classification (N = 16) Classification (N = 32) Classification (N = 64) _ 90.0% S > 80.0% g 5 & 70.0% < 2 60.0% 50.0% - 2= 25 27 2° 2° 27 28 2 Qu gm 2 25 27 2 2° 27 28 2 QW QT 22 25 27 2° 2° 27 2 29 Qi QTt Number of Shots Per Class (K) Number of Shots Per Class (K) Number of Shots Per Class (K)\\n\\nFigure 11 | In-Context Classification. Test accuracy for 16, 32 and 64 dimensional linear classification problems, averaged across 5 randomly-generated datasets with 25 points per class for each dataset (250 evaluation points total). As we increase the number of shots, the accuracy improves and approximately tracks the performance of the nearest-neighbor baseline trained from scratch on the same data. We use the default implementation of k-nearest neighbours (with k = 5) from scikit-learn [48]. See Figure A.7 for an example prompt.\\n\\nParity Sequence: 20 Digits\\n\\nInput: 10110001110000100111 Label: Odd Odd Even Odd Odd Odd Odd Even Odd Even Even Even Even Even Odd Odd Odd Even Odd Even\\n\\n—— Gemini 1.5 Pro » Random Chance\\n\\nInput:01100110110011000111 GPT-2 Med (Scratch): 20x data\\n\\nLabel:\\n\\nTest Accuracy (%) (Exact Seq. Match) N oO oO x\\n\\n27 27 2% 2 27 28 29 2 QM gt pg\\n\\nNumber of Shots (K) Figure 12 | Learning Sequential Parity Function In-context. We report test accuracy over 200 unseen inputs, averaged across 3 seeds. Error bars denote standard error of the mean. Task Prompt. (Left) Example prompt with input and output labels of the 20-digit Sequential Parity Function. Test accuracy (Right) Many-shot ICL performance improves almost\\n\\nmonotonically with the number of shots, surpassing performance of GPT-2 Medium sized transformer trained from scratch for 1 forward-backward pass per example on 20x more data.\\n\\nis a random integer in [1, 1000]. See more details in 8A.5. While Wei et al. [63] used only 16 shots per class, we scale ICL up to 2048 shots per class. As shown in Figure 11, while 2048 shots per class perform best when N = 16, we observe slight accuracy decrease beyond 512 shots for higher values of N (Figure 11 C, R). Moreover, many-shot ICL substantially outperforms random-chance accuracy and nearly matches the accuracy of a strong baseline, namely k-nearest neighbors, indicating that many-shot ICL can implement nearest-neighbour search over inputs. This is reminiscent of induction heads that implement prefix matching over sequences [46], a plausible mechanism for ICL abilities.\\n\\nSequential Parity Parity is a fundamental Boolean function that determines if a binary input sequence contains an even or odd number of 1s. Despite their power, transformers trained specifically for in-context learning, struggle to learn the Parity function over 20-digit sequences [7]. In this work, we evaluate how well many-shot ICL performs with a pretrained LLM to learn the sequential parity function f(x) = [f1(x), fa(x),---, fa(x)], where x € {0,1}\" and f;(x) = x1 @xg---@x;, Vie [1,n]. We report the results in Figure 12. We see consistent improvement in test accuracy as we increase the number of shots to 8192. Performance surpasses a GPT-2 Medium sized transformer [51] trained from scratch on 20x more input-output examples (with no repeated examples; §A.6). This result indicates many-shot ICL can implement computations analogous to gradient descent [60].\\n\\n10\\n\\nMany-Shot In-Context Learning\\n\\n4.3. Is Many-Shot ICL Sensitive to Example Ordering?\\n\\nIn few-shot in-context learning (ICL), the order of examples within the prompt can significantly impact model performance [38, 65]. Here, we investigate whether such sensitivity to prompt ordering ob-\\n\\nserved in few-shot ICL persists in many-shot _ scenarios, which remains\\n\\nlargely unexplored. Specifically, we evalu-\\n\\nate ten different random orderings of fixed MATH500: 50-shot Ordering Sensitivity\\n\\n50 in-context examples from MATH training 37° r\\n\\nsplit and evaluate performance on the held-out 1 05.0% \" + |\\n\\nMATHS0O0 test set [33]. 560.0% ° ° : § t—s F As Figure 13 reveals, performance varies sig- 3 55.0% Q ° e g¢ 2 °\\n\\nnificantly across different subareas in MATH500. 8 50.0%] ¢\\n\\nStrikingly, an ordering that that excels in one 45.0%! °\\n\\nsubarea may perform poorly in another, for ex- 9 cae oe et eS 6% (eo\\n\\nample, the best Geometry ordering yields weak e iS cores\\n\\nresults on Number Theory. This fluctuation re-\\n\\nsults in a smaller variation in average perfor- Figure 13 | Many-Shot Sensitivity To Example Ordering. see : Each colored data point represents a different random order-\\n\\nmance compared to individual subareas. Onein- . . . a\\n\\n. . i. ing of 50 in-context examples provided to Gemini 1.5 Pro.\\n\\nteresting extension would be to optimize many-\\n\\nshot prompts using frameworks like DSPy [28] that has been successfully applied for optimizing\\n\\nfew-shot prompts based a given metric. Overall, these findings highlight a key challenge in ensuring\\n\\nreliable results with many-shot ICL for long-context models.\\n\\n4.4. Many-Shot ICL vs. Supervised Fine-Tuning\\n\\nBase Model (@@m_ Supervised FT (@m_ ~Many-Shot ICL\\n\\n~ English > Bemba English > Kurdish\\n\\nBS\\n\\n+ 40 4 4\\n\\n+ a a\\n\\nta ™ ™ “ “\\n\\nE20] & N\\n\\nU\\n\\ni)\\n\\neee eee\\n\\n250 997 250 997\\n\\nNumber of Examples Number of Examples\\n\\nFigure 14 | Comparing SFT with Many-Shot ICL on low-resource translation. We plot mean performance across 3 seeds. The standard deviation is between 0.1% to 0.5%. Base model corresponds to 1-shot performance of Gemini 1.5 Pro.\\n\\nMany-shot ICL could make task-specific fine-tuning less essential or, in some cases, even un- necessary, allowing LLMs to tackle a wider range of tasks without specialization. While supervised fine-tuning (SFT) is the dominant LLM paradigm when making use of hundreds or thousands of examples, it is computationally expensive in terms of training. In contrast, many-shot ICL does not require any training, however it has a larger inference cost, which can be substantially reduced with KV caching [49, 64], which might be available off-the-shelf with context caching [12].\\n\\nHere, we compare many-shot ICL to full fine-tuning for machine translation (§2.1). We run two sets of experiments: one using 250 examples, and another using the entire dev set (997 examples). Our results in Figure 14 show that SFT and ICL performance is quite close for Bemba, while SFT has a slight edge for Kurdish. Overall, these results demonstrate that many-shot ICL can be a viable alternative to SFT for some tasks.\\n\\n11\\n\\nMany-Shot In-Context Learning 4.5. Comparing Many-Shot Abilities of Frontier LLMs\\n\\n—e— Gemini 1.5 Pro GPT-4-Turbo —e— Claude-3-Opus\\n\\nTranslation: English - Bemba Translation: English > Kurdish\\n\\niN o\\n\\nw ua\\n\\nTest chrF2++ (%) WwW uw\\n\\nTest chrF2++ (%)\\n\\n2° 2? 22 23 27 2 2% 27 2F 29 2 2° 2t 22 23 27 2° 2© 27 2% 29 2 Number of Shots (K) Number of Shots (K)\\n\\nFigure 15 | Many-shot ICL with GPT-4-Turbo and Claude-3-Opus [3] on low-resource machine translation (§2.1).\\n\\nThe strong many-shot results with Gemini 1.5 Pro raises the question of whether other long-context frontier LLMs also benefit from many-shot ICL. To do so, we evaluate GPT-4-Turbo (128K context length) and Claude-3-Opus [3] (200K context length) on the low-resource translation (§2.1). For both these models, many-shot ICL scales favorably on Bemba but do not exhibit much improvement on Kurdish. Notably, 1.5 Pro starts lower than Claude-3 on Bemba but improves more rapidly, achieving much higher performance at 997 shots. It also outperforms GPT-4 in few-shot learning and improves\\n\\nfurther with more examples. Overall, these results indicate that frontier LLMs exhibit varying degree of many-shot ICL capability.\\n\\n4.6. Long-context scaling laws may not predict ICL performance\\n\\nNegative Log-Likelinood on Ground-Truth Solutions\\n\\n8 GPQA MATH GSMBK co} £ oe & y 2 $A, gv 0.70 0.8 Lo - - = 7 = Lt 313 s 1D 0.65 0.6 F ee 0.60 eT S44 0.4 ov Zz 5 40 275 50 425 750 4 10 25 50 425750500 4 10 25 50 425750500 Number of Shots (K) Number of Shots (K) Number of Shots (K) —— Reinforced ICL —e— ICL (Ground-Truth) —a— Unsupervised ICL\\n\\nFigure 16 | Negative Log-Likelihood (NLL) as a function of number of shots. We plot NLL on ground truth test set solutions for GPQA, MATH and GSMBK. For GPQA and MATH, questions for Reinforced ICL and Unsupervised ICL comes from the training splits of those datasets. We study GSMB8K in the transfer setting, i.e. questions for Reinforced and Unsupervised ICL come from MATH. The absolute NLL for ICL and Reinforced ICL are not directly comparable to Unsupervised ICL, since they use different prompt formats.\\n\\nPrior works [68, 2, 27] have found that the negative log-likelihood (NLL) for ground-truth test outputs decreases predictably as the context length increases. We confirm this finding for GPQA, Hendrycks MATH and GSM8K with many-shot ICL, and report our results in Figure 16. However, we note that NLL trends are not a strong predictor for downstream task performance. For example, the success rate for both MATH and GPQA with ICL decreases after 125 shots (Figure 7,8), but we do not observe a corresponding increase in the NLL in Figure 16.\\n\\nWe also plot NLL curves for Reinforced and Unsupervised ICL, and find them to generally have a\\n\\n12\\n\\nMany-Shot In-Context Learning\\n\\nsmaller slope when compared to supervised ICL. Interestingly, NLL curves for ICL with ground-truth outputs is lower than with model-generated outputs, even though the latter often performs better. In the GSM8K transfer setting (using MATH problems and solutions to score GSM8K solutions), the change in NLL is close to nil. However, this doesn’t reflect transfer performance on GSM8K, which continues to improve with more examples (Figure 7).\\n\\nOverall, our results demonstrate that NLL is not a reliable proxy when attempting to predict ICL performance for problem-solving domains. This makes intuitive sense: for any given problem, there are a large number of potentially correct CoT solutions that the model can generate, and calculating the log-likelihood on only one such solution may not provide a clear picture for overall model capability. We also explore computing NLL on a diverse set of model-generated outputs on MATH, and our findings are presented in §A.7.\\n\\n5. Related Work\\n\\nScaling in-context learning Brown et al. [8] reported improved performance as you increase the number of examples (up to 64) for in-context learning in LLMs , and later works corroborated this finding [39]. However, very few works have explored using a large number of examples (1000 or above) in the prompt. This is likely due to the fact the context lengths in large language models have been quite limited until recently [16, 3]. One closely related work to ours is from Li et al. [31], who scale the number of examples for in-context learning to 2000. However, Li et al. [31] use a custom model architecture [74] to achieve long context lengths, and only evaluate models of up to 1.3B parameters, which is several orders of magnitude smaller than state-of-the-art language models, and are ineffective for complex tasks, such as GPQA [52].\\n\\nConcurrently to our work, Anil et al. [2] used many-shot prompting (upto 256 shots) to jailbreak language models. In our work, we focus on a much wider range of tasks, use a lot more examples (up to 8192 shots) and use models with much longer context lengths (up to 1M tokens). Also, we explore mitigations for needing many human-generated examples with many-shot ICL. Furthermore, while Anil et al. [2] use many-shot learning to override preferences learned during RLHF phase to elicit the biases stemming from pretraining, our results in §4.1 demonstrate that we can also override pre-training biases themselves. Bertsch et al. [6] also concurrently shows benefits of scaling up in-context learning to many demonstrations on several classification datasets with up to 151 labels, albeit also using smaller context windows of up to 80k tokens (using Llama2-80k [13]).\\n\\nLong-context scaling laws Prior works [68, 2, 27, 16] have reported smaller next-token prediction loss with longer contexts, which Jeon et al. [25] also show using theoretical analysis. Our findings confirm this trend for even longer context lengths, but our analysis reveals some of the limitations of using next-token prediction loss as a metric for evaluating long-context performance, as next-token prediction loss continues to go down even as overall performance plateaus.\\n\\nLearning from self-generated data Numerous recent works [19, 70, 55] propose fine-tuning language models on self-generated data to improve performance. Their approach consists of (1) generate samples from the model and filter them using binary feedback, (2) fine-tune the model on these samples, and (3) repeat this process a few times. In this work, we extend this idea to in-context learning, and study the efficacy of Reinforced ICL in reasoning and problem-solving domains.\\n\\nMany-Shot In-Context Learning\\n\\nSelf-generated data and in-context learning Kim et al. [29] propose using self-generated data for few-shot ICL on classification problems, where they generate demonstrations using the LLM conditioned on the test input for each possible class label, and including these demonstrations in the context when performing the final prediction. Li et al. [32] extend this approach to reasoning and language understanding tasks, where they also generate demonstrations conditioned on the test input. Consistent with our findings, these works show that model-generated demonstrations can outperform human-generated demonstrations in the few-shot regime. Another related approach is AutoCoT [73] that uses a zero-shot CoT prompt to produce model-generated demonstrations for few-shot ICL. To do so, AutoCoT samples diverse questions one-by-one based on embedding-based clustering followed by heuristics-based post-processing for selecting demonstrations.\\n\\nDifferent from above approaches, Reinforced ICL generates demonstrations using the same pro- cedure as Singh et al. [55], does not require clustering, post-processing heuristics, or access to the test inputs for generating demonstrations, and can be applied to any problem for which we can obtain reliable reward signals. Moreover, our work mainly focuses on the utility of randomly-sampled model-generated demonstrations for many-shot ICL.\\n\\nLearning Input-Output Relationships with ICL Numerous works [41, 30, 69, 36] have investigated whether LLMs truly learn input-output relationships during in-context learning. Min et al. [41] found that replacing the ground truth labels in in-context examples with random labels barely effected final performance. Further investigations by Yoo et al. [69] and Kossen et al. [30] found that this finding does not necessarily hold across tasks and model sizes. In particular, Kossen et al. [30], Lin and Lee [36] showed that LLMs can indeed learn input-output relationships via in-context learning, but require more examples in order to do so well. In our work, we extrapolate the trend found in those works to much longer context lengths, showing that pre-training biases can be mostly overcome given enough training examples.\\n\\nLearning Mathematical Functions with LLMs Several prior works investigate whether mathemati- cal functions can be learned with transformers [14, 72, 67, 7]. All these works train transformers specifically to perform in-context learning for such functions. In contrast, we demonstrate that many- shot ICL can learn high-dimensional functions even with pre-trained LLMs. Concurrent to our work, Vacareanu et al. [58] demonstrate that pretrained LLMs are able to perform regression tasks, with performance rivaling that of traditional supervised methods with 500 in-context examples. Our work complement their findings to other synthetic tasks with a much larger number of in-context examples. Dinh et al. [11] fine-tuned GPT-3 on synthetic classification tasks and observed similarities in the decision boundaries learned by the fine-tuned model and kNNs. Our results in Figure 11 show that many-shot ICL also performs comparably to kNNs on high-dimensional classification tasks.\\n\\nComparing ICL with fine-tuning Contrary to task-specific fine-tuning, ICL does not require op- timizing any model weights, allowing LLMs to perform a variety of tasks at inference. As such, several prior works compare fine-tuning with ICL but in few-shot regime. Liu et al. [37] proposed a parameter-efficient few-shot fine-tuning (FT) approach for TO that outperforms few-shot ICL with GPT-3. However, Awadalla et al. [5] argue that few-shot ICL is more robust to distribution shifts than fine-tuning for question answering tasks. Similarly, Asai et al. [4] show better transfer with ICL compared to fine-tuning on some tasks. Mosbach et al. [42] fairly compare ICL with FT by using the same model for both approaches and show that full fine-tuning (FT) generally outperforms ICL in the few-shot regime with 16 examples. More recently, Lin et al. [34] show that few-shot ICL can outperform fine-tuning based approaches for aligning LLMs.\\n\\nMany-Shot In-Context Learning\\n\\nComplementary to prior works, we compare full fine-tuning with many-shot ICL with the same number of examples for low-resource translation. Notably, we find that many-shot ICL performs comparably to FT. Aligned with our findings, Bertsch et al. [6] concurrently show that many-shot ICL generally outperforms parameter-efficient fine-tuning (LoRA) on classification tasks. Overall, many-shot ICL and FT can exhibit comparable behaviors, which we leave for further investigation.\\n\\nExemplar vs. Rule-based ICL generalization Chan et al. [9] indicate that ICL tends to generalize in a more exemplar-based way, compared to rule-based generalization during in-weights learning. Using a clever experiment with blocked attention, Bertsch et al. [6] also argue that the benefits of many in-context demonstrations arise from having access to more similar examples. While our results on in-context linear classification agree with this conclusion, our sequential parity results seem to contradict it. Strikingly, sequential parity was the task on which we saw the most improvement, whereas it should be a task that benefits least from seeing similar examples — after all, the nearest neighbor is always going to give the wrong answer (off by 1 bit). Chan et al. [9] do show that a transformer’s inductive biases towards exemplar-based generalization can be shifted both by the training data and the model size, with larger models being less exemplar-based — perhaps this explains the contradictory findings, given that our work used a larger and much more capable model, though this remains an open question.\\n\\n6. Discussion, Limitations and Future Work\\n\\nWe found significant gains in performance when going from few-shot to many-shot ICL on a wide range of tasks, including translation, summarization, planning, reward modeling, mathematical problem solving, question-answering, algorithmic reasoning, and sentiment analysis. To overcome the challenges of obtaining a large number of high-quality human-written rationales for many-shot ICL, we introduced two regimes: Reinforced ICL and Unsupervised ICL. Moreover, we demonstrate that, unlike few-shot ICL, many-shot ICL is effective at overriding pretraining biases, can learn high-dimensional functions with numerical inputs, and performs comparably to SFT.\\n\\nOne limitation of our work is that it mainly evaluates many-shot ICL with Gemini 1.5 Pro. That said, concurrent works [2, 6] as well as our preliminary results with GPT-4-Turbo and Claude-3- Opus (84.5) indicate that other LLMs can also benefit from many-shot ICL. Future work should focus on evaluating the many-shot abilities of a wide range of long context models, as they become available. Furthermore, many-shot performance can likely serve as a valuable metric for evaluating the quality of long-context models, going beyond the prevalent needle-in-a-haystack test [26].\\n\\nAnother limitation of our work is that we don’t completely understand why performance can sometimes degrades with more examples in the prompt (for example, for MATH). Our analysis found that negative log-likelihood trends are insufficient to explain this degradation, and future work should investigate new directions to shed light on the matter and improving many-shot ICL capabilities. Overall, we hope that this work lays a foundation for understanding and optimizing the use of long-context models for ICL, opening up a new frontier of LLM capabilities.\\n\\nAcknowledgements We would like to thank Gheorghe Comanici for reviewing an early draft of this work. We are also\\n\\ngrateful to Doina Precup, Aviral Kumar, Dale Schuurmans, Ankit Anand, Ross Goroshin, Urvashi Singh, Jannik Kossen, Charline Le Lan, and Daniel Toyoma for helpful discussions.\\n\\nMany-Shot In-Context Learning\\n\\nContribution Statement\\n\\nRA initiated and led the project, ran majority of the many-shot experiments and analysis, came up with reinforced ICL, on-boarded collaborators, wrote the initial draft. AS contributed initial infra for experiments on MATH and GSMB8kK, ran BBH experiments, co-led the fine-tuning experiments, conducted NLL analysis on problem-solving tasks, and wrote several sections.\\n\\nLZ contributed results for in-context verifier. BB contributed the planning logistics task. LR led the fine-tuning experiments. BZ contributed the many-shot results for GPT-4 and Claude-3. AA helped with GPQA, SC contributed the baseline for parity task and both helped edit the paper. AF and HL provided feedback on an early draft. HL also suggested the unsupervised ICL experiments. Others were involved in project discussions and minor edits to the paper.\\n\\nReferences\\n\\n(1]\\n\\n[2]\\n\\n[3]\\n\\n[4]\\n\\n[5\\n\\nfa\\n\\n[6\\n\\nfa\\n\\n[7]\\n\\n[8]\\n\\nS.N. Akter, Z. Yu, A. Muhamed, T. Ou, A. Bauerle, A. A. Cabrera, K. Dholakia, C. Xiong, and G. Neubig. An in-depth look at gemini’s language abilities. arXiv preprint arXiv:2312.11444, 2023.\\n\\nC. Anil, E. Durmus, M. Sharma, J. Benton, S. Kundu, J. Batson, N. Rimsky, M. Tong, J. Mu, D. Ford, F. Mosconi, R. Agrawal, R. Schaeffer, N. Bashkansky, S. Svenningsen, M. Lambert, A. Radhakrishnan, C. Denison, E. J. Hubinger, Y. Bai, T. Bricken, T. Maxwell, N. Schiefer, J. Sully, A. Tamkin, T. Lanham, K. Nguyen, T. Korbak, J. Kaplan, D. Ganguli, S. R. Bowman, E. Perez, R. Grosse, and D. Duvenaud. Many-shot jailbreaking. Technical report, Anthropic, 2024.\\n\\nAnthropic. The claude 3 model family: Opus, sonnet, haiku. Technical Report, 2024.\\n\\nA. Asai, S. Kudugunta, X. V. Yu, T. Blevins, H. Gonen, M. Reid, Y. Tsvetkov, S. Ruder, and H. Hajishirzi. Buffet: Benchmarking large language models for few-shot cross-lingual transfer. arXiv preprint arXiv:2305.14857, 2023.\\n\\nA. Awadalla, M. Wortsman, G. Ilharco, S. Min, I. Magnusson, H. Hajishirzi, and L. Schmidt. Exploring the landscape of distributional robustness for question answering models. arXiv preprint arXiv:2210.12517, 2022.\\n\\nA. Bertsch, M. Ivgi, U. Alon, J. Berant, M. R. Gormley, and G. Neubig. In-Context Learning with Long-Context Models: An In-Depth Exploration, Apr. 2024. URL http: //arxiv.org/abs/ 2405 .00200. arXiv:2405.00200 [cs].\\n\\nS. Bhattamishra, A. Patel, P. Blunsom, and V. Kanade. Understanding in-context learning in transformers and Ilms by learning to learn discrete functions. arXiv preprint arXiv:2310.03016, 2023.\\n\\nT. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Lan- guage models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. Bal- can, and H. Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6- 12, 2020, virtual, 2020. URL https: //proceedings.neurips.cc/paper/2020/hash/ 1457cO0d6bf cb4967418bfb8ac142f64a-Abstract .html.\\n\\n[9]\\n\\nKe) fi]\\n\\n[20]\\n\\n[21]\\n\\n[22]\\n\\nMany-Shot In-Context Learning\\n\\nS. C. Y. Chan, I. Dasgupta, J. Kim, D. Kumaran, A. K. Lampinen, and F. Hill. Transformers generalize differently from information stored in context vs in weights, Oct. 2022. URL http: //arxiv.org/abs/2210.05675. arXiv:2210.05675 [cs].\\n\\nK. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.\\n\\nT. Dinh, Y. Zeng, R. Zhang, Z. Lin, M. Gira, S. Rajput, J.-y. Sohn, D. Papailiopoulos, and K. Lee. Lift: Language-interfaced fine-tuning for non-language machine learning tasks. Advances in Neural Information Processing Systems, 35:11763-11784, 2022.\\n\\nG. A. for Developers. Context caching guide, 2024. URL https://ai.google.dev/ gemini-api/docs/caching.\\n\\nY. Fu, R. Panda, X. Niu, X. Yue, H. Hajishirzi, Y. Kim, and H. Peng. Data Engineering for Scaling Language Models to 128K Context, Feb. 2024. URL http: //arxiv.org/abs/2402.10171. arXiv:2402.10171 [es].\\n\\nS. Garg, D. Tsipras, P. S. Liang, and G. Valiant. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 35: 30583-30598, 2022.\\n\\nG. Gemini Team. Gemini: A family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.\\n\\nG. Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arxiv:2403.05530, 2024.\\n\\nM. Ghallab, A. Howe, C. Knoblock, D. Mcdermott, A. Ram, M. Veloso, D. Weld, and D. Wilkins. PDDL—The Planning Domain Definition Language, 1998.\\n\\nN. Goyal, C. Gao, V. Chaudhary, P. Chen, G. Wenzek, D. Ju, S. Krishnan, M. Ranzato, F. Guzman, and A. Fan. The flores-101 evaluation benchmark for low-resource and multilingual machine translation. Trans. Assoc. Comput. Linguistics, 10:522-538, 2022.\\n\\nC. Gulcehre, T. L. Paine, S. Srinivasan, K. Konyushkova, L. Weerts, A. Sharma, A. Siddhant, A. Ahern, M. Wang, C. Gu, et al. Reinforced self-training (rest) for language modeling. arXiv preprint arXiv:2308.08998, 2023.\\n\\nT. Hasan, A. Bhattacharjee, M. S. Islam, K. S. Mubasshir, Y. Li, Y. Kang, M. S. Rahman, and R. Shahriyar. Xl-sum: Large-scale multilingual abstractive summarization for 44 languages. In C. Zong, F. Xia, W. Li, and R. Navigli, editors, Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021, volume ACL/IJCNLP 2021 of Findings of ACL, pages 4693-4703. Association for Computational Linguistics, 2021.\\n\\nM. Helmert. The fast downward planning system. Journal of Artificial Intelligence Research, 26: 191-246, July 2006. ISSN 1076-9757. doi: 10.1613/jair.1705. URL http: //dx.doi.org/ 10.1613/jair.1705.\\n\\nR. Hendel, M. Geva, and A. Globerson. In-context learning creates task vectors. arXiv preprint arXiv:2310.15916, 2023.\\n\\n[23]\\n\\n[24]\\n\\n[25]\\n\\n[26]\\n\\n[27]\\n\\n[28]\\n\\n[29]\\n\\n[30]\\n\\n[31]\\n\\n[32]\\n\\n[33]\\n\\n[34]\\n\\n[35]\\n\\n[36]\\n\\nMany-Shot In-Context Learning\\n\\nD. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Mea- suring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.\\n\\nA. Hosseini, X. Yuan, N. Malkin, A. Courville, A. Sordoni, and R. Agarwal. V-star: Training verifiers for self-taught reasoners. arXiv preprint arXtv:2402.06457, 2024.\\n\\nH. J. Jeon, J. D. Lee, Q. Lei, and B. Van Roy. An information-theoretic analysis of in-context learning. arXiv preprint arXiv:2401.15530, 2024.\\n\\nG. Kamradt. LLMTest_NeedleInAHaystack. https://github.com/gkamradt/LLMTest_ NeedleInAHaystack, 2023. Accessed: 2024-04-16.\\n\\nJ. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.\\n\\nO. Khattab, A. Singhvi, P. Maheshwari, Z. Zhang, K. Santhanam, S. V. A, S. Haq, A. Sharma, T. T. Joshi, H. Moazam, H. Miller, M. Zaharia, and C. Potts. DSPy: Compiling declarative language model calls into state-of-the-art pipelines. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=sY5N0zY50d.\\n\\nH. J. Kim, H. Cho, J. Kim, T. Kim, K. M. Yoo, and S. Lee. Self-generated in-context learning: Lever- aging auto-regressive language models as a demonstration generator. CoRR, abs/2206.08082, 2022. doi: 10.48550/ARXIV.2206.08082. URL https: //doi.org/10.48550/arXiv.2206. 08082.\\n\\nJ. Kossen, Y. Gal, and T. Rainforth. In-context learning learns label relationships but is not conventional learning. In The Twelfth International Conference on Learning Representations, 2023.\\n\\nM. Li, S. Gong, J. Feng, Y. Xu, J. Zhang, Z. Wu, and L. Kong. In-context learning with many demonstration examples. CoRR, abs/2302.04931, 2023. doi: 10.48550/ARXIV.2302.04931. URL https: //doi.org/10.48550/arXiv .2302.04931.\\n\\nR. Li, G. Wang, and J. Li. Are human-generated demonstrations necessary for in-context learning? In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=frRDT6EOhg.\\n\\nH. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe. Let’s verify step by step. CoRR, abs/2305.20050, 2023. doi: 10.48550/ARXIV.2305.20050. URL https: //doi.org/10.48550/arXiv.2305.20050.\\n\\nB. Y. Lin, A. Ravichander, X. Lu, N. Dziri, M. Sclar, K. Chandu, C. Bhagavatula, and Y. Choi. The unlocking spell on base Ilms: Rethinking alignment via in-context learning. arXiv preprint arXiv:2312.01552, 2023.\\n\\nC.-Y. Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summariza- tion Branches Out, pages 74-81, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https: //aclanthology.org/W04-1013.\\n\\nZ. Lin and K. Lee. Dual operating modes of in-context learning. arXiv preprint arXiv:2402.18819, 2024.\\n\\n[37]\\n\\n[38]\\n\\n[39]\\n\\n[40]\\n\\n[41]\\n\\n[42]\\n\\n[43]\\n\\n[44]\\n\\n[45]\\n\\n[46]\\n\\n[47]\\n\\n[48]\\n\\nMany-Shot In-Context Learning\\n\\nH. Liu, D. Tam, M. Mugeeth, J. Mohta, T. Huang, M. Bansal, and C. A. Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in Neural Information Processing Systems, 35:1950-1965, 2022.\\n\\nY. Lu, M. Bartolo, A. Moore, S. Riedel, and P. Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786, 2021.\\n\\nY. Lu, M. Bartolo, A. Moore, S. Riedel, and P. Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In S. Muresan, P. Nakov, and A. Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 8086-8098. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022. ACL-LONG.556. URL https: //doi.org/10.18653/v1/2022.acl-long.556.\\n\\nP. Malo, A. Sinha, P. Korhonen, J. Wallenius, and P. Takala. Good debt or bad debt: Detecting semantic orientations in economic texts. Journal of the Association for Information Science and Technology, 65(4):782-796, 2014.\\n\\nS. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Hajishirzi, and L. Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In Y. Goldberg, Z. Kozareva, and Y. Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 11048-11064. Association for Computational Linguistics, 2022.\\n\\nM. Mosbach, T. Pimentel, S. Ravfogel, D. Klakow, and Y. Elazar. Few-shot fine-tuning vs. in-context learning: A fair comparison and evaluation. arXiv preprint arXiv:2305.16938, 2023.\\n\\nS. Narayan, S. B. Cohen, and M. Lapata. Don’t give me the details, just the summary! topic- aware convolutional neural networks for extreme summarization. In E. Riloff, D. Chiang, J. Hockenmaier, and J. Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 1797-1807. Association for Computational Linguistics, 2018.\\n\\nA. Ni, S. Iyer, D. Radev, V. Stoyanov, W.-t. Yih, S. Wang, and X. V. Lin. Lever: Learning to verify language-to-code generation with execution. In International Conference on Machine Learning, pages 26106-26128. PMLR, 2023.\\n\\nM. A. NLLB Team. No language left behind: Scaling human-centered machine translation. arXiv preprint, 2022.\\n\\nC. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, S. Johnston, A. Jones, J. Kernion, L. Lovitt, K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish, and C. Olah. In-context learning and induction heads. Transformer Circuits Thread, 2022. https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.\\n\\nJ. Pan. What in-context learning “learns” in-context: Disentangling task recognition and task learning. PhD thesis, Princeton University, 2023.\\n\\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pret- tenhofer, R. Weiss, V. Dubourg, et al. Scikit-learn: Machine learning in python. the Journal of machine Learning research, 12:2825-2830, 2011.\\n\\n[49]\\n\\n[50]\\n\\n[51]\\n\\n[52]\\n\\n[53]\\n\\n[54]\\n\\n[55]\\n\\n[56]\\n\\n[57]\\n\\n[58]\\n\\n[59]\\n\\n[60]\\n\\n[61]\\n\\n[62]\\n\\nMany-Shot In-Context Learning\\n\\nR. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, J. Heek, K. Xiao, S. Agrawal, and J. Dean. Efficiently scaling transformer inference. Proceedings of Machine Learning and Systems, 5, 2023.\\n\\nM. Popovic. chrf++: words helping character n-grams. In Proceedings of the second conference on machine translation, pages 612-618, 2017.\\n\\nA. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\\n\\nD. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman. Gpqa: A graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023.\\n\\nN. R. Robinson, P. Ogayo, D. R. Mortensen, and G. Neubig. Chatgpt mt: Competitive for high-(but not low-) resource languages. arXiv preprint arXiv:2309.07423, 2023.\\n\\nJ. Seipp, A. Torralba, and J. Hoffmann. PDDL generators. https://doi.org/10.5281/ zenodo . 6382173, 2022.\\n\\nA. Singh, J. D. Co-Reyes, R. Agarwal, A. Anand, P. Patil, X. Garcia, P. J. Liu, J. Harrison, J. Lee, K. Xu, A. T. Parisi, A. Kumar, A. A. Alemi, A. Rizkowsky, A. Nova, B. Adlam, B. Bohnet, G. F. Elsayed, H. Sedghi, I. Mordatch, I. Simpson, I. Gur, J. Snoek, J. Pennington, J. Hron, K. Kenealy, K. Swersky, K. Mahajan, L. A. Culp, L. Xiao, M. Bileschi, N. Constant, R. Novak, R. Liu, T. Warkentin, Y. Bansal, E. Dyer, B. Neyshabur, J. Sohl-Dickstein, and N. Fiedel. Beyond human data: Scaling self-training for problem-solving with language models. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https: //openreview.net/ forum?id=1NAyUngGFK. Expert Certification.\\n\\nM. Suzgun, N. Scales, N. Scharli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. H. Chi, D. Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.\\n\\nH. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\\n\\nR. Vacareanu, V.-A. Negru, V. Suciu, and M. Surdeanu. From words to numbers: Your large language model is secretly a capable regressor when given in-context examples. arXiv preprint arXiv:2404.07544, 2024.\\n\\nK. Valmeekam, M. Marquez, S. Sreedharan, and S. Kambhampati. On the planning abilities of large language models-a critical investigation. Advances in Neural Information Processing Systems, 36, 2024.\\n\\nJ. von Oswald, E. Niklasson, E. Randazzo, J. Sacramento, A. Mordvintsev, A. Zhmoginov, and M. Vladymyrov. Transformers learn in-context by gradient descent, Dec. 2022. URL http://arxiv.org/abs/2212.07677. arXiv:2212.07677 [cs].\\n\\nX. Wang, W. Zhu, M. Saxon, M. Steyvers, and W. Y. Wang. Large language models are latent variable models: Explaining and finding good demonstrations for in-context learning. Advances in Neural Information Processing Systems, 36, 2024.\\n\\nJ. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-of- thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824—24837, 2022.\\n\\n20\\n\\nMany-Shot In-Context Learning\\n\\n[63]\\n\\n[64]\\n\\n[65]\\n\\n[66]\\n\\n[67]\\n\\n[68]\\n\\n[69]\\n\\n[70]\\n\\n[71]\\n\\n[72]\\n\\n[73]\\n\\n[74]\\n\\nJ. Wei, J. Wei, Y. Tay, D. Tran, A. Webson, Y. Lu, X. Chen, H. Liu, D. Huang, D. Zhou, et al. Larger language models do in-context learning differently. arXiv preprint arXtv:2303.03846, 2023.\\n\\nH. Wu and K. Tu. Layer-condensed kv cache for efficient inference of large language models. arXiv preprint arXiv:2405.10637, 2024.\\n\\nY. Xiang, H. Yan, L. Gui, and Y. He. Addressing order sensitivity of in-context demonstration examples in causal language models. arXiv preprint arXiv:2402.15637, 2024.\\n\\nS. M. Xie, A. Raghunathan, P. Liang, and T. Ma. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021.\\n\\nY. Xing, X. Lin, N. Suh, Q. Song, and G. Cheng. Benefits of transformer: In-context learning in linear regression tasks with unstructured data. arXiv preprint arXiv:2402.00743, 2024.\\n\\nW. Xiong, J. Liu, I. Molybog, H. Zhang, P. Bhargava, R. Hou, L. Martin, R. Rungta, K. A. Sankararaman, B. Oguz, M. Khabsa, H. Fang, Y. Mehdad, S. Narang, K. Malik, A. Fan, S. Bhosale, S. Edunov, M. Lewis, S. Wang, and H. Ma. Effective long-context scaling of foundation models. CoRR, abs/2309.16039, 2023. doi: 10.48550/ARXIV.2309.16039. URL https://doi.org/ 10.48550/arXiv.2309.16039.\\n\\nK. M. Yoo, J. Kim, H. J. Kim, H. Cho, H. Jo, S. Lee, S. Lee, and T. Kim. Ground-truth labels matter: A deeper look into input-label demonstrations. In Y. Goldberg, Z. Kozareva, and Y. Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 2422-2437. Association for Computational Linguistics, 2022.\\n\\nZ. Yuan, H. Yuan, C. Li, G. Dong, C. Tan, and C. Zhou. Scaling relationship on learning mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825, 2023.\\n\\nJ. Zhang, Y. Zhao, M. Saleh, and P. Liu. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. In International conference on machine learning, pages 11328-11339. PMLR, 2020.\\n\\nR. Zhang, S. Frei, and P. L. Bartlett. Trained transformers learn linear models in-context. arXiv preprint arXiv:2306.09927, 2023.\\n\\nZ. Zhang, A. Zhang, M. Li, and A. Smola. Automatic chain of thought prompting in large language models. In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id=5NTt8GF jUHkr.\\n\\nL. Zheng, J. Yuan, C. Wang, and L. Kong. Efficient attention via control variates. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https: //openreview.net/pdf?id=G-uNfHKrj46.\\n\\n21\\n\\nMany-Shot In-Context Learning\\n\\nA. Appendix\\n\\nA.1. Example Prompts\\n\\nYou are an expert translator. I am going to give you one or more example pairs of text snippets where the first is in English and the second is a translation of the first snippet into Kurdish. The sentences will be written\\n\\nEnglish: <first sentence>\\n\\nKurdish: <translated first sentence>\\n\\nAfter the example pairs, I am going to provide another sentence in English and I want you to translate it into Kurdish. Give only the translation, and no extra commentary, formatting, or chattiness. Translate the text from English to Kurdish.\\n\\nEnglish: Its remnants produced showers across most of the islands, though as of yet, no damage or flooding has been reported.\\n\\nKurdish: Li heréma Serengetiyé, Parka Neteweyi ya Serengeti ya Tanzanyayé, Cihé Parastina Ngorongoro i Cihé Parastina Giyanewerén Néciré Maswa ti Cihé Parastina Neteweyi ya Masai Mara ya Kendyayé hene.\\n\\nEnglish: - -- Kurdish:\\n\\nFigure A.1 | Example prompt with a test input for translation from English to Kurdish on FLORES-MT benchmark in §2.1.\\n\\nI will first show a news article and then provide a very short one sentence long summary of it in fluent English.\\n\\nSummarize the following article: Burberry reported pre-tax profits of £166m for the year to March.\\n\\nA year ago it made a loss of £16.1m, hit by charges at its Spanish operations.\\n\\nIn the past year it has opened 21 new stores and closed nine. It plans to open 20-30 stores this year worldwide.\\n\\nThe group has also focused on promoting the Burberry brand online.\\n\\nSales rose 7% to £1.28bn, with the company recording double-digit sales growth in Europe and Asia Pacific.\\n\\nAdjusted profit rose 23% to £215m, taking into account one-off items and a favourable exchange rate.\\n\\nStores in London in particular benefited from favourable currency movements and increased tourism.\\n\\n“Looking forward, while mindful of the economic environment, Burberry plans to build on its strong financial position by accelerating investment in growth initiatives in retail, digital and new markets, while continuing to enhance the brand,” said chief executive Angela Ahrendts. Burberry shares were up 7.6% at 659 pence in afternoon trading.\\n\\nSummary: Luxury fashion designer Burberry has returned to profit after opening new stores and spending more on online marketing\\n\\nFigure A.2 | Example 1-shot prompt used for summarization on XSum and XLSum in §2.2.\\n\\n22\\n\\nMany-Shot In-Context Learning\\n\\nPlease solve the problem: (define (problem logistics-c2-s1-p1-a2) (:domain logistics-strips) (objects\\n\\na0 al\\n\\nc0 cl\\n\\ntO tl\\n\\n10-0 11-0\\n\\npo\\n\\n)\\n\\nGinit\\n\\n(AIRPLANE a0) (AIRPLANE a1) (CITY c0)\\n\\n(CITY c1) (TRUCK t0) (TRUCK t1) (LOCATION 10-0) (in-city 10-0 c0) (LOCATION 11-0) (in-city 11-0 ¢1) (AIRPORT 10-0) (AIRPORT 11-0) (OBJ pO)\\n\\n(at tO 10-0)\\n\\n(at t1 11-0)\\n\\n(at pO 11-0)\\n\\n(at a0 10-0)\\n\\n(at al 11-0)\\n\\n)\\n\\n(goal\\n\\n(and\\n\\n(at pO 10-0)\\n\\n)\\n\\n)\\n\\n)\\n\\nYour plan as plain text without formatting: (load-airplane pO al 11-0)\\n\\n(fly-airplane al 11-0 10-0)\\n\\n(unload-airplane pO a1 10-0)\\n\\ndone.\\n\\nPlease solve the problem: (define (problem -- -)\\n\\nYour plan as plain text without formatting:\\n\\nFigure A.3 | An example 1-shot PDDL [17] prompt, with a test example for the Logistics domain in §2.3. Within a city, the locations are directly linked, allowing trucks to travel between any two of these locations. Similarly, cities are directly connected to each other allowing airplanes to travel between any two cities. Each city is equipped with one truck and has a designated location that functions as an airport\\n\\nYou will be given a multiple choice question with different choices such as (A), (B), (C), (D). Think step by step before giving a final answer to this question. Always finish your answer with Final Answer: (X)’, where X is the correct answer choice. If none of the options match, choose the closest option as the final answer.\\n\\nFigure A.4 | Zero-shot prompt for GPQA.\\n\\n23\\n\\nMany-Shot In-Context Learning\\n\\n# problem: It starts raining at 7:00\\n\\nCalculate the total time\\n\\n# solution: def solution():\\n\\nCalculate the total time\\n\\nsecond_day_rain_d\\n\\nreturn result\\n\\nYes\\n\\n# problem: She spends two hours s!\\n\\nWhat percentage of the rounded to the nearest\\n\\n# solution: def solution():\\n\\nShe spends two hours s!\\n\\nWhat percentage of the rounded to the nearest\\n\\nfirst_day_rain_duration = 17 - 7 # 10 hours\\n\\nthird_day_rain_duration = second_day_rain_duration * 2 # 24 hours total_rain_duration = first_day_rain_duration + second_day_rain_duration + third_day_rain_duration result = total_rain_duration\\n\\nand pours heavily until its stops at 17:00 on a particular day.\\n\\nOn the second day, the rain takes 2 more hours than it took on the first day to stop. On the third day, the rain pours for twice the amount of time it took on the second day.\\n\\nit was raining in the three days.\\n\\n\"Tt starts raining at 7:00 and pours heavily until its stops at 17:00 on a particular day. On the second day, the rain takes 2 more hours than it took on the first day to stop. On the third day, the rain pours for twice the amount of time it took on the second day.\\n\\nme\\n\\nit was raining in the three days.\\n\\nuration = first_day_rain_duration + 2 # 12 hours\\n\\n# is the solution correct?\\n\\nHaley is getting ready to watch a comet fly over her house.\\n\\nhopping for a telescope, half an hour getting everything set up in the backyard,\\n\\nthree times the setup time making snacks, and 20 minutes watching the comet.\\n\\ntotal time she spent on all those activities was spent watching the comet, ercent?\\n\\n\"Haley is getting ready to watch a comet fly over her house.\\n\\nhopping for a telescope, half an hour getting everything set up in the backyard,\\n\\nthree times the setup time making snacks, and 20 minutes watching the comet.\\n\\ntotal time she spent on all those activities was spent watching the comet, ercent?\"\"\"\\n\\nshopping time = 2 # hours setup_time = 0.5 # hours\\n\\nsnack_time = setuj\\n\\nreturn result\\n\\nNo\\n\\n) time * 3 # hours\\n\\ncomet_time = 20 # minutes\\n\\ntotal_time = shopping time + setup_time + snack_time + comet_time percentage_comet_time = (comet_time / total_time) * 100 rounded_percentage_comet_time = round(percentage_comet_time) result = rounded_percentage_comet_time\\n\\n# is the solution correct?\\n\\nFigure A.5 | An example 2-shot code correctness verifier prompt on GSMS8K. Here, the solutions are generated using the Gemini 1.0 Pro [15] model. See results in §2.4.\\n\\n24\\n\\nMany-Shot In-Context Learning\\n\\nVx=2\\n\\nProblem: Find the domain of the expression Vex =x\\n\\nSolution:\\n\\nI need to find the domain of this expression.\\n\\nThis expression has two square root expressions.\\n\\nI know the values under each square root must be non-negative. So let me start by ensuring that.\\n\\nI set x — 2 > 0, which simplifies to x > 2.\\n\\nThen, I set 5 — x > 0, which simplifies to x < 5.\\n\\nBut I need to make sure the denominator is not zero.\\n\\nSo, 5 — x must also be greater than zero, which gives me x < 5. Therefore, the domain of the expression is | [2, 5) |. Answer: [2,5). Final Answer: The final answer is [2,5). I hope it is correct.\\n\\nProblem: If det A = 2 and det B = 12, then find det(AB).\\n\\nSolution:\\n\\nI need to find the determinant of the product of matrices A and B.\\n\\nI remember that the determinant of the product of two matrices equals the product of their determinants. So, det(AB) = (det A)(det B) = (2)(12) = [24]. Answer: 24.\\n\\nFinal Answer: The final answer is 24. I hope it is correct.\\n\\nProblem: Terrell usually lifts two 20-pound weights 12 times. If he uses two 15-pound weights instead, how many times must Terrell lift them in order to lift the same total weight?\\n\\nSolution:\\n\\nOkay, so Terrell lifts a total of 2 - 12 - 20 = 480 pounds of weight with the 20-pound weights.\\n\\nWell, if he switches to 15-pound weights, the total weight lifted will be 2-15 -n = 30n pounds, where n is the number of lifts.\\n\\nI want to find the number of lifts, n, for the total weight lifted to be the same.\\n\\nI equate 30n to 480 pounds and solve for n.\\n\\n30n = 480\\n\\n=> n=480/30 =| 16\\n\\nAnswer: 16. Final Answer: The final answer is 16. I hope it is correct.\\n\\nProblem: If the system of equations\\n\\n6x - 4y =a, 6y - 9x =b.\\n\\nhas a solution (x, y) where x and y are both nonzero, find ¢, assuming b is nonzero.\\n\\nSolution:\\n\\nI’m given a system of two equations.\\n\\nI see that if I multiply the first equation by -3, Til get another equation that has the same left-hand side as the second equation, 6y — 9x.\\n\\nLet me try that\\n\\n3 6y - 9x = ~3t Ah, I also know that 6y — 9x = b, so I can equate these two equations. So, 3 a 2 ~3% =b> B= 73h\\n\\n~ 2\\n\\nAnswer: —3.\\n\\nFinal Answer: The final answer is -3. I hope it is correct.\\n\\nFigure A.6 | 4-Shot Inner Monologue prompt used for MATH and GSMB8K.\\n\\n25\\n\\nMany-Shot In-Context Learning\\n\\nnput: 255 378 650 363 42 447 898 211 104 145 975 6 827 769 977 901 Output: Foo\\n\\nnput: 111 677 874 692 540 800 771 325 295 106 980 148 275 882 246 136 Output: Foo\\n\\nnput: 136 215 529 65 265 475 45 639 678 95 460 902 746 919 181 838 Output: Foo\\n\\nnput: 62 583 498 50 198 277 519 22 935 351 142 369 349 272 880 125 Output: Bar\\n\\nnput: 101 99 830 735 732 76 243 703 564 3 225 20 136 333 195 441 Output: Bar\\n\\nnput: 242 430 80 153 39 269 898 6 530 524 89 377 238 697 212 539 Output: Bar\\n\\nnput: 261 83 244 37 170 277 161 779 544 272 893 535 71 394 64 607 Output: Bar\\n\\nnput: 402 863 114 193 413 905 894 143 193 288 174 646 411 938 212 285 Output: Bar\\n\\nnput: 869 365 622 671 191 780 492 836 381 450 184 388 604 79 924 926 Output: Foo\\n\\nnput: 548 823 66 658 380 81 779 449 641 673 94 130 258 229 299 278 Output: Bar\\n\\nnput: 700 409 398 375 236 745 32 33 333 173 902 399 176 95 851 897 Output: Foo\\n\\nnput: 673 211 14 221 508 752 147 309 338 23 827 980 373 861 980 946 Output: Foo\\n\\nnput: 528 608 334 210 228 186 559 20 302 93 84 436 726 114 785 865 Output: Bar\\n\\nnput: 117 190 66 628 31 838 183 687 598 11 187 226 381 979 171 39 Output: Bar\\n\\nnput: 802 730 854 392 529 95 15 987 800 266 551 816 145 390 419 686 Output: Foo\\n\\nnput: 723 701 860 30 217 633 226 477 720 839 548 880 277 178 512 585 Output: Foo\\n\\nmput: ---\\n\\nOutput:\\n\\nFigure A.7 | Example prompt with 8 shots per class for the linear classification in 16 dimensions, discussed in §4.2. Here, we use semantically-unrelated labels (‘Foo’ and ‘Bar’) following Wei et al. [63].\\n\\n26\\n\\nMany-Shot In-Context Learning\\n\\nA.2. Prompts for Unsupervised ICL\\n\\nis (—oo, -3] U Problem: Let\\n\\nProblem: Let\\n\\nFind f (29).\\n\\nrespond only\\n\\nSolution: I need to find\\n\\nBut I need to So, 5 — x musi\\n\\nFinal Answer:\\n\\nSolution: I need to fin\\n\\nAnswer: 24. Final Answer:\\n\\nProblem: Eva\\n\\nProblem: Find the domain of the expression\\n\\nTherefore, the domain of the expression is | [2, 5) |. Answer: [2,5).\\n\\nYou will be provided Problems similar to the ones below: Problem: What is the remainder when 369,963 is divided by 6? Problem: The solution to the inequality\\n\\ny= x? +ax+b <0\\n\\n[5, oo). Find the vertex of the parabola y = -x2 +ax +b.\\n\\nx be an angle such that tanx = ¢ and tan2x = as- Then the least positive value of x equals\\n\\ntan7!k. Compute k. Problem: Compute sin 0°.\\n\\nFe) 9x+4 if x is an integer, x) = [x]+5 ifx is not an integer.\\n\\nNow, I am going to give you a series of demonstrations of math Problems and Solutions. When you respond,\\n\\nwith the Solution of the final Problem, thinking step by step.”\\n\\nVx-2 V5-x\"\\n\\nthe domain of this expression.\\n\\nThis expression has two square root expressions.\\n\\nI know the values under each square root must be non-negative. So let me start by ensuring that.\\n\\nI set x — 2 > 0, which simplifies to x > 2.\\n\\nThen, I set 5 — x > 0, which simplifies to x < 5.\\n\\nmake sure the denominator is not zero. t also be greater than zero, which gives me x < 5.\\n\\nThe final answer is [2,5). I hope it is correct.\\n\\nProblem: If det A = 2 and det B = 12, then find det(AB).\\n\\nthe determinant of the product of matrices A and B.\\n\\nI remember that the determinant of the product of two matrices equals the product of their determinants. So, det(AB) = (det A)(det B) = (2)(12) =| 24].\\n\\nThe final answer is 24. I hope it is correct.\\n\\nluate (x + y)(x —y) when x = 13 andy =5.\\n\\nFigure A.8 | Prompt\\n\\nused for Unsupervised ICL with MATH and GSMB8K. We first start with a preamble saying that we are\\n\\ngoing to list a number of problems, and then we list the problems. We then give another pre-amble to specify the output\\n\\nformat, and include\\n\\nup to 4 examples to fully describe this output format. As we go to the many-shot setting with hun\\n\\nof examples, we on\\n\\nly increase the number of problems in the prompt, not the problem-solution pairs at the end.\\n\\nreds\\n\\n27\\n\\nMany-Shot In-Context Learning\\n\\nYou will be provided questions similar to the ones below:\\n\\nQuestion:\\n\\nA large gene has dozens of exons, of which the central ones code for folded triple helical repeats that connect the cytoskeleton with sarcolemma and extracellular space. Each exon usually codes for one folded triple alpha helix. The most common mutations of the gene are central exon deletions that create out-of-frame peptides and progressive degenerative organ waste. A solution is to deliver a Morpholino that recognizes the 5’ end of the out-of-frame exon in pre-mRNA. The molecule prevents binding of the spliceosome and creates exon skipping and in-frame joining. Several missing exons are well tolerated by an organism. Which structure below is not involved in the proposed therapy?\\n\\n(A) antisense\\n\\n(B) polyA tail\\n\\n(C) R-loops\\n\\n(D) lariat\\n\\nQuestion:\\n\\nYou will be given a multiple choice question with different choices such as (A), (B), (C), (D). Think step by step before giving a final answer to this question. Always finish your answer with ’Final Answer: (X)’, where X is the correct answer choice. If none of the options match, choose the closest option as the final answer.\\n\\nFigure A.9 | Unsupervised ICL Prompt for GPQA. We first start with a preamble saying that we are going to list a number of questions, and then we list the questions. We then give another preamble to specify the output format. As we go to the many-shot setting with hundreds of examples, we only increase the number of questions in the prompt.\\n\\nYou will be provided source sentences in English to translate in into Kurdish similar to the ones below:\\n\\nEnglish: Its remnants produced showers across most of the islands, though as of yet, no damage or flooding has been reported.\\n\\nYou are an expert translator. I am going to give you one or more example pairs of text snippets where the first is in English and the second is a translation of the first snippet into Kurdish. The sentences will be written\\n\\nEnglish: <first sentence>\\n\\nKurdish: <translated first sentence>\\n\\nAfter the example pairs, I am going to provide another sentence in English and I want you to translate it into Kurdish. Give only the translation, and no extra commentary, formatting, or chattiness. Translate the text from English to Kurdish.\\n\\nEnglish: Its remnants produced showers across most of the islands, though as of yet, no damage or flooding\\n\\nhas been reported.\\n\\nKurdish: Li heréma Serengetiyé, Parka Neteweyi ya Serengeti ya Tanzanyayé, Cihé Parastina Ngorongoro i\\n\\nCihé Parastina Giyanewerén Néciré Maswa ti Cihé Parastina Neteweyi ya Masai Mara ya Kendyayé hene. English: - -- Kurdish:\\n\\nFigure A.10 | Unsupervised ICL Prompt for the low-resource MT task. We first start with a preamble saying that we are going to list a number of source sentences, and then we list the sentences. We then give another preamble with 1 input-output example to specify the output format. As we go to the many-shot setting with hundreds of examples, we only increase the number of source sentences in the prompt.\\n\\n28\\n\\nMany-Shot In-Context Learning A.3. Unsupervised ICL on Machine Translation\\n\\nTranslation: English > Kurdish\\n\\nS 45} -* ICL (Source and Target) . —e— Unsupervised ICL (Source Only) a 40 uw c rc » 35 wn & 30\\n\\n1 5 10 25 50 125250500997 Number of Shots (K)\\n\\nFigure A.11 | Unsupervised ICL does not work for low-resource machine translation. This is expected as providing\\n\\nonly source sentences for translation task doesn’t improve the task specification. See Figure A.10 for the prompt used for unsupervised ICL for this experiment.\\n\\nA.4. Reinforced ICL: Data-collection Prompt Sensitivity and Iteration 2\\n\\nIter 1 (Minerva) Mm Iter1 Mmm Iter 2 MATHS500: Reinforced ICL 60.0%\\n\\n_ re i °\\n\\n257.5% 2 i om eS\\n\\n>. Ht th i\\n\\n% 55.0% 4- Mini P,\\n\\na\\n\\no 52.5%\\n\\n252.\\n\\nra 0,\\n\\n90.0% shot InnerMono. Prompt 47.5%\\n\\n25 50 125 250 500 Number of Shots (K)\\n\\nFigure A.12 | Reinforced ICL Hendrycks MATH. We find the performance of model-generated rationales with 4-shot Minerva prompt is generally better or comparable to the ones generated by 4-shot InnerMono. MATH prompt. Furthermore, another iteration of Reinforced ICL - generating rationales from the best performing 25-shot prompt (with model-generated\\n\\nrationales) on the MATH training set and using the problems which were not solved in first iteration - seem to further improve many-shot performance.\\n\\nA.5. Linear Classification: Data Generation\\n\\nFor each classification dataset, we randomly sample another N-dimensional vector as the decision boundary and a decision threshold. We then provide K N-dimensional points above this threshold and K points below that same threshold as in-context exemplars, and the model must determine whether unseen N-dimensional points are above or below the threshold (we do not tell the model the equation or the threshold). We provide the python code for date generation below.\\n\\nimport numpy as np def _generate_dataset(minv, maxv, N, k, a, t):\\n\\nxtrain, ytrain = [], [] count_pos, count_neg = 0, 0\\n\\n29\\n\\nMany-Shot In-Context Learning\\n\\nwhile (count_pos < k) or (count_neg < k): x_ex = np.random.randint(minv, maxv, size=N) label = 1 if np.dot(x_ex, a) >t:\\n\\nif count_pos >= k:\\n\\ncontinue count_pos += 1 else: if count_neg >= k: continue count_neg += 1 label = -1\\n\\nxtrain .append(x_ex) ytrain . append (label) return np.array(xtrain).astype(str), np.array(ytrain)\\n\\ndef GENERATEEVAL(N, k, seed): \"\"\"Generates one evaluation example for N-dimensional linear classification.\\n\\nArgs: N: Dimensionality of the data. k: Number of in-context exemplars per class.\\n\\nReturns: xtrain: A list of 2k training examples (k positive, k negative) ytrain: A list of corresponding labels for training examples. xeval: A list of evaluation examples (25 positive, 25 negative) yeval: Ground-truth labels for evaluation examples.\\n\\n# Step 2: Generate ground-truth coefficients np.random. seed (seed) minv, maxv = 1, 1000 a = np.random.randint(minv, maxv, size=N) # Random integer coefficients\\n\\n# Step 3: Generate a pivot point p = np.random.randint(minv, maxv, size=N)\\n\\n# Step 4: Calculate the classification threshold t = np.dot(a, p)\\n\\n# Steps 5: Generate training examples xtrain, ytrain = generate _dataset(minv, maxv, N, k, a, t)\\n\\n# Steps 6: Generate the evaluation example xeval, yeval = _generate_dataset(minv, maxv, N, 25, a, t)\\n\\nreturn xtrain, ytrain, (xeval, yeval)\\n\\nListing 1 | Code for Generating Sythetic datasets for Linear Classification in High Dimensions.\\n\\nA.6. Training GPT-2 from scratch on the sequential parity task\\n\\n: Max num examples\\n\\n10 | in many-shot ICL. 1 1 08 |} 1 1 > 1 Joe | 3 I Performance of @ 04 -}--------------------} AP ---------------- many-shot ICL 1 1 0.2 | | — best small 1 — 0.0 best medium 0.0 0.5 1.0 15 2.0 2.5 3.0 Num examples seen 1e5\\n\\nFigure A.13 | For the sequential parity task, training a transformer from scratch does not meet 8192-shot ICL performance (dashed lines) until 20x the number of examples. We trained two transformers on the sequential parity task (from §4.2). The smaller model was the size of GPT-2 Small, with 12 layers and 768 embedding dimension. The larger model was the size of GPT-2 Medium, with 24 layers and 1024 embedding dimension. We trained using a linear warmup and square root decay schedule, sweeping max learning rate values [le-5, 5e-5, le-4, 5e-4, 1-e3] and num warmup steps [50, 100, 500, 1000, 5000]. The best values for both models (fastest learning) were max_lr=1e-4, warmup_steps=1000.\\n\\n30\\n\\nMany-Shot In-Context Learning\\n\\nA.7. Negative Log-Likelihood on Model-Generated Data\\n\\nNegative Log Likelihood (NLL) on Model Generated Test Solutions with L-shot prompt\\n\\nL=4 L=10 L=25 L=50 0.125 0.10 0.11 a 30.10 a a = 0.100 2 2 3 0.10 0.09 0.075 0.08 0.08 TS Pe PPS TS PS Pes Ss PS PSS Ss PS PSS Num Shots in Scoring Prompt Num Shots in Scoring Prompt Num Shots in Scoring Prompt Num Shots in Scoring Prompt L=125 L=250 L=500 0.14 0.150 0.12 oro yet Z 0.125 = = o10 = 0.100 0.08 RS pS OS RS po pS RS pO PP So LS PP PPS SPP PES PS VPP PES Num Shots in Scoring Prompt Num Shots in Scoring Prompt Num Shots in Scoring Prompt —— Correct —— Incorrect\\n\\nFigure A.14 | NLL For Model-Generated Data. We compute negative log-likelihood on model-generated solutions for Hendrycks MATH. We generate solutions using prompts of varying lengths, starting from (model-generated) 4-shot prompts, and going up to 500-shot prompts. We then score the solutions with (model-generated) prompts of different lengths, from 4-shot to 500-shot prompts. We see that, for the solutions generated using the 4-shot prompt, the NLL is lowest for 4-shot and 10-shot scoring prompts, and increases as the length of the scoring prompt increases. Note that the generation and scoring prompts contain different subsets of examples from the training sets (and are randomized for each sample). Generally speaking, we see that the NLL is lowest when the generation and the scoring prompts are of similar lengths, and higher when they are further apart. This reveals to us that prompts of similar lengths induce similar distributions over the outputs, even when the actual contents of the prompt are different. Further, we see that correct and incorrect solutions follow similar curves, and the gap between NLL for correct and incorrect solutions does not follow any specific pattern.\\n\\nA.8. Many-Shot ICL: Hallucinations on Summarization\\n\\nHere is an example on XSum showing summaries with halluncinated date and time when increasing the number of shots for many-shot ICL, discussed in §2.2.\\n\\nReference Summary: Russia’s leading independent polling agency has been labelled a \"foreign agent\" by the justice ministry and says it cannot now work.\\n\\n1-shot: The Levada Centre, a Russian organization that conducts political opinion surveys, has been designated as a \"foreign agent\" by the government. This label restricts the organization’s activities and effectively prevents it from conducting independent polls, particularly in relation to the upcoming parliamentary elections. Critics view this move as political censorship and an attempt to suppress dissent.\\n\\n5-shot: Russia’s leading independent pollster has been labelled a \"foreign agent\" by the justice ministry, two weeks before parliamentary elections.\\n\\n10-shot: A leading Russian opinion pollster has been labelled a \"foreign agent\" by the authorities, effectively barring it from conducting any work linked to the upcoming parliamentary elections.\\n\\n25-shot: Russia’s leading independent pollster has been labelled a \"foreign agent\" by the justice ministry, effectively barring it from conducting election-related work.\\n\\n50-shot: 12 September 2016 Last updated at 14:32 GMT One of Russia’s last independent pollsters has been designated a \"foreign agent\" by the justice ministry.\\n\\n125-shot: 12 September 2016 Last updated at 14:43 GMT\\n\\nOne of Russia’s last independent pollsters has been branded a \"foreign agent\" by the justice ministry. 250-shot: 2016-09-06 Last updated at 14:33 GMT\\n\\nRussia’s only independent pollster has been branded a \"foreign agent\" by the justice ministry.\\n\\n500-shot: 12 September 2016 Last updated at 14:22 GMT One of Russia’s last independent pollsters has been branded a \"foreign agent\" by the government.\\n\\n31'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "online_pdf_data[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b003256-99d1-4704-8609-121f322bc6a5",
   "metadata": {},
   "source": [
    "## SingleStoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38efaa84-f5d9-4088-81e2-d3be1c32a279",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='arXiv:2404.11018v2 [cs.LG] 22 May 2024\\n\\nGoogle DeepMind 2024-5-24\\n\\nMany-Shot In-Context Learning\\n\\nRishabh Agarwal’, Avi Singh”, Lei M. Zhang\", Bernd Bohnet\\', Luis Rosias\\', Stephanie Chan‘, Biao Zhang\", Ankesh Anand , Zaheer Abbas , Azade Nova , John D. Co-Reyes , Eric Chu , Feryal Behbahani , Aleksandra Faust and Hugo Larochelle\\n\\n“Contributed equally, ‘Key contribution\\n\\nLarge language models (LLMs) excel at few-shot in-context learning (ICL) — learning from a few input- output examples (“shots”) provided in context at inference, without any weight updates. Newly expanded context windows allow us to investigate ICL with hundreds or thousands of examples - the many-shot regime. Going from few-shot to many-shot, we observe significant performance gains across a wide variety of generative and discriminative tasks. While promising, many-shot ICL can be bottlenecked by the available amount of human-generated outputs. To mitigate this limitation, we explore two settings: (1) “Reinforced ICL” that uses model-generated chain-of-thought rationales in place of human rationales, and (2) “Unsupervised ICL” where we remove rationales altogether, and prompt the model only with domain-specific inputs. We find that both Reinforced and Unsupervised ICL can be effective in the many-shot regime, particularly on complex reasoning tasks. Furthermore, we demonstrate that, unlike few-shot learning, many-shot learning is effective at overriding pretraining biases, can learn high-dimensional functions with numerical inputs, and performs comparably to fine-tuning. Finally, we reveal the limitations of next-token prediction loss as an indicator of ICL performance.\\n\\n1. Introduction', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='100 Few-Shot ICL {i Many-Shot ICL 19 +18.2 +7. xs +5.0 +10.9 <a 80 Pr FS +21.0 U s 4 60 +7.9 a +15.3 sé +150 +364 192 © Ee 40 ) . ov +5.0 rf x 8 2 L 3 ge . 5 3 3 & 5 @ 4 wv 20 4 3 4 z F & i <i i) a ec a % cd B g + ~ re) a oS 0 = 3 , <8 gga 330% wi go get ey oe ext IP ances) POE 0 eo) OD NET NY ger slower acne igey no Pre 2 xo) ae AVS gid (ge Sof 98S x0 © orrno oo\" yo oe Sas ool war eG Cy week? Be @ ® Fe aN\\n\\nFigure 1 | Many-shot vs Few-Shot In-Context Learning (ICL) across several tasks. Many-shot ICL consistently outperforms few-shot ICL, particularly on difficult non-natural language tasks. Optimal number of shots for many-shot ICL are shown inside the bar for each task. For few-shot ICL, we either use typical number of shots used on a benchmark, for example, 4-shot for MATH, or the longest prompt among the ones we tested with less than the GPT-3 context length of 2048 tokens. Reasoning-oriented tasks, namely MATH, GSM8K, BBH, and GPQA use chain-of-thought rationales. For translation, we report performance on English to Bemba, summarization uses XLSum, MATH corresponds to the MATH500 test set, and sentiment analysis results are reported with semantically-unrelated labels. See §2, §3, and §4 for more details.\\n\\nA limiting factor for in-context learning (ICL) in LLMs is the context window, restricting prior research to the few-shot ICL regime. Many-shot learning — ICL with a large number of shots, for example, hundreds or thousands — allows for better task specification, can reduce the need for fine- tuning, and potentially make LLMs more versatile and adaptable. Exploring many-shot ICL is now\\n\\n© 2024 Google DeepMind. All rights reserved\\n\\nMany-Shot In-Context Learning\\n\\nMany-Shot ICL: Context Length versus Number of Shots\\n\\n2 1x10° F\\n\\no lm Best-Performing Shots x\\n\\n(© —3%10°] mm Maximum Shots\\n\\nco 1 5\\n\\n=1n 1x10\\n\\nBa 4\\n\\nf= 3x10\\n\\nafé 4\\n\\nue 1x 10’\\n\\n£~3x 103 L Cc\\n\\n°\\n\\nOQ 1x10', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='ae\" gah NOP ote 0% ai 8) oe tO) cs yest! ON ete) at 225 25) ac ba go seu Se cob eco GR) Gor 0 co ret Qo Sey CaN a0 ye! se\"\\n\\nFigure 2 | Context Length for best-performing and the maximum number of shots tested for each task. The horizontal dashed line shows the context length of GPT-3 (2048 tokens), which is representative of typical few-shot prompts tested in the LLM literature. For several tasks, we observed the best-performing shots correspond to the maximum number of shots we tested, which was often limited by the number of available examples for in-context learning. On some tasks €.g., code verifier, planning), we did observe slight performance deterioration beyond a certain number of shots.\\n\\nfeasible, given the recent increase in context windows of publicly available LLMs by at least 100x: from only a few thousand tokens in GPT-3 [8] and Llama 2 [57] to 1M tokens in Gemini 1.5 Pro [16].\\n\\nIn this paper, we investigate how scaling the number of shots affects ICL performance on a wide variety of tasks (§2): problem solving using MATH [23] and GSM8K [10], question-answering [GPQA, 52], summarization using XSum [43] and XLSum [20], algorithmic reasoning [BBH, 56], reward modeling [Code Verifier, 24], low-resource machine translation [FLORES, 18], planning [Logistics, 54], and sentiment analysis [FP, 40]. Compared to few-shot ICL, many-shot learning performs significant better across these tasks, using several hundreds or thousands of shots (Figure 1). Furthermore, maximum performance is often achieved only once the number of shots reaches up to hundreds of thousands of tokens (Figure 2). Concurrent to our work, recent works explore many-shot ICL to jailbreak LLMs [2] (up to 256 shots) and tackle NLP classification tasks [6] (up to 80K tokens). In our work, we focus on a much wider range of tasks, use a lot more examples (up to 8192 shots), and much longer context lengths (up to 1M tokens). See §5 for a detailed discussion of related work.', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='While many-shot ICL holds significant promise, it can be constrained by the need for high-quality, human-generated outputs. To overcome this, we introduce reinforced ICL and unsupervised ICL (§3). Inspired by the efficacy of model-generated solutions for fine-tuning [55], Reinforced ICL involves replacing human-written rationales with model-generated ones, filtered via answer correctness, for in-context learning. Inspired by task-recognition view of ICL [66], we also introduce Unsupervised ICL where we prompt the model with only problems instead of problem-solution pairs. On problem-solving tasks such as MATH, GPQA and Big-Bench Hard, we find that both reinforced and unsupervised ICL with many-shots can be more effective than few-shot ICL with human-generated rationales, with reinforced ICL being more broadly effective.\\n\\nFinally, we empirically study how the learning dynamics of in-context learning changes from few-shot to the many-shot regime (84). We find that with sufficient examples, ICL can overcome pre- training biases, perform comparably to full fine-tuning, and solve high-dimensional prediction tasks with numerical inputs, namely sequential parity prediction and linear classification. This suggests the potential of many-shot ICL to adapt to unseen tasks and domains that might be misaligned with an LLM’s training data. Surprisingly, the order of examples can influence many-shot performance (§4.3) Finally, we demonstrate that long-context scaling laws [2, 68, 27] based on next-token prediction loss may not reliably predict ICL performance on problem-solving and reasoning tasks.\\n\\nMany-Shot In-Context Learning\\n\\nOur key contributions are as follows:\\n\\nScaling ICL (§2): We systematically evaluate ICL performance at different scales of in-context examples for a wide range of tasks with Gemini 1.5 Pro. Our results indicate large performance jumps when transitioning from few-shot to many-shot regime.', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='¢ Reinforced and Unsupervised ICL (§3): We find that using model-generated rationales or only problems can reduce the dependence of many-shot ICL on human-generated data.\\n\\n¢ Analysing ICL (§4): We show that many-shot ICL can overcome pre-training biases, perform comparably to fine-tuning, and learn non-NLP prediction tasks, where few-shot ICL struggles. We also reveal that next-token prediction loss may not be a good predictor of ICL performance.\\n\\n2. Scaling In-Context Learning\\n\\nDuring in-context learning (ICL), the LLM receives a prompt containing a set of input-output examples, also called shots, that illustrate the desired task. At the end of the prompt, we append a test input and allow the LM to make a prediction just by conditioning on the prompt and predicting the next tokens auto-regressively. Recent increase in context windows of LLMs allow using many more shots for ICL than typically used. Exposure to many more shots can lead to better generalization, handle more complex problems than what is possible with few-shot ICL, make fine-tuning less essential, and greater control over model outputs, potentially reducing biases stemming from pre-training.\\n\\nEvaluation We evaluate the many-shot performance of Gemini 1.5 Pro! [16] model with 1 million token context length, the largest publicly available so far. Unless specified otherwise, we use greedy decoding. For reliable results, we randomly sample in-context examples for each K-shot prompt multiple times using different random seeds and report average performance, along with some visualization for performance on individual seeds. To ensure that using more shots provides additional information, any K-shot prompt in our setup includes all in-context examples from prompts with less than K examples. To reduce the inference cost, we use KV caching [49]. Next, we study many-shot ICL on typical LLM use-cases (also see §2.4).\\n\\n2.1. Machine Translation', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='We consider translation from English to a low-resource target language, where many-shot ICL can complement the existing knowledge within the LLM. We use the target languages with the largest gap reported between LLMs and state-of-the-art systems [53], namely Bemba and Kurdish, from FLORES-200 benchmark [45]. We modify the default 1-shot MT prompt from Gemini Team [15] to include multiple translation pairs as shots from the FLORES dev split (containing 997 examples). We evaluate performance on the first 150 sentences from the test set using chrF2++ [50], a standard metric based on character and word n-gram overlap between generated and reference translation.\\n\\nSee Figure 3 for results. Similar to Robinson et al. [53], we observed small gains in the few-shot regime from 1-shot to 10-shot, particularly on Kurdish. However, when using the entire dev set for many-shot ICL, we observe improvements of 15.3% on Bemba and 4.5% on Kurdish, relative to the 1-shot Gemini prompt. Overall, these results establish the new-state-of-art for these language pairs.\\n\\n1This corresponds to original version in the Gemini 1.5 Tech Report, released in February 2024. We note that the Gemini 1.5 Pro API now serves a newer version starting from April 2024.\\n\\nMany-Shot In-Context Learning\\n\\nMany-shot ICL: Machine Translation Many-shot ICL: Summarization Beef PEGASUS (Fine-tuned on XSum) = —e— English > Bemba S45) English > Kurdish 30) pee nS (iinetuned on ) t o + _ =) gy 40 025 = ina G 35 NLLB (SOTA) a o eee NB sorAy | 2 2 F 20 —— GEM-XSum Fr 30 —— XLSum (Transfer) 2° 27 22 23 2% 2° 2 27 28 QF QW 27 2 2 27 2\\n\\nNumber of Shots (K) Number of Shots (K)', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='Figure 3 | Machine Translation (MT). Test Performance im- Figure 4 | Summarization. As we increase the number of proves monotonically as we increase the number of MT pairs shots from XSum dev set, XSum test performance improves provided as in-context examples during inference. Notably, up to 50 shots and then deteriorates. In contrast, XLSum many-shot ICL outperforms state-of-the-art chRF2++ scores performance typically improves with more shots from XSum. of 35% (NLLB) on Bemba and 40% (Google Translate) on The 500-shot prompt corresponds to 205K tokens. PEGA- Kurdish [53]. We note that 997-shot prompt corresponds to SUS [71] and mTS5 [20] are specialized models fine-tuned around 85K tokens. See an example prompt in Figure A.l. for summarization. See an example prompt in Figure A.2.\\n\\n2.2. Abstractive Summarization\\n\\nTo investigate how scaling ICL examples can impact the comprehension ability of LLMs, we now consider abstractive news summarization using XSum dataset from the GEM benchmark [1]. Using XSum dev set examples containing news articles and summaries, we also evaluate how many-shot ICL generalizes to XLSum [20]. We report performance on 150 test articles using ROUGE-L [35], which measures the longest common subsequence between reference and generated summaries.\\n\\nAs depicted in Figure 4, peak performance with many-shot ICL is remarkably close to specialized models fine-tuned on XSum and XLSum. However, XSum performance declines with more than 50 in-context examples. Surprisingly, we observed the many-shot prompted model occasionally generating summaries with fabricated dates and times (§A.8), despite the absence of such data in the in-context summaries. Nonetheless, performance on XLSum monotonically improves with more shots, demonstrating positive transfer from many-shot learning to a related task.\\n\\n2.3. Planning: Logistics Domain', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='Recent work has highlighted shortcomings in planning abilities of LLMs [59]. To this 40.0% end, we evaluate whether many-shot ICL can improve their ability to generate sim- ple plans on the Logistics domain, a widely used benchmark. The objective in this do- main is to transport packages within cities\\n\\nPlanning: Logistics Domain (600 instances)\\n\\nSuccess Rate br N Ww Oo Oo Oo 8 ge8g BS BS BS\\n\\n: ee . . —e— Gemini 1.5 Pro via trucks, and between cities via airplanes. ---+ GPT-4 (Valmeekam et. al, 2024)\\n\\nWe generate a set of planning problems 0.0%\\n\\nwith 2-3 cities, 1-2 packages, 1 truck and airplane per city using a formal planning language (PDDL) generator [54], result- Figure 5 | In-context Planning. Success rate quickly improves\\n\\ning in 1.3K problems for learning and 600 with up to 10 shots (37K tokens), followed by saturation up to 400\\n\\n: : shots and a sudden performance jump at 800 shots. As a reference, for evaluation. To compute optimal solu- we report 1-shot GPT-4 results from Valmeekam et al. [59]. See tions for each problem, we use the Fast- Figure A.3 for an example 1-shot prompt.\\n\\nDownward planner [21].\\n\\n2° 27 22 2% 27 2> 2° 27 2F 2% QU Number of Shots (K)\\n\\nMany-Shot In-Context Learning\\n\\nAs shown in Figure 5, we observe significant improvement in success rate with increasing numbers of ICL shots. While far from state-of-the-art planning approaches (e.g., Fast-Downward), our results demonstrate the potential of many-shot ICL to improve the commonsense planning abilities of LLMs.\\n\\n2.4. Reward Modelling with Many-Shot ICL: Learning Code Verifiers\\n\\nCode Verifier: Best-of-4 Performance 05 Code Verifier: Conditional Probabilities 82.0% . ° > ~~ P(Yes | Correct) S 2 —|=— P(Yes | Incorrect) = 80.0% 504 > oO 5 78.0% a7 Bo ppp g 0.2\\n\\n4 76.0% 5 wz --- Pass@1 >\\n\\n74.0% —- Best-of-4 tol a\\n\\n0% 1 2 4 8 16 32 64 128 256 512 1 2 4 8 16 32 64 128 256 512 Number of Shots (K) Number of Shots (K)', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='Figure 6 | Learning Verifiers In-Context for checking correctness of GSM8K code solutions. Error bars denotes standard error of mean over 3 seeds. See Figure A.5 for a 2-shot prompt. Best-of-N accuracy. (Left) Average accuracy of top-ranked code solutions (among 4 solutions) based on the verifier score on 200 GSMBK test problems. Best-of-4 selection with 128-shot bridges the gap between Pass@1 accuracy of 77.25% and Pass@4 accuracy of 90% with Gemini 1.0 Pro model. Verifier Confidence. (Right) Conditional Probabilities of the Yes token P(Yes) from the verifier, averaged over all correct and incorrect solutions on test problems.\\n\\nA standard approach to improve LLM reasoning is to use test-time verification [10, 44, 24]. Specifically, an LLM generates multiple candidate solutions for a given problem and a verifier, also known as an outcome reward model, ranks these solutions and selects the best one. Here, we focus on learning such verifiers in-context for code verification.\\n\\nTo create in-context verification examples, we utilize correct and incorrect code solutions in Python generated using Gemini 1.0 Pro [15] on the GSMS8kK train set. In the prompt, each (problem, solution) pair is appended with the question “Is the solution correct?” followed by the Yes or No token according to ground truth correctness. At inference, we modify each test (problem, solution) pair in the same way and record the logit of the Yes and No tokens (denoted by Ly,;, Lyo). To compute the verifier score, we use the normalized probability of the Yes token: P(Yes) = exp(Lyes)/( €XP(Lyes) + €XP(Lno)). We evaluate verifier performance using best-of-4 selection based on the verifier score on 200 problems from GSMBK test set with Gemini 1.0 solutions.', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='As shown in Figure 6 (left), best-of-4 accuracy with the few-shot prompted verifier significantly improves above pass@1 accuracy with 16 or more in-context examples. Along with an accuracy improvement, the probabilities of the Yes token conditioned on ground-truth correct and incorrect solutions separate with increasing the number of shots up to 256, as shown in Figure 6 (right). Overall, these results show a proof-of-concept that the Gemini model becomes better at verifying correctness of solutions with many-shot ICL.\\n\\n3. Many-shot Learning without Human-Written Rationales\\n\\nMany-shot ICL could potentially be limited by the availability of high-quality human-generated rationales or demonstrations. This is particularly challenging for complex reasoning tasks, such as GPQA [52], where human-generated rationales require significant resources and expert knowledge. In this work, we explore two simple approaches for addressing this issue.\\n\\nMany-Shot In-Context Learning\\n\\nReinforced ICL Recent work [55] proposed a simplified version of Reinforced Self-Training [19], demonstrating that fine-tuning using model-generated rationales can be more effective than human- generated rationales for problem-solving tasks. Inspired by their work, we introduce Reinforced ICL, where we use model-generated rationales for in-context learning. To do so, we use a zero-shot or few-shot chain-of-thought [62] prompt as a starting point to sample multiple rationales for each training problem. Then, we select rationales that obtain the correct final answer (we assume access to ground truth final answers or correctness checks), and arrange them into in-context examples containing (problem, rationale) pairs.', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='One potential issue with model-generated rationales is that of false positives: it is possible for an incorrect reasoning chain to lead to the correct final answer, and fine-tuning or prompting using such a reasoning chain would typically harm performance. Nevertheless, as we discuss in later sections, we often find model-generated rationales to be at least as effective human-written rationales.\\n\\nUnsupervised ICL We now go one step further than Reinforced ICL: what if we removed rationales from the many-shot prompt altogether, and prompt the model only with inputs? Specifically, the Unsupervised ICL prompt consists of: 1) a preamble, such as, “You will be provided questions similar to the ones below:”, 2) a list of unsolved inputs or problems, and 3) a zero-shot instruction or a few-shot prompt with outputs for the desired output format. See §A.2 for the exact prompts we use.\\n\\nOne hypothesis for how many-shot unsupervised ICL might surpass few-shot learning with human demonstrations is that, when the LLM already possesses the required knowledge to solve a task, any information inserted in the prompt that can narrow down what knowledge is needed for the task becomes helpful. This would be consistent with the view that ICL simply “locates” latent concepts (e.g., math problem-solving) the LLM acquired during pre-training [66, 22, 61]. As such, any of the prompt components — inputs, outputs, and their mapping — can help locate such concepts. While Unsupervised ICL is broadly applicable, it may not perform well, for example, when outputs are critical for specifying the task (Figure 9 and A.11).\\n\\n3.1. Problem-solving: Hendrycks MATH & GSM8K\\n\\nooo-- 4-shot InnerMono. MATH Prompt Mm ICL (Ground-Truth) lm Unsupervised ICL Mm Reinforced ICL', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='60.0% MATH500 GSMB8K (Transfer using MATH prompts) “ew ° Se é 2 95.0% ° 3 eo . 8 e ° oe + e ° J57.5% a a oe ofl 3 S57.5% 8 90.0% \\\\ > 8 8 55.0% é 8 85.0%| ¢ fe 52.5% 80.0% ua 50.0% 75.0% . 47.5% 70.0%! © = Z 7 = =“! 25 50 125 250 500 4 10 25 50 125 250 500 Number of Shots (K) Number of Shots (K)\\n\\nFigure 7 | Many-shot Reinforced and Unsupervised ICL for problem-solving generally outperform ICL with ground-truth MATH solutions. MATH. (Left) The bar plots depict the average performance across five random seeds on the MATH500 test set. Each random seed (denoted by the dots) corresponds to a different subset of problems along with ground truth or model-generated solutions (if any) in the prompt. Transfer to GSM8K. (Right) We see that the prompt obtained from MATH transfers well to the GSM8K test split containing 500 problems. Our results with many-shot ICL outperform the 4-shot Minerva prompt, which obtains a test accuracy of 55.7% on MATHS00 and 90.6% on GSM8K.\\n\\nMany-Shot In-Context Learning\\n\\nWe evaluate Reinforced and Unsupervised ICL on Hendrycks MATH [23], which consists of challenging high school competition-level mathematics problems. We use the MATHS0O0 test set from Lightman et al. [33] to report performance, and our 4-shot MATH prompt for data generation can be found in Figure A.6. For Unsupervised ICL, we append this 4-shot prompt after the unsolved problems (see Figure A.8). For comparison, we also evaluate ICL with human-written solutions (ground-truth) from the MATH training set, with the same problems used for many-shot prompts.', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='Our results are shown in the Figure 7 (left). On MATH500, both Reinforced and Unsupervised ICL outperforms ICL with ground-truth solutions in both the few-shot and many-shot regime. For ICL, we observe that the performance improves with more examples in the prompt up to a point, and then declines (with the peak being at about 125 examples). Performance for Reinforced ICL also improves with the number of examples, and reaches a plateau at around 25 examples (while being about 5% higher than ICL), and unlike ICL, we don’t see a significant drop in performance even for a very large number of examples in the context. Notably, many-shot ICL achieves comparable or superior performance when using only problems compared to using problems with solutions. This suggests solutions may be redundant for eliciting problem-solving via in-context learning on this domain, potentially due to extensive math-related data seen during pretraining.\\n\\nCan many-shot ICL enable out-of-distribution generalization? Singh et al. [55] found that fine- tuning a model on model-generated solutions from MATH resulted in improved test performance on GSM8K [10], which has a different distribution of problems than MATH. Here, we investigate whether many-shot ICL also improves transfer performance on GSMBK, indicating an improvement in general problem-solving abilities from in-context learning. Our results in Figure 7 (right) show that this is indeed the case — Reinforced ICL with MATH prompts excels on GSMB8K, outperforming ICL with ground truth MATH solutions as well as Unsupervised ICL in the many-shot setting with at least 25 shots. This indicates that model-generated solutions can enable better generalization than just using problems or combining them with ground-truth solutions for ICL.\\n\\n3.2. Question Answering: Google-Proof QA (GPQA)\\n\\nGoogle-Proof QA (GPQA): Diamond Split', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='° ° ¥ 45.0% ° ee * el > Ps 0-Shot Prompt fa . @ mmm ICL (Ground-Truth) 3 40.0% hd Mmm Unsupervised ICL < a | | ---- ill Reinforced ICL 3 o\\n\\n35.0%\\n\\n5 10 25 50 125 250 Number of Shots (K)\\n\\nFigure 8 | Many-shot Reinforced and Unsupervised ICL for GPQA. The baseline zero-shot prompt, which is used for generating rationales for Reinforced ICL and appended to the prompt for Unsupervised ICL, obtains a performance of 38.8%. The average test accuracy with 125-shot prompt with both ground-truth or model-generated rationales surpass the 40.4% obtained by Claude-3 Sonnet. As we vary the number of shots, while Unsupervised ICL matches or outperforms the zero-shot prompt, Reinforced ICL consistently outperforms it.\\n\\nGPQA [52] is a multiple-choice QA benchmark, with difficult questions focused on graduate-level reasoning in biology, physics, and chemistry. Following Claude-3 [3], we use the diamond split (198 problems) for evaluation. This split focuses on questions where domain experts agree but experts in other domains struggle despite extended effort and internet access. Remaining 250 questions in non-\\n\\nMany-Shot In-Context Learning\\n\\ndiamond split are used for many-shot ICL with and without human-written rationales. For Reinforced ICL, we use a zero-shot prompt (Figure A.4) to generate multiple rationales on the non-diamond split, solving 129 problems. We also append this zero-shot prompt after the GPQA problems for specifying output format for Unsupervised ICL.', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='As shown in Figure 8, average test accuracy with ground-truth rationales improves substantially from 5 shots to 125 shots, with the best-performing 125-shot prompt nearly matching the accuracy of the state-of-the-art Claude-3 Opus. However, we do observe a performance degradation with 250 shots. Moreover, Reinforced ICL results indicate that model-generated rationales on GPQA seem to be better than ground-truth rationales up to 25 shots, while resulting in similar performance with more shots. Additionally, Unsupervised ICL does not follow any systematic trend: it sometimes performs better ICL with ground-truth rationales depending on the number of shots, but generally underperforms Reinforced ICL. As noted in Anthropic [3], GPQA is a small evaluation dataset and has an inherent higher variance across different runs, which might explain the non-systematic trends.\\n\\n3.3. Algorithmic and Symbolic Reasoning: Big-Bench Hard\\n\\nMultistep Arithmetic Two Logical Deduction [Seven] Geometric Shapes Salient Translation Error Detection\\n\\n3 10 425 50 100 3 10 25 50 100 3 10 425 50 100 3 10 25 50 100\\n\\nObject Count Reasoning About Colored Objects Dyck Languages Word Sort\\n\\n1,\\n\\n@ 1.00 00 e 0.9 fom vs ‘toto E do<<l. 5 0.50 0.90 0.8 a\\n\\n3 10 25 50 100 3 10 25 50 100 3 10 25 50 100 3 10 25 50 100\\n\\nNumber of Shots Number of Shots Number of Shots Number of Shots\\n\\n===: 3-shot CoT (Human-written) —}— Reinforced ICL —}~ Unsupervised ICL\\n\\nFigure 9 | BIG-Bench Hard. Reinforced and Unsupervised ICL with varying number of shots, averaged across five random seeds. We evaluate test performance on a held-out set of 100 problems. The error bars denote standard deviation. Reinforced ICL outperforms Unsupervised ICL for all tasks, which in turns outperforms the human-written chain-of-thought (CoT) prompt. Averaged across tasks, CoT prompting using human-written rationales gets a success rate of 72.1%, Unsupervised ICL obtains 77.1%, while Reinforced ICL gets 83%.', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='We now evaluate Reinforced ICL and Unsupervised ICL on BIG-Bench Hard [56], a suite of challenging algorithmic reasoning tasks. To reduce the impact of false positives, we select 8 tasks out of 23 in BIG-Bench Hard for which the likelihood of getting a false positive is low: either the answer string is long, or the number of options for each question is large (at least 6). For Reinforced ICL, we use the standard 3-shot CoT prompt from Suzgun et al. [56] to sample 10 rationales per problem from a training set of 150 problem at a temperature of 1.0. We filter the rationales based on final answer correctness and arrange them into prompts containing 3 to 100 (problem, rationale) pairs.\\n\\nAs shown in Figure 9, Reinforced ICL strongly outperforms Unsupervised ICL for almost all tasks, which in turn outperforms the standard 3-shot CoT prompt. Performance for Reinforced ICL generally improves monotonically with the number of prompts for 7 out of 8 tasks. These results indicate the Reinforced ICL is a more robust technique than Unsupervised ICL, especially for tasks in which the demonstrations contain crucial information about the task. For a few tasks, Reinforced ICL outperforms the human-written 3-shot prompt even in the 3-shot setting. This result suggests that model-generated rationales can sometimes outperform human-written rationales even when controlling for the amount of data, mirroring the results reported by Singh et al. [55] for fine-tuning.\\n\\nMany-Shot In-Context Learning\\n\\n4. Analyzing Many-Shot ICL\\n\\n4.1. Overcoming Pre-training Biases with Many-Shot ICL\\n\\nSentiment Analysis (FP): Replacement Labels\\n\\nSentiment Analysis (FP): Replacement Labels\\n\\n100%\\n\\nPr °o\\n\\nA —-- Abstract labels 7 —— Default labels\\n\\nY —-~ Flipped labels\\n\\n. —-- Abstract labels Lf —— Default labels ~~ Flipped labels\\n\\nTest Accuracy (%) [or] 3 x \\\\ ™ Confidence (Label Probability) ° a —— a ~.\\n\\n2 2 2 2 2 2 2 27 27 2 Number of Shots (K) Number of Shots (K)', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='Figure 10 | Overcoming Pre-Training Bias with Many-Shot ICL. (Left) Many-shot ICL overcomes label flips: Test accuracy for sentiment analysis typically improves with more training shots. Flipped and abstract labels eventually approaching the performance of default labels. (Right) Confidence shift in overcoming bias. For flipped and abstract labels, model confidence in its predicted sentiment labels initially drops, then sharply increases with more training shots to similar value, suggesting a period of overcoming pre-training bias.\\n\\nWhile LLMs demonstrate in-context learning of novel tasks, Kossen et al. [30] suggest that ICL may have difficulty unlearning biases derived from pre-training data. Their experiments, however, focused mainly on few-shot ICL due to LLM context length limitations. Here, we revisit their study using many-shot ICL on the Financial PhraseBank (FP) sentiment analysis dataset [40]. Like Kossen et al. [30], we study label relationships that affect pre-training biases:\\n\\n¢ Flipped Labels: Default labels are rotated, that is, [‘negative’, ‘neutral’, ‘positive’] becomes [‘neutral’, ‘positive’, ‘negative’]. This conflicts with sentiment biases an LLM might have learned.\\n\\n¢ Abstract Labels: We use [‘’, ‘B’, ‘C’], removing any pre-existing sentiment association [63].', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='For ICL shots, we sample examples from the validation set (with replaced labels) to exhibit the input-label relationship and report the results in Figure 10. With few shots, test accuracy with replacement labels is much lower than with default labels. This suggests that with few-shot ICL, the model struggles to overcome its pre-existing biases from pre-training. However, as the number of shots increases, performance on flipped and abstract labels dramatically improves, approaching that of default labels. For default labels, confidence in predicted labels steadily increases with more shots, as shown in Figure 10 (right). In contrast, for flipped labels, confidence initially drops then sharply increases before reaching a plateau, suggesting a period of overcoming pre-training bias.\\n\\nWe posit that the initial drop in performance and confidence in the few-shot regime may be attributed to the “early ascent” phenomenon [47, 36]: a small number of shots may lead to the retrieval of an incorrect skill, which eventually diminishes as task learning takes effect in the many- shot regime. Overall, these results indicate that many-shot ICL can overcome pre-training biases.\\n\\n4.2. Learning Non-Natural Language Tasks: High-Dimensional Functions\\n\\nWe now test many-shot ICL’s ability to learn abstract mathematical functions with numerical inputs, which let us stress test its generality and applicability to possibly unseen tasks.\\n\\nBinary Linear Classification in High Dimensions Following the setup from Wei et al. [63], we create datasets with N-dimensional inputs vectors and their binary class labels, where each dimension\\n\\nMany-Shot In-Context Learning', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='—— Gemini 1.5 Pro —-—- k-Nearest Neighbors — ----- Random Classification (N = 16) Classification (N = 32) Classification (N = 64) _ 90.0% S > 80.0% g 5 & 70.0% < 2 60.0% 50.0% - 2= 25 27 2° 2° 27 28 2 Qu gm 2 25 27 2 2° 27 28 2 QW QT 22 25 27 2° 2° 27 2 29 Qi QTt Number of Shots Per Class (K) Number of Shots Per Class (K) Number of Shots Per Class (K)\\n\\nFigure 11 | In-Context Classification. Test accuracy for 16, 32 and 64 dimensional linear classification problems, averaged across 5 randomly-generated datasets with 25 points per class for each dataset (250 evaluation points total). As we increase the number of shots, the accuracy improves and approximately tracks the performance of the nearest-neighbor baseline trained from scratch on the same data. We use the default implementation of k-nearest neighbours (with k = 5) from scikit-learn [48]. See Figure A.7 for an example prompt.\\n\\nParity Sequence: 20 Digits\\n\\nInput: 10110001110000100111 Label: Odd Odd Even Odd Odd Odd Odd Even Odd Even Even Even Even Even Odd Odd Odd Even Odd Even\\n\\n—— Gemini 1.5 Pro » Random Chance\\n\\nInput:01100110110011000111 GPT-2 Med (Scratch): 20x data\\n\\nLabel:\\n\\nTest Accuracy (%) (Exact Seq. Match) N oO oO x\\n\\n27 27 2% 2 27 28 29 2 QM gt pg\\n\\nNumber of Shots (K) Figure 12 | Learning Sequential Parity Function In-context. We report test accuracy over 200 unseen inputs, averaged across 3 seeds. Error bars denote standard error of the mean. Task Prompt. (Left) Example prompt with input and output labels of the 20-digit Sequential Parity Function. Test accuracy (Right) Many-shot ICL performance improves almost\\n\\nmonotonically with the number of shots, surpassing performance of GPT-2 Medium sized transformer trained from scratch for 1 forward-backward pass per example on 20x more data.', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='is a random integer in [1, 1000]. See more details in 8A.5. While Wei et al. [63] used only 16 shots per class, we scale ICL up to 2048 shots per class. As shown in Figure 11, while 2048 shots per class perform best when N = 16, we observe slight accuracy decrease beyond 512 shots for higher values of N (Figure 11 C, R). Moreover, many-shot ICL substantially outperforms random-chance accuracy and nearly matches the accuracy of a strong baseline, namely k-nearest neighbors, indicating that many-shot ICL can implement nearest-neighbour search over inputs. This is reminiscent of induction heads that implement prefix matching over sequences [46], a plausible mechanism for ICL abilities.\\n\\nSequential Parity Parity is a fundamental Boolean function that determines if a binary input sequence contains an even or odd number of 1s. Despite their power, transformers trained specifically for in-context learning, struggle to learn the Parity function over 20-digit sequences [7]. In this work, we evaluate how well many-shot ICL performs with a pretrained LLM to learn the sequential parity function f(x) = [f1(x), fa(x),---, fa(x)], where x € {0,1}\" and f;(x) = x1 @xg---@x;, Vie [1,n]. We report the results in Figure 12. We see consistent improvement in test accuracy as we increase the number of shots to 8192. Performance surpasses a GPT-2 Medium sized transformer [51] trained from scratch on 20x more input-output examples (with no repeated examples; §A.6). This result indicates many-shot ICL can implement computations analogous to gradient descent [60].\\n\\n10\\n\\nMany-Shot In-Context Learning\\n\\n4.3. Is Many-Shot ICL Sensitive to Example Ordering?\\n\\nIn few-shot in-context learning (ICL), the order of examples within the prompt can significantly impact model performance [38, 65]. Here, we investigate whether such sensitivity to prompt ordering ob-\\n\\nserved in few-shot ICL persists in many-shot _ scenarios, which remains\\n\\nlargely unexplored. Specifically, we evalu-', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='ate ten different random orderings of fixed MATH500: 50-shot Ordering Sensitivity\\n\\n50 in-context examples from MATH training 37° r\\n\\nsplit and evaluate performance on the held-out 1 05.0% \" + |\\n\\nMATHS0O0 test set [33]. 560.0% ° ° : § t—s F As Figure 13 reveals, performance varies sig- 3 55.0% Q ° e g¢ 2 °\\n\\nnificantly across different subareas in MATH500. 8 50.0%] ¢\\n\\nStrikingly, an ordering that that excels in one 45.0%! °\\n\\nsubarea may perform poorly in another, for ex- 9 cae oe et eS 6% (eo\\n\\nample, the best Geometry ordering yields weak e iS cores\\n\\nresults on Number Theory. This fluctuation re-\\n\\nsults in a smaller variation in average perfor- Figure 13 | Many-Shot Sensitivity To Example Ordering. see : Each colored data point represents a different random order-\\n\\nmance compared to individual subareas. Onein- . . . a\\n\\n. . i. ing of 50 in-context examples provided to Gemini 1.5 Pro.\\n\\nteresting extension would be to optimize many-\\n\\nshot prompts using frameworks like DSPy [28] that has been successfully applied for optimizing\\n\\nfew-shot prompts based a given metric. Overall, these findings highlight a key challenge in ensuring\\n\\nreliable results with many-shot ICL for long-context models.\\n\\n4.4. Many-Shot ICL vs. Supervised Fine-Tuning\\n\\nBase Model (@@m_ Supervised FT (@m_ ~Many-Shot ICL\\n\\n~ English > Bemba English > Kurdish\\n\\nBS\\n\\n+ 40 4 4\\n\\n+ a a\\n\\nta ™ ™ “ “\\n\\nE20] & N\\n\\nU\\n\\ni)\\n\\neee eee\\n\\n250 997 250 997\\n\\nNumber of Examples Number of Examples\\n\\nFigure 14 | Comparing SFT with Many-Shot ICL on low-resource translation. We plot mean performance across 3 seeds. The standard deviation is between 0.1% to 0.5%. Base model corresponds to 1-shot performance of Gemini 1.5 Pro.', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='Many-shot ICL could make task-specific fine-tuning less essential or, in some cases, even un- necessary, allowing LLMs to tackle a wider range of tasks without specialization. While supervised fine-tuning (SFT) is the dominant LLM paradigm when making use of hundreds or thousands of examples, it is computationally expensive in terms of training. In contrast, many-shot ICL does not require any training, however it has a larger inference cost, which can be substantially reduced with KV caching [49, 64], which might be available off-the-shelf with context caching [12].\\n\\nHere, we compare many-shot ICL to full fine-tuning for machine translation (§2.1). We run two sets of experiments: one using 250 examples, and another using the entire dev set (997 examples). Our results in Figure 14 show that SFT and ICL performance is quite close for Bemba, while SFT has a slight edge for Kurdish. Overall, these results demonstrate that many-shot ICL can be a viable alternative to SFT for some tasks.\\n\\n11\\n\\nMany-Shot In-Context Learning 4.5. Comparing Many-Shot Abilities of Frontier LLMs\\n\\n—e— Gemini 1.5 Pro GPT-4-Turbo —e— Claude-3-Opus\\n\\nTranslation: English - Bemba Translation: English > Kurdish\\n\\niN o\\n\\nw ua\\n\\nTest chrF2++ (%) WwW uw\\n\\nTest chrF2++ (%)\\n\\n2° 2? 22 23 27 2 2% 27 2F 29 2 2° 2t 22 23 27 2° 2© 27 2% 29 2 Number of Shots (K) Number of Shots (K)\\n\\nFigure 15 | Many-shot ICL with GPT-4-Turbo and Claude-3-Opus [3] on low-resource machine translation (§2.1).', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='The strong many-shot results with Gemini 1.5 Pro raises the question of whether other long-context frontier LLMs also benefit from many-shot ICL. To do so, we evaluate GPT-4-Turbo (128K context length) and Claude-3-Opus [3] (200K context length) on the low-resource translation (§2.1). For both these models, many-shot ICL scales favorably on Bemba but do not exhibit much improvement on Kurdish. Notably, 1.5 Pro starts lower than Claude-3 on Bemba but improves more rapidly, achieving much higher performance at 997 shots. It also outperforms GPT-4 in few-shot learning and improves\\n\\nfurther with more examples. Overall, these results indicate that frontier LLMs exhibit varying degree of many-shot ICL capability.\\n\\n4.6. Long-context scaling laws may not predict ICL performance\\n\\nNegative Log-Likelinood on Ground-Truth Solutions\\n\\n8 GPQA MATH GSMBK co} £ oe & y 2 $A, gv 0.70 0.8 Lo - - = 7 = Lt 313 s 1D 0.65 0.6 F ee 0.60 eT S44 0.4 ov Zz 5 40 275 50 425 750 4 10 25 50 425750500 4 10 25 50 425750500 Number of Shots (K) Number of Shots (K) Number of Shots (K) —— Reinforced ICL —e— ICL (Ground-Truth) —a— Unsupervised ICL\\n\\nFigure 16 | Negative Log-Likelihood (NLL) as a function of number of shots. We plot NLL on ground truth test set solutions for GPQA, MATH and GSMBK. For GPQA and MATH, questions for Reinforced ICL and Unsupervised ICL comes from the training splits of those datasets. We study GSMB8K in the transfer setting, i.e. questions for Reinforced and Unsupervised ICL come from MATH. The absolute NLL for ICL and Reinforced ICL are not directly comparable to Unsupervised ICL, since they use different prompt formats.', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='Prior works [68, 2, 27] have found that the negative log-likelihood (NLL) for ground-truth test outputs decreases predictably as the context length increases. We confirm this finding for GPQA, Hendrycks MATH and GSM8K with many-shot ICL, and report our results in Figure 16. However, we note that NLL trends are not a strong predictor for downstream task performance. For example, the success rate for both MATH and GPQA with ICL decreases after 125 shots (Figure 7,8), but we do not observe a corresponding increase in the NLL in Figure 16.\\n\\nWe also plot NLL curves for Reinforced and Unsupervised ICL, and find them to generally have a\\n\\n12\\n\\nMany-Shot In-Context Learning\\n\\nsmaller slope when compared to supervised ICL. Interestingly, NLL curves for ICL with ground-truth outputs is lower than with model-generated outputs, even though the latter often performs better. In the GSM8K transfer setting (using MATH problems and solutions to score GSM8K solutions), the change in NLL is close to nil. However, this doesn’t reflect transfer performance on GSM8K, which continues to improve with more examples (Figure 7).\\n\\nOverall, our results demonstrate that NLL is not a reliable proxy when attempting to predict ICL performance for problem-solving domains. This makes intuitive sense: for any given problem, there are a large number of potentially correct CoT solutions that the model can generate, and calculating the log-likelihood on only one such solution may not provide a clear picture for overall model capability. We also explore computing NLL on a diverse set of model-generated outputs on MATH, and our findings are presented in §A.7.\\n\\n5. Related Work', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='Scaling in-context learning Brown et al. [8] reported improved performance as you increase the number of examples (up to 64) for in-context learning in LLMs , and later works corroborated this finding [39]. However, very few works have explored using a large number of examples (1000 or above) in the prompt. This is likely due to the fact the context lengths in large language models have been quite limited until recently [16, 3]. One closely related work to ours is from Li et al. [31], who scale the number of examples for in-context learning to 2000. However, Li et al. [31] use a custom model architecture [74] to achieve long context lengths, and only evaluate models of up to 1.3B parameters, which is several orders of magnitude smaller than state-of-the-art language models, and are ineffective for complex tasks, such as GPQA [52].\\n\\nConcurrently to our work, Anil et al. [2] used many-shot prompting (upto 256 shots) to jailbreak language models. In our work, we focus on a much wider range of tasks, use a lot more examples (up to 8192 shots) and use models with much longer context lengths (up to 1M tokens). Also, we explore mitigations for needing many human-generated examples with many-shot ICL. Furthermore, while Anil et al. [2] use many-shot learning to override preferences learned during RLHF phase to elicit the biases stemming from pretraining, our results in §4.1 demonstrate that we can also override pre-training biases themselves. Bertsch et al. [6] also concurrently shows benefits of scaling up in-context learning to many demonstrations on several classification datasets with up to 151 labels, albeit also using smaller context windows of up to 80k tokens (using Llama2-80k [13]).', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='Long-context scaling laws Prior works [68, 2, 27, 16] have reported smaller next-token prediction loss with longer contexts, which Jeon et al. [25] also show using theoretical analysis. Our findings confirm this trend for even longer context lengths, but our analysis reveals some of the limitations of using next-token prediction loss as a metric for evaluating long-context performance, as next-token prediction loss continues to go down even as overall performance plateaus.\\n\\nLearning from self-generated data Numerous recent works [19, 70, 55] propose fine-tuning language models on self-generated data to improve performance. Their approach consists of (1) generate samples from the model and filter them using binary feedback, (2) fine-tune the model on these samples, and (3) repeat this process a few times. In this work, we extend this idea to in-context learning, and study the efficacy of Reinforced ICL in reasoning and problem-solving domains.\\n\\nMany-Shot In-Context Learning\\n\\nSelf-generated data and in-context learning Kim et al. [29] propose using self-generated data for few-shot ICL on classification problems, where they generate demonstrations using the LLM conditioned on the test input for each possible class label, and including these demonstrations in the context when performing the final prediction. Li et al. [32] extend this approach to reasoning and language understanding tasks, where they also generate demonstrations conditioned on the test input. Consistent with our findings, these works show that model-generated demonstrations can outperform human-generated demonstrations in the few-shot regime. Another related approach is AutoCoT [73] that uses a zero-shot CoT prompt to produce model-generated demonstrations for few-shot ICL. To do so, AutoCoT samples diverse questions one-by-one based on embedding-based clustering followed by heuristics-based post-processing for selecting demonstrations.', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='Different from above approaches, Reinforced ICL generates demonstrations using the same pro- cedure as Singh et al. [55], does not require clustering, post-processing heuristics, or access to the test inputs for generating demonstrations, and can be applied to any problem for which we can obtain reliable reward signals. Moreover, our work mainly focuses on the utility of randomly-sampled model-generated demonstrations for many-shot ICL.\\n\\nLearning Input-Output Relationships with ICL Numerous works [41, 30, 69, 36] have investigated whether LLMs truly learn input-output relationships during in-context learning. Min et al. [41] found that replacing the ground truth labels in in-context examples with random labels barely effected final performance. Further investigations by Yoo et al. [69] and Kossen et al. [30] found that this finding does not necessarily hold across tasks and model sizes. In particular, Kossen et al. [30], Lin and Lee [36] showed that LLMs can indeed learn input-output relationships via in-context learning, but require more examples in order to do so well. In our work, we extrapolate the trend found in those works to much longer context lengths, showing that pre-training biases can be mostly overcome given enough training examples.', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='Learning Mathematical Functions with LLMs Several prior works investigate whether mathemati- cal functions can be learned with transformers [14, 72, 67, 7]. All these works train transformers specifically to perform in-context learning for such functions. In contrast, we demonstrate that many- shot ICL can learn high-dimensional functions even with pre-trained LLMs. Concurrent to our work, Vacareanu et al. [58] demonstrate that pretrained LLMs are able to perform regression tasks, with performance rivaling that of traditional supervised methods with 500 in-context examples. Our work complement their findings to other synthetic tasks with a much larger number of in-context examples. Dinh et al. [11] fine-tuned GPT-3 on synthetic classification tasks and observed similarities in the decision boundaries learned by the fine-tuned model and kNNs. Our results in Figure 11 show that many-shot ICL also performs comparably to kNNs on high-dimensional classification tasks.\\n\\nComparing ICL with fine-tuning Contrary to task-specific fine-tuning, ICL does not require op- timizing any model weights, allowing LLMs to perform a variety of tasks at inference. As such, several prior works compare fine-tuning with ICL but in few-shot regime. Liu et al. [37] proposed a parameter-efficient few-shot fine-tuning (FT) approach for TO that outperforms few-shot ICL with GPT-3. However, Awadalla et al. [5] argue that few-shot ICL is more robust to distribution shifts than fine-tuning for question answering tasks. Similarly, Asai et al. [4] show better transfer with ICL compared to fine-tuning on some tasks. Mosbach et al. [42] fairly compare ICL with FT by using the same model for both approaches and show that full fine-tuning (FT) generally outperforms ICL in the few-shot regime with 16 examples. More recently, Lin et al. [34] show that few-shot ICL can outperform fine-tuning based approaches for aligning LLMs.\\n\\nMany-Shot In-Context Learning', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='Complementary to prior works, we compare full fine-tuning with many-shot ICL with the same number of examples for low-resource translation. Notably, we find that many-shot ICL performs comparably to FT. Aligned with our findings, Bertsch et al. [6] concurrently show that many-shot ICL generally outperforms parameter-efficient fine-tuning (LoRA) on classification tasks. Overall, many-shot ICL and FT can exhibit comparable behaviors, which we leave for further investigation.\\n\\nExemplar vs. Rule-based ICL generalization Chan et al. [9] indicate that ICL tends to generalize in a more exemplar-based way, compared to rule-based generalization during in-weights learning. Using a clever experiment with blocked attention, Bertsch et al. [6] also argue that the benefits of many in-context demonstrations arise from having access to more similar examples. While our results on in-context linear classification agree with this conclusion, our sequential parity results seem to contradict it. Strikingly, sequential parity was the task on which we saw the most improvement, whereas it should be a task that benefits least from seeing similar examples — after all, the nearest neighbor is always going to give the wrong answer (off by 1 bit). Chan et al. [9] do show that a transformer’s inductive biases towards exemplar-based generalization can be shifted both by the training data and the model size, with larger models being less exemplar-based — perhaps this explains the contradictory findings, given that our work used a larger and much more capable model, though this remains an open question.\\n\\n6. Discussion, Limitations and Future Work', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='We found significant gains in performance when going from few-shot to many-shot ICL on a wide range of tasks, including translation, summarization, planning, reward modeling, mathematical problem solving, question-answering, algorithmic reasoning, and sentiment analysis. To overcome the challenges of obtaining a large number of high-quality human-written rationales for many-shot ICL, we introduced two regimes: Reinforced ICL and Unsupervised ICL. Moreover, we demonstrate that, unlike few-shot ICL, many-shot ICL is effective at overriding pretraining biases, can learn high-dimensional functions with numerical inputs, and performs comparably to SFT.\\n\\nOne limitation of our work is that it mainly evaluates many-shot ICL with Gemini 1.5 Pro. That said, concurrent works [2, 6] as well as our preliminary results with GPT-4-Turbo and Claude-3- Opus (84.5) indicate that other LLMs can also benefit from many-shot ICL. Future work should focus on evaluating the many-shot abilities of a wide range of long context models, as they become available. Furthermore, many-shot performance can likely serve as a valuable metric for evaluating the quality of long-context models, going beyond the prevalent needle-in-a-haystack test [26].\\n\\nAnother limitation of our work is that we don’t completely understand why performance can sometimes degrades with more examples in the prompt (for example, for MATH). Our analysis found that negative log-likelihood trends are insufficient to explain this degradation, and future work should investigate new directions to shed light on the matter and improving many-shot ICL capabilities. Overall, we hope that this work lays a foundation for understanding and optimizing the use of long-context models for ICL, opening up a new frontier of LLM capabilities.\\n\\nAcknowledgements We would like to thank Gheorghe Comanici for reviewing an early draft of this work. We are also', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='grateful to Doina Precup, Aviral Kumar, Dale Schuurmans, Ankit Anand, Ross Goroshin, Urvashi Singh, Jannik Kossen, Charline Le Lan, and Daniel Toyoma for helpful discussions.\\n\\nMany-Shot In-Context Learning\\n\\nContribution Statement\\n\\nRA initiated and led the project, ran majority of the many-shot experiments and analysis, came up with reinforced ICL, on-boarded collaborators, wrote the initial draft. AS contributed initial infra for experiments on MATH and GSMB8kK, ran BBH experiments, co-led the fine-tuning experiments, conducted NLL analysis on problem-solving tasks, and wrote several sections.\\n\\nLZ contributed results for in-context verifier. BB contributed the planning logistics task. LR led the fine-tuning experiments. BZ contributed the many-shot results for GPT-4 and Claude-3. AA helped with GPQA, SC contributed the baseline for parity task and both helped edit the paper. AF and HL provided feedback on an early draft. HL also suggested the unsupervised ICL experiments. Others were involved in project discussions and minor edits to the paper.\\n\\nReferences\\n\\n(1]\\n\\n[2]\\n\\n[3]\\n\\n[4]\\n\\n[5\\n\\nfa\\n\\n[6\\n\\nfa\\n\\n[7]\\n\\n[8]\\n\\nS.N. Akter, Z. Yu, A. Muhamed, T. Ou, A. Bauerle, A. A. Cabrera, K. Dholakia, C. Xiong, and G. Neubig. An in-depth look at gemini’s language abilities. arXiv preprint arXiv:2312.11444, 2023.\\n\\nC. Anil, E. Durmus, M. Sharma, J. Benton, S. Kundu, J. Batson, N. Rimsky, M. Tong, J. Mu, D. Ford, F. Mosconi, R. Agrawal, R. Schaeffer, N. Bashkansky, S. Svenningsen, M. Lambert, A. Radhakrishnan, C. Denison, E. J. Hubinger, Y. Bai, T. Bricken, T. Maxwell, N. Schiefer, J. Sully, A. Tamkin, T. Lanham, K. Nguyen, T. Korbak, J. Kaplan, D. Ganguli, S. R. Bowman, E. Perez, R. Grosse, and D. Duvenaud. Many-shot jailbreaking. Technical report, Anthropic, 2024.\\n\\nAnthropic. The claude 3 model family: Opus, sonnet, haiku. Technical Report, 2024.', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='A. Asai, S. Kudugunta, X. V. Yu, T. Blevins, H. Gonen, M. Reid, Y. Tsvetkov, S. Ruder, and H. Hajishirzi. Buffet: Benchmarking large language models for few-shot cross-lingual transfer. arXiv preprint arXiv:2305.14857, 2023.\\n\\nA. Awadalla, M. Wortsman, G. Ilharco, S. Min, I. Magnusson, H. Hajishirzi, and L. Schmidt. Exploring the landscape of distributional robustness for question answering models. arXiv preprint arXiv:2210.12517, 2022.\\n\\nA. Bertsch, M. Ivgi, U. Alon, J. Berant, M. R. Gormley, and G. Neubig. In-Context Learning with Long-Context Models: An In-Depth Exploration, Apr. 2024. URL http: //arxiv.org/abs/ 2405 .00200. arXiv:2405.00200 [cs].\\n\\nS. Bhattamishra, A. Patel, P. Blunsom, and V. Kanade. Understanding in-context learning in transformers and Ilms by learning to learn discrete functions. arXiv preprint arXiv:2310.03016, 2023.\\n\\nT. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Lan- guage models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. Bal- can, and H. Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6- 12, 2020, virtual, 2020. URL https: //proceedings.neurips.cc/paper/2020/hash/ 1457cO0d6bf cb4967418bfb8ac142f64a-Abstract .html.\\n\\n[9]\\n\\nKe) fi]\\n\\n[20]\\n\\n[21]\\n\\n[22]\\n\\nMany-Shot In-Context Learning\\n\\nS. C. Y. Chan, I. Dasgupta, J. Kim, D. Kumaran, A. K. Lampinen, and F. Hill. Transformers generalize differently from information stored in context vs in weights, Oct. 2022. URL http: //arxiv.org/abs/2210.05675. arXiv:2210.05675 [cs].', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.\\n\\nT. Dinh, Y. Zeng, R. Zhang, Z. Lin, M. Gira, S. Rajput, J.-y. Sohn, D. Papailiopoulos, and K. Lee. Lift: Language-interfaced fine-tuning for non-language machine learning tasks. Advances in Neural Information Processing Systems, 35:11763-11784, 2022.\\n\\nG. A. for Developers. Context caching guide, 2024. URL https://ai.google.dev/ gemini-api/docs/caching.\\n\\nY. Fu, R. Panda, X. Niu, X. Yue, H. Hajishirzi, Y. Kim, and H. Peng. Data Engineering for Scaling Language Models to 128K Context, Feb. 2024. URL http: //arxiv.org/abs/2402.10171. arXiv:2402.10171 [es].\\n\\nS. Garg, D. Tsipras, P. S. Liang, and G. Valiant. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 35: 30583-30598, 2022.\\n\\nG. Gemini Team. Gemini: A family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.\\n\\nG. Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arxiv:2403.05530, 2024.\\n\\nM. Ghallab, A. Howe, C. Knoblock, D. Mcdermott, A. Ram, M. Veloso, D. Weld, and D. Wilkins. PDDL—The Planning Domain Definition Language, 1998.\\n\\nN. Goyal, C. Gao, V. Chaudhary, P. Chen, G. Wenzek, D. Ju, S. Krishnan, M. Ranzato, F. Guzman, and A. Fan. The flores-101 evaluation benchmark for low-resource and multilingual machine translation. Trans. Assoc. Comput. Linguistics, 10:522-538, 2022.\\n\\nC. Gulcehre, T. L. Paine, S. Srinivasan, K. Konyushkova, L. Weerts, A. Sharma, A. Siddhant, A. Ahern, M. Wang, C. Gu, et al. Reinforced self-training (rest) for language modeling. arXiv preprint arXiv:2308.08998, 2023.', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='T. Hasan, A. Bhattacharjee, M. S. Islam, K. S. Mubasshir, Y. Li, Y. Kang, M. S. Rahman, and R. Shahriyar. Xl-sum: Large-scale multilingual abstractive summarization for 44 languages. In C. Zong, F. Xia, W. Li, and R. Navigli, editors, Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021, volume ACL/IJCNLP 2021 of Findings of ACL, pages 4693-4703. Association for Computational Linguistics, 2021.\\n\\nM. Helmert. The fast downward planning system. Journal of Artificial Intelligence Research, 26: 191-246, July 2006. ISSN 1076-9757. doi: 10.1613/jair.1705. URL http: //dx.doi.org/ 10.1613/jair.1705.\\n\\nR. Hendel, M. Geva, and A. Globerson. In-context learning creates task vectors. arXiv preprint arXiv:2310.15916, 2023.\\n\\n[23]\\n\\n[24]\\n\\n[25]\\n\\n[26]\\n\\n[27]\\n\\n[28]\\n\\n[29]\\n\\n[30]\\n\\n[31]\\n\\n[32]\\n\\n[33]\\n\\n[34]\\n\\n[35]\\n\\n[36]\\n\\nMany-Shot In-Context Learning\\n\\nD. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Mea- suring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.\\n\\nA. Hosseini, X. Yuan, N. Malkin, A. Courville, A. Sordoni, and R. Agarwal. V-star: Training verifiers for self-taught reasoners. arXiv preprint arXtv:2402.06457, 2024.\\n\\nH. J. Jeon, J. D. Lee, Q. Lei, and B. Van Roy. An information-theoretic analysis of in-context learning. arXiv preprint arXiv:2401.15530, 2024.\\n\\nG. Kamradt. LLMTest_NeedleInAHaystack. https://github.com/gkamradt/LLMTest_ NeedleInAHaystack, 2023. Accessed: 2024-04-16.\\n\\nJ. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='O. Khattab, A. Singhvi, P. Maheshwari, Z. Zhang, K. Santhanam, S. V. A, S. Haq, A. Sharma, T. T. Joshi, H. Moazam, H. Miller, M. Zaharia, and C. Potts. DSPy: Compiling declarative language model calls into state-of-the-art pipelines. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=sY5N0zY50d.\\n\\nH. J. Kim, H. Cho, J. Kim, T. Kim, K. M. Yoo, and S. Lee. Self-generated in-context learning: Lever- aging auto-regressive language models as a demonstration generator. CoRR, abs/2206.08082, 2022. doi: 10.48550/ARXIV.2206.08082. URL https: //doi.org/10.48550/arXiv.2206. 08082.\\n\\nJ. Kossen, Y. Gal, and T. Rainforth. In-context learning learns label relationships but is not conventional learning. In The Twelfth International Conference on Learning Representations, 2023.\\n\\nM. Li, S. Gong, J. Feng, Y. Xu, J. Zhang, Z. Wu, and L. Kong. In-context learning with many demonstration examples. CoRR, abs/2302.04931, 2023. doi: 10.48550/ARXIV.2302.04931. URL https: //doi.org/10.48550/arXiv .2302.04931.\\n\\nR. Li, G. Wang, and J. Li. Are human-generated demonstrations necessary for in-context learning? In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=frRDT6EOhg.\\n\\nH. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe. Let’s verify step by step. CoRR, abs/2305.20050, 2023. doi: 10.48550/ARXIV.2305.20050. URL https: //doi.org/10.48550/arXiv.2305.20050.\\n\\nB. Y. Lin, A. Ravichander, X. Lu, N. Dziri, M. Sclar, K. Chandu, C. Bhagavatula, and Y. Choi. The unlocking spell on base Ilms: Rethinking alignment via in-context learning. arXiv preprint arXiv:2312.01552, 2023.\\n\\nC.-Y. Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summariza- tion Branches Out, pages 74-81, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https: //aclanthology.org/W04-1013.', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='Z. Lin and K. Lee. Dual operating modes of in-context learning. arXiv preprint arXiv:2402.18819, 2024.\\n\\n[37]\\n\\n[38]\\n\\n[39]\\n\\n[40]\\n\\n[41]\\n\\n[42]\\n\\n[43]\\n\\n[44]\\n\\n[45]\\n\\n[46]\\n\\n[47]\\n\\n[48]\\n\\nMany-Shot In-Context Learning\\n\\nH. Liu, D. Tam, M. Mugeeth, J. Mohta, T. Huang, M. Bansal, and C. A. Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in Neural Information Processing Systems, 35:1950-1965, 2022.\\n\\nY. Lu, M. Bartolo, A. Moore, S. Riedel, and P. Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786, 2021.\\n\\nY. Lu, M. Bartolo, A. Moore, S. Riedel, and P. Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In S. Muresan, P. Nakov, and A. Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 8086-8098. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022. ACL-LONG.556. URL https: //doi.org/10.18653/v1/2022.acl-long.556.\\n\\nP. Malo, A. Sinha, P. Korhonen, J. Wallenius, and P. Takala. Good debt or bad debt: Detecting semantic orientations in economic texts. Journal of the Association for Information Science and Technology, 65(4):782-796, 2014.\\n\\nS. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Hajishirzi, and L. Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In Y. Goldberg, Z. Kozareva, and Y. Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 11048-11064. Association for Computational Linguistics, 2022.', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='M. Mosbach, T. Pimentel, S. Ravfogel, D. Klakow, and Y. Elazar. Few-shot fine-tuning vs. in-context learning: A fair comparison and evaluation. arXiv preprint arXiv:2305.16938, 2023.\\n\\nS. Narayan, S. B. Cohen, and M. Lapata. Don’t give me the details, just the summary! topic- aware convolutional neural networks for extreme summarization. In E. Riloff, D. Chiang, J. Hockenmaier, and J. Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 1797-1807. Association for Computational Linguistics, 2018.\\n\\nA. Ni, S. Iyer, D. Radev, V. Stoyanov, W.-t. Yih, S. Wang, and X. V. Lin. Lever: Learning to verify language-to-code generation with execution. In International Conference on Machine Learning, pages 26106-26128. PMLR, 2023.\\n\\nM. A. NLLB Team. No language left behind: Scaling human-centered machine translation. arXiv preprint, 2022.\\n\\nC. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, S. Johnston, A. Jones, J. Kernion, L. Lovitt, K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish, and C. Olah. In-context learning and induction heads. Transformer Circuits Thread, 2022. https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.\\n\\nJ. Pan. What in-context learning “learns” in-context: Disentangling task recognition and task learning. PhD thesis, Princeton University, 2023.\\n\\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pret- tenhofer, R. Weiss, V. Dubourg, et al. Scikit-learn: Machine learning in python. the Journal of machine Learning research, 12:2825-2830, 2011.\\n\\n[49]\\n\\n[50]\\n\\n[51]\\n\\n[52]\\n\\n[53]\\n\\n[54]\\n\\n[55]\\n\\n[56]\\n\\n[57]\\n\\n[58]\\n\\n[59]\\n\\n[60]\\n\\n[61]\\n\\n[62]\\n\\nMany-Shot In-Context Learning', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, J. Heek, K. Xiao, S. Agrawal, and J. Dean. Efficiently scaling transformer inference. Proceedings of Machine Learning and Systems, 5, 2023.\\n\\nM. Popovic. chrf++: words helping character n-grams. In Proceedings of the second conference on machine translation, pages 612-618, 2017.\\n\\nA. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\\n\\nD. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman. Gpqa: A graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023.\\n\\nN. R. Robinson, P. Ogayo, D. R. Mortensen, and G. Neubig. Chatgpt mt: Competitive for high-(but not low-) resource languages. arXiv preprint arXiv:2309.07423, 2023.\\n\\nJ. Seipp, A. Torralba, and J. Hoffmann. PDDL generators. https://doi.org/10.5281/ zenodo . 6382173, 2022.\\n\\nA. Singh, J. D. Co-Reyes, R. Agarwal, A. Anand, P. Patil, X. Garcia, P. J. Liu, J. Harrison, J. Lee, K. Xu, A. T. Parisi, A. Kumar, A. A. Alemi, A. Rizkowsky, A. Nova, B. Adlam, B. Bohnet, G. F. Elsayed, H. Sedghi, I. Mordatch, I. Simpson, I. Gur, J. Snoek, J. Pennington, J. Hron, K. Kenealy, K. Swersky, K. Mahajan, L. A. Culp, L. Xiao, M. Bileschi, N. Constant, R. Novak, R. Liu, T. Warkentin, Y. Bansal, E. Dyer, B. Neyshabur, J. Sohl-Dickstein, and N. Fiedel. Beyond human data: Scaling self-training for problem-solving with language models. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https: //openreview.net/ forum?id=1NAyUngGFK. Expert Certification.\\n\\nM. Suzgun, N. Scales, N. Scharli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. H. Chi, D. Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\\n\\nR. Vacareanu, V.-A. Negru, V. Suciu, and M. Surdeanu. From words to numbers: Your large language model is secretly a capable regressor when given in-context examples. arXiv preprint arXiv:2404.07544, 2024.\\n\\nK. Valmeekam, M. Marquez, S. Sreedharan, and S. Kambhampati. On the planning abilities of large language models-a critical investigation. Advances in Neural Information Processing Systems, 36, 2024.\\n\\nJ. von Oswald, E. Niklasson, E. Randazzo, J. Sacramento, A. Mordvintsev, A. Zhmoginov, and M. Vladymyrov. Transformers learn in-context by gradient descent, Dec. 2022. URL http://arxiv.org/abs/2212.07677. arXiv:2212.07677 [cs].\\n\\nX. Wang, W. Zhu, M. Saxon, M. Steyvers, and W. Y. Wang. Large language models are latent variable models: Explaining and finding good demonstrations for in-context learning. Advances in Neural Information Processing Systems, 36, 2024.\\n\\nJ. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-of- thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824—24837, 2022.\\n\\n20\\n\\nMany-Shot In-Context Learning\\n\\n[63]\\n\\n[64]\\n\\n[65]\\n\\n[66]\\n\\n[67]\\n\\n[68]\\n\\n[69]\\n\\n[70]\\n\\n[71]\\n\\n[72]\\n\\n[73]\\n\\n[74]\\n\\nJ. Wei, J. Wei, Y. Tay, D. Tran, A. Webson, Y. Lu, X. Chen, H. Liu, D. Huang, D. Zhou, et al. Larger language models do in-context learning differently. arXiv preprint arXtv:2303.03846, 2023.\\n\\nH. Wu and K. Tu. Layer-condensed kv cache for efficient inference of large language models. arXiv preprint arXiv:2405.10637, 2024.\\n\\nY. Xiang, H. Yan, L. Gui, and Y. He. Addressing order sensitivity of in-context demonstration examples in causal language models. arXiv preprint arXiv:2402.15637, 2024.', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='S. M. Xie, A. Raghunathan, P. Liang, and T. Ma. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021.\\n\\nY. Xing, X. Lin, N. Suh, Q. Song, and G. Cheng. Benefits of transformer: In-context learning in linear regression tasks with unstructured data. arXiv preprint arXiv:2402.00743, 2024.\\n\\nW. Xiong, J. Liu, I. Molybog, H. Zhang, P. Bhargava, R. Hou, L. Martin, R. Rungta, K. A. Sankararaman, B. Oguz, M. Khabsa, H. Fang, Y. Mehdad, S. Narang, K. Malik, A. Fan, S. Bhosale, S. Edunov, M. Lewis, S. Wang, and H. Ma. Effective long-context scaling of foundation models. CoRR, abs/2309.16039, 2023. doi: 10.48550/ARXIV.2309.16039. URL https://doi.org/ 10.48550/arXiv.2309.16039.\\n\\nK. M. Yoo, J. Kim, H. J. Kim, H. Cho, H. Jo, S. Lee, S. Lee, and T. Kim. Ground-truth labels matter: A deeper look into input-label demonstrations. In Y. Goldberg, Z. Kozareva, and Y. Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 2422-2437. Association for Computational Linguistics, 2022.\\n\\nZ. Yuan, H. Yuan, C. Li, G. Dong, C. Tan, and C. Zhou. Scaling relationship on learning mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825, 2023.\\n\\nJ. Zhang, Y. Zhao, M. Saleh, and P. Liu. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. In International conference on machine learning, pages 11328-11339. PMLR, 2020.\\n\\nR. Zhang, S. Frei, and P. L. Bartlett. Trained transformers learn linear models in-context. arXiv preprint arXiv:2306.09927, 2023.\\n\\nZ. Zhang, A. Zhang, M. Li, and A. Smola. Automatic chain of thought prompting in large language models. In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id=5NTt8GF jUHkr.', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='L. Zheng, J. Yuan, C. Wang, and L. Kong. Efficient attention via control variates. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https: //openreview.net/pdf?id=G-uNfHKrj46.\\n\\n21\\n\\nMany-Shot In-Context Learning\\n\\nA. Appendix\\n\\nA.1. Example Prompts\\n\\nYou are an expert translator. I am going to give you one or more example pairs of text snippets where the first is in English and the second is a translation of the first snippet into Kurdish. The sentences will be written\\n\\nEnglish: <first sentence>\\n\\nKurdish: <translated first sentence>\\n\\nAfter the example pairs, I am going to provide another sentence in English and I want you to translate it into Kurdish. Give only the translation, and no extra commentary, formatting, or chattiness. Translate the text from English to Kurdish.\\n\\nEnglish: Its remnants produced showers across most of the islands, though as of yet, no damage or flooding has been reported.\\n\\nKurdish: Li heréma Serengetiyé, Parka Neteweyi ya Serengeti ya Tanzanyayé, Cihé Parastina Ngorongoro i Cihé Parastina Giyanewerén Néciré Maswa ti Cihé Parastina Neteweyi ya Masai Mara ya Kendyayé hene.\\n\\nEnglish: - -- Kurdish:\\n\\nFigure A.1 | Example prompt with a test input for translation from English to Kurdish on FLORES-MT benchmark in §2.1.\\n\\nI will first show a news article and then provide a very short one sentence long summary of it in fluent English.\\n\\nSummarize the following article: Burberry reported pre-tax profits of £166m for the year to March.\\n\\nA year ago it made a loss of £16.1m, hit by charges at its Spanish operations.\\n\\nIn the past year it has opened 21 new stores and closed nine. It plans to open 20-30 stores this year worldwide.\\n\\nThe group has also focused on promoting the Burberry brand online.\\n\\nSales rose 7% to £1.28bn, with the company recording double-digit sales growth in Europe and Asia Pacific.', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='Adjusted profit rose 23% to £215m, taking into account one-off items and a favourable exchange rate.\\n\\nStores in London in particular benefited from favourable currency movements and increased tourism.\\n\\n“Looking forward, while mindful of the economic environment, Burberry plans to build on its strong financial position by accelerating investment in growth initiatives in retail, digital and new markets, while continuing to enhance the brand,” said chief executive Angela Ahrendts. Burberry shares were up 7.6% at 659 pence in afternoon trading.\\n\\nSummary: Luxury fashion designer Burberry has returned to profit after opening new stores and spending more on online marketing\\n\\nFigure A.2 | Example 1-shot prompt used for summarization on XSum and XLSum in §2.2.\\n\\n22\\n\\nMany-Shot In-Context Learning\\n\\nPlease solve the problem: (define (problem logistics-c2-s1-p1-a2) (:domain logistics-strips) (objects\\n\\na0 al\\n\\nc0 cl\\n\\ntO tl\\n\\n10-0 11-0\\n\\npo\\n\\n)\\n\\nGinit\\n\\n(AIRPLANE a0) (AIRPLANE a1) (CITY c0)\\n\\n(CITY c1) (TRUCK t0) (TRUCK t1) (LOCATION 10-0) (in-city 10-0 c0) (LOCATION 11-0) (in-city 11-0 ¢1) (AIRPORT 10-0) (AIRPORT 11-0) (OBJ pO)\\n\\n(at tO 10-0)\\n\\n(at t1 11-0)\\n\\n(at pO 11-0)\\n\\n(at a0 10-0)\\n\\n(at al 11-0)\\n\\n)\\n\\n(goal\\n\\n(and\\n\\n(at pO 10-0)\\n\\n)\\n\\n)\\n\\n)\\n\\nYour plan as plain text without formatting: (load-airplane pO al 11-0)\\n\\n(fly-airplane al 11-0 10-0)\\n\\n(unload-airplane pO a1 10-0)\\n\\ndone.\\n\\nPlease solve the problem: (define (problem -- -)\\n\\nYour plan as plain text without formatting:\\n\\nFigure A.3 | An example 1-shot PDDL [17] prompt, with a test example for the Logistics domain in §2.3. Within a city, the locations are directly linked, allowing trucks to travel between any two of these locations. Similarly, cities are directly connected to each other allowing airplanes to travel between any two cities. Each city is equipped with one truck and has a designated location that functions as an airport', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='You will be given a multiple choice question with different choices such as (A), (B), (C), (D). Think step by step before giving a final answer to this question. Always finish your answer with Final Answer: (X)’, where X is the correct answer choice. If none of the options match, choose the closest option as the final answer.\\n\\nFigure A.4 | Zero-shot prompt for GPQA.\\n\\n23\\n\\nMany-Shot In-Context Learning\\n\\n# problem: It starts raining at 7:00\\n\\nCalculate the total time\\n\\n# solution: def solution():\\n\\nCalculate the total time\\n\\nsecond_day_rain_d\\n\\nreturn result\\n\\nYes\\n\\n# problem: She spends two hours s!\\n\\nWhat percentage of the rounded to the nearest\\n\\n# solution: def solution():\\n\\nShe spends two hours s!\\n\\nWhat percentage of the rounded to the nearest\\n\\nfirst_day_rain_duration = 17 - 7 # 10 hours\\n\\nthird_day_rain_duration = second_day_rain_duration * 2 # 24 hours total_rain_duration = first_day_rain_duration + second_day_rain_duration + third_day_rain_duration result = total_rain_duration\\n\\nand pours heavily until its stops at 17:00 on a particular day.\\n\\nOn the second day, the rain takes 2 more hours than it took on the first day to stop. On the third day, the rain pours for twice the amount of time it took on the second day.\\n\\nit was raining in the three days.\\n\\n\"Tt starts raining at 7:00 and pours heavily until its stops at 17:00 on a particular day. On the second day, the rain takes 2 more hours than it took on the first day to stop. On the third day, the rain pours for twice the amount of time it took on the second day.\\n\\nme\\n\\nit was raining in the three days.\\n\\nuration = first_day_rain_duration + 2 # 12 hours\\n\\n# is the solution correct?\\n\\nHaley is getting ready to watch a comet fly over her house.\\n\\nhopping for a telescope, half an hour getting everything set up in the backyard,\\n\\nthree times the setup time making snacks, and 20 minutes watching the comet.\\n\\ntotal time she spent on all those activities was spent watching the comet, ercent?', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='\"Haley is getting ready to watch a comet fly over her house.\\n\\nhopping for a telescope, half an hour getting everything set up in the backyard,\\n\\nthree times the setup time making snacks, and 20 minutes watching the comet.\\n\\ntotal time she spent on all those activities was spent watching the comet, ercent?\"\"\"\\n\\nshopping time = 2 # hours setup_time = 0.5 # hours\\n\\nsnack_time = setuj\\n\\nreturn result\\n\\nNo\\n\\n) time * 3 # hours\\n\\ncomet_time = 20 # minutes\\n\\ntotal_time = shopping time + setup_time + snack_time + comet_time percentage_comet_time = (comet_time / total_time) * 100 rounded_percentage_comet_time = round(percentage_comet_time) result = rounded_percentage_comet_time\\n\\n# is the solution correct?\\n\\nFigure A.5 | An example 2-shot code correctness verifier prompt on GSMS8K. Here, the solutions are generated using the Gemini 1.0 Pro [15] model. See results in §2.4.\\n\\n24\\n\\nMany-Shot In-Context Learning\\n\\nVx=2\\n\\nProblem: Find the domain of the expression Vex =x\\n\\nSolution:\\n\\nI need to find the domain of this expression.\\n\\nThis expression has two square root expressions.\\n\\nI know the values under each square root must be non-negative. So let me start by ensuring that.\\n\\nI set x — 2 > 0, which simplifies to x > 2.\\n\\nThen, I set 5 — x > 0, which simplifies to x < 5.\\n\\nBut I need to make sure the denominator is not zero.\\n\\nSo, 5 — x must also be greater than zero, which gives me x < 5. Therefore, the domain of the expression is | [2, 5) |. Answer: [2,5). Final Answer: The final answer is [2,5). I hope it is correct.\\n\\nProblem: If det A = 2 and det B = 12, then find det(AB).\\n\\nSolution:\\n\\nI need to find the determinant of the product of matrices A and B.\\n\\nI remember that the determinant of the product of two matrices equals the product of their determinants. So, det(AB) = (det A)(det B) = (2)(12) = [24]. Answer: 24.\\n\\nFinal Answer: The final answer is 24. I hope it is correct.', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='Problem: Terrell usually lifts two 20-pound weights 12 times. If he uses two 15-pound weights instead, how many times must Terrell lift them in order to lift the same total weight?\\n\\nSolution:\\n\\nOkay, so Terrell lifts a total of 2 - 12 - 20 = 480 pounds of weight with the 20-pound weights.\\n\\nWell, if he switches to 15-pound weights, the total weight lifted will be 2-15 -n = 30n pounds, where n is the number of lifts.\\n\\nI want to find the number of lifts, n, for the total weight lifted to be the same.\\n\\nI equate 30n to 480 pounds and solve for n.\\n\\n30n = 480\\n\\n=> n=480/30 =| 16\\n\\nAnswer: 16. Final Answer: The final answer is 16. I hope it is correct.\\n\\nProblem: If the system of equations\\n\\n6x - 4y =a, 6y - 9x =b.\\n\\nhas a solution (x, y) where x and y are both nonzero, find ¢, assuming b is nonzero.\\n\\nSolution:\\n\\nI’m given a system of two equations.\\n\\nI see that if I multiply the first equation by -3, Til get another equation that has the same left-hand side as the second equation, 6y — 9x.\\n\\nLet me try that\\n\\n3 6y - 9x = ~3t Ah, I also know that 6y — 9x = b, so I can equate these two equations. So, 3 a 2 ~3% =b> B= 73h\\n\\n~ 2\\n\\nAnswer: —3.\\n\\nFinal Answer: The final answer is -3. I hope it is correct.\\n\\nFigure A.6 | 4-Shot Inner Monologue prompt used for MATH and GSMB8K.\\n\\n25\\n\\nMany-Shot In-Context Learning\\n\\nnput: 255 378 650 363 42 447 898 211 104 145 975 6 827 769 977 901 Output: Foo\\n\\nnput: 111 677 874 692 540 800 771 325 295 106 980 148 275 882 246 136 Output: Foo\\n\\nnput: 136 215 529 65 265 475 45 639 678 95 460 902 746 919 181 838 Output: Foo\\n\\nnput: 62 583 498 50 198 277 519 22 935 351 142 369 349 272 880 125 Output: Bar\\n\\nnput: 101 99 830 735 732 76 243 703 564 3 225 20 136 333 195 441 Output: Bar\\n\\nnput: 242 430 80 153 39 269 898 6 530 524 89 377 238 697 212 539 Output: Bar\\n\\nnput: 261 83 244 37 170 277 161 779 544 272 893 535 71 394 64 607 Output: Bar\\n\\nnput: 402 863 114 193 413 905 894 143 193 288 174 646 411 938 212 285 Output: Bar', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='nput: 869 365 622 671 191 780 492 836 381 450 184 388 604 79 924 926 Output: Foo\\n\\nnput: 548 823 66 658 380 81 779 449 641 673 94 130 258 229 299 278 Output: Bar\\n\\nnput: 700 409 398 375 236 745 32 33 333 173 902 399 176 95 851 897 Output: Foo\\n\\nnput: 673 211 14 221 508 752 147 309 338 23 827 980 373 861 980 946 Output: Foo\\n\\nnput: 528 608 334 210 228 186 559 20 302 93 84 436 726 114 785 865 Output: Bar\\n\\nnput: 117 190 66 628 31 838 183 687 598 11 187 226 381 979 171 39 Output: Bar\\n\\nnput: 802 730 854 392 529 95 15 987 800 266 551 816 145 390 419 686 Output: Foo\\n\\nnput: 723 701 860 30 217 633 226 477 720 839 548 880 277 178 512 585 Output: Foo\\n\\nmput: ---\\n\\nOutput:\\n\\nFigure A.7 | Example prompt with 8 shots per class for the linear classification in 16 dimensions, discussed in §4.2. Here, we use semantically-unrelated labels (‘Foo’ and ‘Bar’) following Wei et al. [63].\\n\\n26\\n\\nMany-Shot In-Context Learning\\n\\nA.2. Prompts for Unsupervised ICL\\n\\nis (—oo, -3] U Problem: Let\\n\\nProblem: Let\\n\\nFind f (29).\\n\\nrespond only\\n\\nSolution: I need to find\\n\\nBut I need to So, 5 — x musi\\n\\nFinal Answer:\\n\\nSolution: I need to fin\\n\\nAnswer: 24. Final Answer:\\n\\nProblem: Eva\\n\\nProblem: Find the domain of the expression\\n\\nTherefore, the domain of the expression is | [2, 5) |. Answer: [2,5).\\n\\nYou will be provided Problems similar to the ones below: Problem: What is the remainder when 369,963 is divided by 6? Problem: The solution to the inequality\\n\\ny= x? +ax+b <0\\n\\n[5, oo). Find the vertex of the parabola y = -x2 +ax +b.\\n\\nx be an angle such that tanx = ¢ and tan2x = as- Then the least positive value of x equals\\n\\ntan7!k. Compute k. Problem: Compute sin 0°.\\n\\nFe) 9x+4 if x is an integer, x) = [x]+5 ifx is not an integer.\\n\\nNow, I am going to give you a series of demonstrations of math Problems and Solutions. When you respond,\\n\\nwith the Solution of the final Problem, thinking step by step.”\\n\\nVx-2 V5-x\"\\n\\nthe domain of this expression.\\n\\nThis expression has two square root expressions.', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='I know the values under each square root must be non-negative. So let me start by ensuring that.\\n\\nI set x — 2 > 0, which simplifies to x > 2.\\n\\nThen, I set 5 — x > 0, which simplifies to x < 5.\\n\\nmake sure the denominator is not zero. t also be greater than zero, which gives me x < 5.\\n\\nThe final answer is [2,5). I hope it is correct.\\n\\nProblem: If det A = 2 and det B = 12, then find det(AB).\\n\\nthe determinant of the product of matrices A and B.\\n\\nI remember that the determinant of the product of two matrices equals the product of their determinants. So, det(AB) = (det A)(det B) = (2)(12) =| 24].\\n\\nThe final answer is 24. I hope it is correct.\\n\\nluate (x + y)(x —y) when x = 13 andy =5.\\n\\nFigure A.8 | Prompt\\n\\nused for Unsupervised ICL with MATH and GSMB8K. We first start with a preamble saying that we are\\n\\ngoing to list a number of problems, and then we list the problems. We then give another pre-amble to specify the output\\n\\nformat, and include\\n\\nup to 4 examples to fully describe this output format. As we go to the many-shot setting with hun\\n\\nof examples, we on\\n\\nly increase the number of problems in the prompt, not the problem-solution pairs at the end.\\n\\nreds\\n\\n27\\n\\nMany-Shot In-Context Learning\\n\\nYou will be provided questions similar to the ones below:\\n\\nQuestion:\\n\\nA large gene has dozens of exons, of which the central ones code for folded triple helical repeats that connect the cytoskeleton with sarcolemma and extracellular space. Each exon usually codes for one folded triple alpha helix. The most common mutations of the gene are central exon deletions that create out-of-frame peptides and progressive degenerative organ waste. A solution is to deliver a Morpholino that recognizes the 5’ end of the out-of-frame exon in pre-mRNA. The molecule prevents binding of the spliceosome and creates exon skipping and in-frame joining. Several missing exons are well tolerated by an organism. Which structure below is not involved in the proposed therapy?\\n\\n(A) antisense\\n\\n(B) polyA tail', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='(C) R-loops\\n\\n(D) lariat\\n\\nQuestion:\\n\\nYou will be given a multiple choice question with different choices such as (A), (B), (C), (D). Think step by step before giving a final answer to this question. Always finish your answer with ’Final Answer: (X)’, where X is the correct answer choice. If none of the options match, choose the closest option as the final answer.\\n\\nFigure A.9 | Unsupervised ICL Prompt for GPQA. We first start with a preamble saying that we are going to list a number of questions, and then we list the questions. We then give another preamble to specify the output format. As we go to the many-shot setting with hundreds of examples, we only increase the number of questions in the prompt.\\n\\nYou will be provided source sentences in English to translate in into Kurdish similar to the ones below:\\n\\nEnglish: Its remnants produced showers across most of the islands, though as of yet, no damage or flooding has been reported.\\n\\nYou are an expert translator. I am going to give you one or more example pairs of text snippets where the first is in English and the second is a translation of the first snippet into Kurdish. The sentences will be written\\n\\nEnglish: <first sentence>\\n\\nKurdish: <translated first sentence>\\n\\nAfter the example pairs, I am going to provide another sentence in English and I want you to translate it into Kurdish. Give only the translation, and no extra commentary, formatting, or chattiness. Translate the text from English to Kurdish.\\n\\nEnglish: Its remnants produced showers across most of the islands, though as of yet, no damage or flooding\\n\\nhas been reported.\\n\\nKurdish: Li heréma Serengetiyé, Parka Neteweyi ya Serengeti ya Tanzanyayé, Cihé Parastina Ngorongoro i\\n\\nCihé Parastina Giyanewerén Néciré Maswa ti Cihé Parastina Neteweyi ya Masai Mara ya Kendyayé hene. English: - -- Kurdish:', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='Figure A.10 | Unsupervised ICL Prompt for the low-resource MT task. We first start with a preamble saying that we are going to list a number of source sentences, and then we list the sentences. We then give another preamble with 1 input-output example to specify the output format. As we go to the many-shot setting with hundreds of examples, we only increase the number of source sentences in the prompt.\\n\\n28\\n\\nMany-Shot In-Context Learning A.3. Unsupervised ICL on Machine Translation\\n\\nTranslation: English > Kurdish\\n\\nS 45} -* ICL (Source and Target) . —e— Unsupervised ICL (Source Only) a 40 uw c rc » 35 wn & 30\\n\\n1 5 10 25 50 125250500997 Number of Shots (K)\\n\\nFigure A.11 | Unsupervised ICL does not work for low-resource machine translation. This is expected as providing\\n\\nonly source sentences for translation task doesn’t improve the task specification. See Figure A.10 for the prompt used for unsupervised ICL for this experiment.\\n\\nA.4. Reinforced ICL: Data-collection Prompt Sensitivity and Iteration 2\\n\\nIter 1 (Minerva) Mm Iter1 Mmm Iter 2 MATHS500: Reinforced ICL 60.0%\\n\\n_ re i °\\n\\n257.5% 2 i om eS\\n\\n>. Ht th i\\n\\n% 55.0% 4- Mini P,\\n\\na\\n\\no 52.5%\\n\\n252.\\n\\nra 0,\\n\\n90.0% shot InnerMono. Prompt 47.5%\\n\\n25 50 125 250 500 Number of Shots (K)\\n\\nFigure A.12 | Reinforced ICL Hendrycks MATH. We find the performance of model-generated rationales with 4-shot Minerva prompt is generally better or comparable to the ones generated by 4-shot InnerMono. MATH prompt. Furthermore, another iteration of Reinforced ICL - generating rationales from the best performing 25-shot prompt (with model-generated\\n\\nrationales) on the MATH training set and using the problems which were not solved in first iteration - seem to further improve many-shot performance.\\n\\nA.5. Linear Classification: Data Generation', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='For each classification dataset, we randomly sample another N-dimensional vector as the decision boundary and a decision threshold. We then provide K N-dimensional points above this threshold and K points below that same threshold as in-context exemplars, and the model must determine whether unseen N-dimensional points are above or below the threshold (we do not tell the model the equation or the threshold). We provide the python code for date generation below.\\n\\nimport numpy as np def _generate_dataset(minv, maxv, N, k, a, t):\\n\\nxtrain, ytrain = [], [] count_pos, count_neg = 0, 0\\n\\n29\\n\\nMany-Shot In-Context Learning\\n\\nwhile (count_pos < k) or (count_neg < k): x_ex = np.random.randint(minv, maxv, size=N) label = 1 if np.dot(x_ex, a) >t:\\n\\nif count_pos >= k:\\n\\ncontinue count_pos += 1 else: if count_neg >= k: continue count_neg += 1 label = -1\\n\\nxtrain .append(x_ex) ytrain . append (label) return np.array(xtrain).astype(str), np.array(ytrain)\\n\\ndef GENERATEEVAL(N, k, seed): \"\"\"Generates one evaluation example for N-dimensional linear classification.\\n\\nArgs: N: Dimensionality of the data. k: Number of in-context exemplars per class.\\n\\nReturns: xtrain: A list of 2k training examples (k positive, k negative) ytrain: A list of corresponding labels for training examples. xeval: A list of evaluation examples (25 positive, 25 negative) yeval: Ground-truth labels for evaluation examples.\\n\\n# Step 2: Generate ground-truth coefficients np.random. seed (seed) minv, maxv = 1, 1000 a = np.random.randint(minv, maxv, size=N) # Random integer coefficients\\n\\n# Step 3: Generate a pivot point p = np.random.randint(minv, maxv, size=N)\\n\\n# Step 4: Calculate the classification threshold t = np.dot(a, p)\\n\\n# Steps 5: Generate training examples xtrain, ytrain = generate _dataset(minv, maxv, N, k, a, t)\\n\\n# Steps 6: Generate the evaluation example xeval, yeval = _generate_dataset(minv, maxv, N, 25, a, t)\\n\\nreturn xtrain, ytrain, (xeval, yeval)', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='Listing 1 | Code for Generating Sythetic datasets for Linear Classification in High Dimensions.\\n\\nA.6. Training GPT-2 from scratch on the sequential parity task\\n\\n: Max num examples\\n\\n10 | in many-shot ICL. 1 1 08 |} 1 1 > 1 Joe | 3 I Performance of @ 04 -}--------------------} AP ---------------- many-shot ICL 1 1 0.2 | | — best small 1 — 0.0 best medium 0.0 0.5 1.0 15 2.0 2.5 3.0 Num examples seen 1e5\\n\\nFigure A.13 | For the sequential parity task, training a transformer from scratch does not meet 8192-shot ICL performance (dashed lines) until 20x the number of examples. We trained two transformers on the sequential parity task (from §4.2). The smaller model was the size of GPT-2 Small, with 12 layers and 768 embedding dimension. The larger model was the size of GPT-2 Medium, with 24 layers and 1024 embedding dimension. We trained using a linear warmup and square root decay schedule, sweeping max learning rate values [le-5, 5e-5, le-4, 5e-4, 1-e3] and num warmup steps [50, 100, 500, 1000, 5000]. The best values for both models (fastest learning) were max_lr=1e-4, warmup_steps=1000.\\n\\n30\\n\\nMany-Shot In-Context Learning\\n\\nA.7. Negative Log-Likelihood on Model-Generated Data\\n\\nNegative Log Likelihood (NLL) on Model Generated Test Solutions with L-shot prompt\\n\\nL=4 L=10 L=25 L=50 0.125 0.10 0.11 a 30.10 a a = 0.100 2 2 3 0.10 0.09 0.075 0.08 0.08 TS Pe PPS TS PS Pes Ss PS PSS Ss PS PSS Num Shots in Scoring Prompt Num Shots in Scoring Prompt Num Shots in Scoring Prompt Num Shots in Scoring Prompt L=125 L=250 L=500 0.14 0.150 0.12 oro yet Z 0.125 = = o10 = 0.100 0.08 RS pS OS RS po pS RS pO PP So LS PP PPS SPP PES PS VPP PES Num Shots in Scoring Prompt Num Shots in Scoring Prompt Num Shots in Scoring Prompt —— Correct —— Incorrect', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='Figure A.14 | NLL For Model-Generated Data. We compute negative log-likelihood on model-generated solutions for Hendrycks MATH. We generate solutions using prompts of varying lengths, starting from (model-generated) 4-shot prompts, and going up to 500-shot prompts. We then score the solutions with (model-generated) prompts of different lengths, from 4-shot to 500-shot prompts. We see that, for the solutions generated using the 4-shot prompt, the NLL is lowest for 4-shot and 10-shot scoring prompts, and increases as the length of the scoring prompt increases. Note that the generation and scoring prompts contain different subsets of examples from the training sets (and are randomized for each sample). Generally speaking, we see that the NLL is lowest when the generation and the scoring prompts are of similar lengths, and higher when they are further apart. This reveals to us that prompts of similar lengths induce similar distributions over the outputs, even when the actual contents of the prompt are different. Further, we see that correct and incorrect solutions follow similar curves, and the gap between NLL for correct and incorrect solutions does not follow any specific pattern.\\n\\nA.8. Many-Shot ICL: Hallucinations on Summarization\\n\\nHere is an example on XSum showing summaries with halluncinated date and time when increasing the number of shots for many-shot ICL, discussed in §2.2.\\n\\nReference Summary: Russia’s leading independent polling agency has been labelled a \"foreign agent\" by the justice ministry and says it cannot now work.\\n\\n1-shot: The Levada Centre, a Russian organization that conducts political opinion surveys, has been designated as a \"foreign agent\" by the government. This label restricts the organization’s activities and effectively prevents it from conducting independent polls, particularly in relation to the upcoming parliamentary elections. Critics view this move as political censorship and an attempt to suppress dissent.', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'}),\n",
       " Document(page_content='5-shot: Russia’s leading independent pollster has been labelled a \"foreign agent\" by the justice ministry, two weeks before parliamentary elections.\\n\\n10-shot: A leading Russian opinion pollster has been labelled a \"foreign agent\" by the authorities, effectively barring it from conducting any work linked to the upcoming parliamentary elections.\\n\\n25-shot: Russia’s leading independent pollster has been labelled a \"foreign agent\" by the justice ministry, effectively barring it from conducting election-related work.\\n\\n50-shot: 12 September 2016 Last updated at 14:32 GMT One of Russia’s last independent pollsters has been designated a \"foreign agent\" by the justice ministry.\\n\\n125-shot: 12 September 2016 Last updated at 14:43 GMT\\n\\nOne of Russia’s last independent pollsters has been branded a \"foreign agent\" by the justice ministry. 250-shot: 2016-09-06 Last updated at 14:33 GMT\\n\\nRussia’s only independent pollster has been branded a \"foreign agent\" by the justice ministry.\\n\\n500-shot: 12 September 2016 Last updated at 14:22 GMT One of Russia’s last independent pollsters has been branded a \"foreign agent\" by the government.\\n\\n31', metadata={'source': 'C:\\\\Users\\\\CAIO_B~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfoev7n9v\\\\tmp.pdf'})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(online_pdf_data)\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "025787ea-c207-4389-9b54-de66c5394376",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caio_barros\\AppData\\Local\\anaconda3\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for OpenAIEmbeddings\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m embedder \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:183\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    182\u001b[0m     emit_warning()\n\u001b[1;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\pydantic\\main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for OpenAIEmbeddings\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "embedder = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2136af81-901a-4176-b404-fcf98499e782",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"SINGLESTOREDB_URL\"] = f\"admin:{os.environ[\"SINGLESTORE_PASSWORD\"]}@{os.environ[\"SINGLESTORE_USER\"]}:3306/db_CaioBarros_7afc1\"\n",
    "\n",
    "docsearch = SingleStoreDB.from_documents(\n",
    "    texts,\n",
    "    embedder,\n",
    "    table_name = \"pdf_docs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c378b9-db7e-4052-b229-1fa78838e3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"What is the table of contexts of this pdf?\"\n",
    "\n",
    "docs = docsearch.similarity_search(query_text)\n",
    "\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c61414c-02ec-4243-b23d-f57e342af46f",
   "metadata": {},
   "source": [
    "## ChatCompletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece247d8-a657-4c50-94af-499a3ac5cbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"The user asked: {query_text}. The most similar text from the document is: {docs[0].page_content}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
